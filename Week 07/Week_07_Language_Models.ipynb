{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE7fXh-OSJYF"
   },
   "outputs": [],
   "source": [
    "#!pip3 -qq install torch==0.4.1\n",
    "# !pip install torchtext==0.3.1\n",
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1Pq4aklVdj-sOnQw68e1ZZ_ImMiC8IR1V' -O tweets.csv.zip\n",
    "# !wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ji7dhr9FojPeV51dDlKRERIqr3vdZfhu\" -O surnames.txt\n",
    "# !unzip tweets.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhvfH55PUJ8K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVcnkGDgxfNx"
   },
   "source": [
    "# Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Kjg1Z3xxmEP"
   },
   "source": [
    "*Языковая модель* - это штука, которая умеет оценивать вероятности встретить последовательность слов $w_1, \\ldots, w_n$:   \n",
    "$$\\mathbf{P}(w_1, \\ldots, w_n) = \\prod_k \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{1}).$$\n",
    "\n",
    "Интерпретируемы и интересны тут именно условные вероятности - какое слово языковая модель ожидает вслед за данными. У нас у всех такая языковая модель есть, так-то. Например, в таком контексте\n",
    "\n",
    "![](https://hsto.org/web/956/239/601/95623960157b4e15a1b3f599aed62ed2.png \" \")\n",
    "\n",
    "моя языковая модель говорит - после *честных* навряд ли пойдёт *мой*. А вот *и* или, конечно, *правил* - очень даже.\n",
    "\n",
    "А задача такая: научиться генерировать политические твиты по образу и подобию `Russian Troll Tweets`. Датасет взят отсюда: https://www.kaggle.com/vikasg/russian-troll-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpjfUoN4_WY7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @RivalThoughts: @CNN This lack of forethought is what ran the country into the ground.',\n",
       " \"#TopNews Icahn on CNBC:'Archie Bunker of the world' will vote for Trump\",\n",
       " 'RT @mclark1951: Just 5.7 Percent Of #Clinton Foundation Budget Actually Went To Charitable Grants https://t.co/c2EIYW4w9H #uniteblue2016 #p…',\n",
       " \"RT @JamilSmith: Read @jelani9 on Rudy Giuliani's flailing effort to gain relevance in this Trump moment. https://t.co/48p9d31pgi\",\n",
       " 'RT @Laura_A_Diaz: #StandUpWithEvan #MakeHistoryWithEvan  Unite and #Vote3rdParty #Deny270 https://t.co/quB1FdZZAo',\n",
       " 'RT @MommyExchangeGa: Camouflage Wedding Rings Made From Titanium! High Quality. Choose from Promise, Wedding, Friendship and Couples https:…',\n",
       " 'RT @NewssTrump: BREAKING: Trump’s UN Ambassador Just Put The Fear Of God In Our Enemies! She Just Gave The UN Teeth For The First… https://…',\n",
       " '@Nero March for Trump at Trump tower NY happening now:\\n#Trump #MAGA https://t.co/SWvbWxOEjO',\n",
       " \"Why don't Portuguese Muslims speak out and condemn this guy? If he doesn't represent Islam hasn't he offended them?… https://t.co/3hWEBCNgyg\",\n",
       " 'RT @theclobra: People either live in anonymity and then attack people for putting themselves out there or they are hypocrites and do the sa…',\n",
       " 'RT @GoldStarMomTX55: ChristiChat: RT ChristiChat: Dem IRONY!\\r\\nbillclinton address in 1995 “We are a nation of immigrants, but we are als… ht…',\n",
       " 'RT @pollygolightly: Boundaries. #GiftIdeasForPoliticians https://t.co/N0EN6hvVaD',\n",
       " 'RT @TheYoungTurks: .@HillaryClinton was caught on tape discussing rigging an election. https://t.co/a3WTZ0fIXz',\n",
       " 'RT @FeministaJones: Old Bay Macaroni and Cheese https://t.co/8yyIOxMmUh',\n",
       " 'RT @mansplainer123: Iran said it only takes 7min to hit Tel-Aviv.We need to point out that it will take 45min after that to turn them and t…']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('tweets.csv')\n",
    "\n",
    "data.text.sample(15).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_key</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>posted</th>\n",
       "      <th>mentions</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.868981e+09</td>\n",
       "      <td>ryanmaxwell_1</td>\n",
       "      <td>1.458672e+12</td>\n",
       "      <td>2016-03-22 18:31:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>7.123460e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"IslamKills\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.571870e+09</td>\n",
       "      <td>detroitdailynew</td>\n",
       "      <td>1.476133e+12</td>\n",
       "      <td>2016-10-10 20:57:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>7.855849e+17</td>\n",
       "      <td>&lt;a href=\"http://twitterfeed.com\" rel=\"nofollow...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"http://detne.ws/2e172jF\"]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.710805e+09</td>\n",
       "      <td>cookncooks</td>\n",
       "      <td>1.487767e+12</td>\n",
       "      <td>2017-02-22 12:43:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>8.343832e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.584153e+09</td>\n",
       "      <td>queenofthewo</td>\n",
       "      <td>1.482765e+12</td>\n",
       "      <td>2016-12-26 15:06:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>8.134006e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"ChristmasAftermath\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.768260e+09</td>\n",
       "      <td>mrclydepratt</td>\n",
       "      <td>1.501987e+12</td>\n",
       "      <td>2017-08-06 02:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>8.940243e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id         user_key    created_at          created_str  \\\n",
       "0  1.868981e+09    ryanmaxwell_1  1.458672e+12  2016-03-22 18:31:42   \n",
       "1  2.571870e+09  detroitdailynew  1.476133e+12  2016-10-10 20:57:00   \n",
       "2  1.710805e+09       cookncooks  1.487767e+12  2017-02-22 12:43:43   \n",
       "3  2.584153e+09     queenofthewo  1.482765e+12  2016-12-26 15:06:41   \n",
       "4  1.768260e+09     mrclydepratt  1.501987e+12  2017-08-06 02:36:24   \n",
       "\n",
       "   retweet_count retweeted  favorite_count  \\\n",
       "0            NaN       NaN             NaN   \n",
       "1            0.0     False             0.0   \n",
       "2            NaN       NaN             NaN   \n",
       "3            NaN       NaN             NaN   \n",
       "4            NaN       NaN             NaN   \n",
       "\n",
       "                                                text      tweet_id  \\\n",
       "0  #IslamKills Are you trying to say that there w...  7.123460e+17   \n",
       "1  Clinton: Trump should’ve apologized more, atta...  7.855849e+17   \n",
       "2  RT @ltapoll: Who was/is the best president of ...  8.343832e+17   \n",
       "3  RT @jww372: I don't have to guess your religio...  8.134006e+17   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...  8.940243e+17   \n",
       "\n",
       "                                              source                hashtags  \\\n",
       "0                                                NaN          [\"IslamKills\"]   \n",
       "1  <a href=\"http://twitterfeed.com\" rel=\"nofollow...                      []   \n",
       "2                                                NaN                      []   \n",
       "3                                                NaN  [\"ChristmasAftermath\"]   \n",
       "4                                                NaN                      []   \n",
       "\n",
       "                 expanded_urls  posted mentions  retweeted_status_id  \\\n",
       "0                           []  POSTED       []                  NaN   \n",
       "1  [\"http://detne.ws/2e172jF\"]  POSTED       []                  NaN   \n",
       "2                           []  POSTED       []                  NaN   \n",
       "3                           []  POSTED       []                  NaN   \n",
       "4                           []  POSTED       []                  NaN   \n",
       "\n",
       "   in_reply_to_status_id  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAQ4d__2_sAz"
   },
   "source": [
    "Да, результаты будут упороты, сразу предупреждаю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Qvqidof7Fsi"
   },
   "source": [
    "## Чтение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSu56oDX-KY5"
   },
   "source": [
    "Кого-нибудь уже достало писать все эти построения батчей, словари - вот это всё? Лично меня - да!\n",
    "\n",
    "В pytorch есть специальный класс для генерации батчей - `Dataset`. Вместо того, чтобы писать функцию типа `iterate_batches`, можно отнаследовать от него и переопределить методы `__len__` и `__getitem__`... и реализовать в них почти всё то, что было в `iterate_batches`. Пока не впечатляет, да?\n",
    "\n",
    "Ещё там есть `DataLoader`, умеющий работать с датасетом. Он позволяет делать shuffle батчей и генерацию их в отдельных процессах - это особенно важно, когда генерация батча - долгая операция. Например, в картинках. Почитать про это всё можно здесь: [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "Но пока что всё равно не особо круто, мне кажется. Интересно другое - у pytorch в репозитории живет отдельная библиотечка - [torchtext](https://github.com/pytorch/text). Вот она уже даст нам специальные реализации `Dataset` для работы с текстом и всякие тулзы, делающие жизнь чуточку проще.\n",
    "\n",
    "Библиотеке, на мой взгляд, недостает туториалов, в которых бы показывалось, как с ней работать - но можно читать исходный код, он приятный.\n",
    "\n",
    "План такой: построить класс `torchtext.data.Dataset`, для него создать итератор, и учить модель.\n",
    "\n",
    "Данный датасет инициализируется двумя параметрами:\n",
    "```\n",
    "            examples: List of Examples.\n",
    "            fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
    "                string is a field name, and the Field is the associated field.\n",
    "```\n",
    "Разберемся сначала со вторым.\n",
    "\n",
    "`Field` - это такая мета-информация для датасета + обработчик сэмплов.  \n",
    "\n",
    "Он имеет кучу параметров, на которые проще посмотреть [здесь](https://github.com/pytorch/text/blob/master/torchtext/data/field.py). Если коротко, то он может предобрабатывать (например, токенизировать) предложения, строить словарь (отображение из слова в индекс), строить батчи - добавлять паддинги и конвертировать в тензоры. Что ещё нужно в жизни?\n",
    "\n",
    "Мы будем делать character-level языковую модель, поэтому токенизация для нас - превращение строки в набор символов. Попросим также добавлять в начало и конец спец-символы `<s>` и `</s>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilAMVxA8Xy4L"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "text_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_i0Z6JhF0rA"
   },
   "source": [
    "Препроцессинг будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-8IPlPHFyKa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'i',\n",
       " 's',\n",
       " 'l',\n",
       " 'a',\n",
       " 'm',\n",
       " 'k',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 't',\n",
       " 'r',\n",
       " 'y',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'r',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " 't',\n",
       " 'a',\n",
       " 'c',\n",
       " 'k',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 'u',\n",
       " 'r',\n",
       " 'o',\n",
       " 'p',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'f',\n",
       " 'u',\n",
       " 'g',\n",
       " 'e',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.preprocess(data.text.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19NFhTSNF1_1"
   },
   "source": [
    "Сконвертируем всё и посмотрим на распределение длин:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wz1QnivMBmU3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a list of 30 Patch objects>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data['text'] = data['text'].fillna('')\n",
    "lines = data.apply(lambda row: text_field.preprocess(row['text']), axis=1).tolist()\n",
    "\n",
    "lengths = [len(line) for line in lines]\n",
    "\n",
    "plt.hist(lengths, bins=30)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dE9rPW9UHE7d"
   },
   "source": [
    "Отсечем слишком короткие строки и преобразуем оставшиеся в `Example`'ы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfTlpBxODBg8"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Example\n",
    "\n",
    "lines = [line for line in lines if len(line) >= 50]\n",
    "\n",
    "fields = [('text', text_field)]\n",
    "examples = [Example.fromlist([line], fields) for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z1wPlz_HeEP"
   },
   "source": [
    "По `Example` можно получить обратно все поля, которые мы туда запихнули. Например, сейчас мы создали одно поле `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGMRSuk_HYCm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'i',\n",
       " 's',\n",
       " 'l',\n",
       " 'a',\n",
       " 'm',\n",
       " 'k',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 't',\n",
       " 'r',\n",
       " 'y',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'r',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " 't',\n",
       " 'a',\n",
       " 'c',\n",
       " 'k',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 'u',\n",
       " 'r',\n",
       " 'o',\n",
       " 'p',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'f',\n",
       " 'u',\n",
       " 'g',\n",
       " 'e',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yef1bv2MQcEA"
   },
   "source": [
    "Построим, наконец, датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSccEmVIHAaQ"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "\n",
    "dataset = Dataset(examples, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEe5YXIpRCYD"
   },
   "source": [
    "Датасет можно разбить на части:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21whmJDFRBV1"
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.split(split_ratio=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14CyhugSQsOf"
   },
   "source": [
    "По нему можно построить словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQs3jbhyQkJD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 308\n",
      "['<unk>', '<pad>', '<s>', '</s>', ' ', 't', 'e', 'a', 'o', 'r', 'i', 's', 'n', 'l', 'h', 'c', 'p', 'd', 'm', 'u', '/', 'g', 'y', ':', 'w', 'b', 'f', '.', '@', 'k', 'v', '#', 'j', 'z', 'x', '\\n', \"'\", '…', '1', ',', '0', '2', 'q', '\\r', '!', '4', '6', '7', '3', '\"', '5', '-', '9', '8', '_', '?', ';', '’', '&', ')', '(', '‘', '$', '“', '😂', '|', '*', '”', '%', '–', '️', 'о', '🇸', '🇺', 'а', 'ü', '\\xa0', 'и', 'т', 'е', '▶', '🔥', 'н', '~', '+', 'ä', 'р', '[', ']', '💥', '—', 'л', 'с', '=', 'в', '🚨', 'é', 'к', 'м', '❤', 'п', 'д', 'ö', '👇', '👍', '🤔', 'ا', '‼', '😭', '👏', 'у', '★', '🏻', '`', '�', 'б', '🏾', 'з', '👉', '🙏', 'ы', 'ل', '😍', '🏼', '😡', 'г', 'ь', 'я', 'ß', '🏽', '»', '•', '✔', 'ч', '😊', 'ي', '🏿', 'й', 'م', '💯', '😎', '💀', '🙄', '😳', 'à', 'ر', 'х', 'و', '✨', 'ж', '➡', 'ن', '🙌', '💨', '✊', '👊', '«', '😉', 'è', '🌟', '⚡', '😘', 'ю', 'ा', 'ت', '💪', 'ш', '🎉', '😏', '🚂', '😩', 'د', '➠', '►', 'े', '💩', '💰', 'ب', '❗', 'ه', '👀', 'र', '😁', '👌', '😱', 'ع', '✅', '👈', 'क', '🌹', '´', '☺', '💣', '😅', '🎶', '💕', '\\x92', '\\u200d', 'ç', 'س', '\\u200b', '☆', '😄', 'י', '🔴', '⬇', 'の', '😠', 'ة', '😆', '❌', '😒', '😔', '😜', '🚫', 'ح', '🇷', '🎄', '🎯', 'ф', 'ц', 'ह', 'ı', 'त', '™', '✌', '💙', '🤣', 'ו', 'स', '💔', '💦', '😢', '💜', '\\\\', 'š', 'щ', 'ك', 'á', '„', '♫', '。', '🆓', '💗', '♥', '🇫', '🔫', 'ê', '👑', '😈', 'ר', 'म', 'ि', '🗽', 'ف', 'ी', '⁉', '⭐', '💃', 'न', 'い', '🆘', '😀', 'ž', '्', '☀', '🐾', '🔶', '😝', 'ה', 'ो', 'ñ', 'ó', 'ת', 'ج', '🌴', '£', '¯', 'č', '→', '❓', '❣', '⤵', '💖', '💞', '😷', '🏆', '🤗', 'ق', 'ं', 'प', 'ब', '🎈', '👆', '🤘', '^', 'ش', '☕', '♡', 'は', '😑', '😴', 'ग', 'ल', '➖', '👎', '💫', '💲']\n"
     ]
    }
   ],
   "source": [
    "text_field.build_vocab(train_dataset, min_freq=30)\n",
    "\n",
    "print('Vocab size =', len(text_field.vocab))\n",
    "print(text_field.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_EAdgsWRTzj"
   },
   "source": [
    "Наконец, по нему можно итерироваться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaEMoxdVG98p"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMG4L1-5RXnb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 150x32 (GPU 0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZZgplOkReeq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2,  2,  ...,  2,  2,  2],\n",
       "        [ 9, 31,  9,  ...,  9,  9, 17],\n",
       "        [ 5, 12,  5,  ...,  5,  5,  8],\n",
       "        ...,\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTrSUkqEhZzh"
   },
   "source": [
    "## Перплексия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqc9HpTM-FwD"
   },
   "source": [
    "Нашу задачу, как всегда, нужно начинать с двух вопросов - какую метрику оптимизируем и какой бейзлайн.\n",
    "\n",
    "С метрикой всё просто - мы хотим, чтобы модель как можно лучше умела приближать распределение слов языка. Всего языка у нас нету, поэтому обойдёмся тестовой выборкой.\n",
    "\n",
    "На ней можно посчитать кросс-энтропийные потери: \n",
    "$$H(w_1, \\ldots, w_n) = - \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1).$$\n",
    "\n",
    "Здесь вероятность $\\mathbf{P}$ - это вероятность, оцененная нашей языковой моделью. Идеальная модель давала бы вероятность равную 1 для слов в тексте и потери были бы нулевыми - хотя это, конечно, невозможно, даже вы же не можете предсказать следующее слово, что уж про бездушную машину говорить.\n",
    "\n",
    "Таким образом, всё как всегда - оптимизируем кросс-энтропию и стремимся сделать её как можно ниже.\n",
    "\n",
    "Ну, почти всё. Ещё есть отдельная метрика для языковых моделей - *перплексия*. Это просто возведенные в экспоненту кросс-энтропийные потери:\n",
    "\n",
    "$$PP(w_1, \\ldots, w_n) = e^{H(w_1, \\ldots, w_n)} = e^{- \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1)} = \\left(\\mathbf{P}(w_1, \\ldots, w_n) \\right)^{-\\frac 1n}.$$\n",
    "\n",
    "У её измерения есть некоторый сакральный смысл кроме банальной интепретируемости: представим модель, предсказывающую слова из словаря равновероятно вне зависимости от контекста. Для неё $\\mathbf{P}(w) = \\frac 1 N$, где $N$ — размер словаря, а перплексия будет равна размеру словаря — $N$. Конечно, это совершенно глупая модель, но оглядываясь на неё, можно трактовать перплексию реальных моделей как уровень неоднозначности генерации слова.\n",
    "\n",
    "Скажем, в модели с перплексией 100 выбор следующего слова также неоднозначен, как выбор из равномерного распределения среди 100 слов. И если такой перплексии удалось достичь на словаре в 100 000, получается, что удалось сократить эту неоднозначность на три порядка по сравнению с тупым рандомом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW8I0lKv9y1H"
   },
   "source": [
    "## Бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wInBuBn-DIf"
   },
   "source": [
    "Вообще, бейзлайн тут тоже очень простой. Мы, на самом деле, даже смотрели его на курсе концепций: [N-граммная языковая модель](https://colab.research.google.com/drive/1lz9vO6Ue5zOiowEx0-koXNiejBrrnbj0). Можно подсчитывать вероятности N-грамм слов по частотностям их появления в обучающем корпусе. А дальше использовать аппроксимацию $\\mathbf{P}(w_k|w_1, \\ldots, w_{k-1}) \\approx \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{k-N + 1})$.\n",
    "\n",
    "Применим лучше сеточки для реализации того же.\n",
    "\n",
    "![](https://image.ibb.co/buMnLf/2018-10-22-00-22-56.png \"\")  \n",
    "*From cs224n, Lecture 8 [pdf](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)*\n",
    "\n",
    "На вход приходит последовательность слов, они эмбеддятся, а дальше с помощью выходного слоя считается наиболее вероятное следующее слово.\n",
    "\n",
    "Стоп... Но мы же уже реализовывали такое! В Word2vec CBoW модели мы по контексту предсказывали центральное слово - единственное отличие в том, что теперь мы имеем только левый контекст. Значит, всё, идём к следующей модели?\n",
    "\n",
    "Нет! Тут ещё есть с чем развлечься. В Word2vec мы формировали батчи таким образом:\n",
    "![](https://image.ibb.co/bs3wgV/training-data.png \"\")  \n",
    "*From [Word2Vec Tutorial - The Skip-Gram Model, Chris McCormic](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)*\n",
    "\n",
    "То есть нарезали из текста набор пар <контекст, слово> (и как-то их использовали в зависимости от метода).\n",
    "\n",
    "Это нерационально - каждое слово повторяется много раз. Но можно использовать сверточные сети - они за нас применят операцию умножения на $W$ к каждому окну. В результате размер входного батча будет сильно меньше.\n",
    "\n",
    "Чтобы правильно всё обработать, нужно добавить паддинг в начало последовательности размером `window_size - 1` - тогда первое слово будет предсказываться по `<pad>...<pad><s>`.\n",
    "\n",
    "**Задание** Реализуйте языковую модель с фиксированным окном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-tn_Gmi3pU0"
   },
   "outputs": [],
   "source": [
    "class ConvLM(nn.Module):\n",
    "    def __init__(self, vocab_size, window_size=5, emb_dim=16, filters_count=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._window_size = window_size\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=1)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, filters_count, kernel_size = self._window_size),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm1d(filters_count)\n",
    "        )\n",
    "        self.lin = nn.Linear(filters_count, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        padding = inputs.new_zeros((self._window_size -1, inputs.shape[1]))\n",
    "        x = torch.cat([padding, inputs], dim =0)\n",
    "        x = self.emb(x)\n",
    "        x = x.permute((1,2,0))\n",
    "        x = self.conv(x)\n",
    "        x = x.permute((2,0,1))\n",
    "        x = self.lin(x)\n",
    "        return x, None  # hacky way to use training cycle for RNN and Conv simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjpOLKBH5yS5"
   },
   "source": [
    "Проверим, что оно работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ks_RTZ14nMRz",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 32, 308])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "model(batch.text)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lb_2VTBW5v_7"
   },
   "source": [
    "**Задание** Реализуйте функцию для сэмплирования последовательности из языковой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oUg0BjV2JjE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👌ح👆в💲/|は🏽👎👎=✨|فq9ר🆓🔫😂ıгי😂⤵क\\🏿ž😍—ग👊★💰ी😘تю🙌🎯💃🎶👇😈#🙏فıはلç🔫èв'*m+…iلж✅🗽mjг☺|*i👌💩èं🙌म🏾щ⁉l'תж💦☺👌'o´गسसm¯ç™ה🔴2ب;म🏾jà➡ब.иि~😷😭⭐י~⚡🙏❣ब💲ת😷יжक🆘\\b💩гр`★_èग]”のüह💞а🐾ц"
     ]
    }
   ],
   "source": [
    "def sample(probs, temp):\n",
    "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
    "    probs = (probs / temp).exp()\n",
    "    probs /= probs.sum()\n",
    "    probs = probs.cpu().numpy()\n",
    "\n",
    "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
    "\n",
    "\n",
    "def generate(model, temp=0.7):\n",
    "    model.eval()\n",
    "    \n",
    "    history = [train_dataset.fields['text'].vocab.stoi['<s>']]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(150):\n",
    "            inputs = torch.LongTensor(history).unsqueeze(1).cuda()\n",
    "            \n",
    "            preds, _ = model(inputs)\n",
    "            preds = preds[-1]\n",
    "            \n",
    "            idx = sample(preds, temp)\n",
    "            history.append(idx)\n",
    "            print(train_dataset.fields[\"text\"].vocab.itos[idx], end = \"\")\n",
    "\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXuN871a852l"
   },
   "source": [
    "**Задание** Мы до сих пор не задали никакой target. А предсказывать нам будет нужно следующие слова - то есть просто сдвинутый на 1 входной тензор. Реализуйте построение target'а и подсчет потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGLkcXARjhTM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):                \n",
    "                logits, _ = model(batch.text)\n",
    "                \n",
    "                targets = torch.cat(\n",
    "                    [\n",
    "                        batch.text[1:], batch.text.new_ones((1, batch.text.shape[1]))\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
    "\n",
    "        generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIj0Lcdh9UJy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.20885, PPX = 9.11: 100%|██████████| 4381/4381 [00:26<00:00, 166.32it/s]\n",
      "[1 / 30]   Val: Loss = 2.07226, PPX = 7.94: 100%|██████████| 366/366 [00:02<00:00, 151.25it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sinews can pland! the life the for tomest dox of the meene suristy clinton will the we was of his a reale sed for all bebuted on eaders whilere bl"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 2.05359, PPX = 7.80: 100%|██████████| 4381/4381 [00:26<00:00, 167.90it/s]\n",
      "[2 / 30]   Val: Loss = 2.03711, PPX = 7.67: 100%|██████████| 366/366 [00:02<00:00, 157.01it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @thimmed freed don't ever barry. #makers https://t.co/nnyspresident of your hore to mant a donaldaring a king the land proming shomy are the one to"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 2.03006, PPX = 7.61: 100%|██████████| 4381/4381 [00:25<00:00, 169.66it/s]\n",
      "[3 / 30]   Val: Loss = 2.02205, PPX = 7.55: 100%|██████████| 366/366 [00:02<00:00, 158.88it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @shoot americans omth yee if it my the der on in the endia https://t.co/pkmaze ther @kaseaple every don't deforced https://t.co/rikying the wirl no"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 2.01761, PPX = 7.52: 100%|██████████| 4381/4381 [00:26<00:00, 165.47it/s]\n",
      "[4 / 30]   Val: Loss = 2.01263, PPX = 7.48: 100%|██████████| 366/366 [00:02<00:00, 156.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @blicqer: ▶ @reald how publishady5t</s>️🔥🔥 #election he why parking forceper in 201m so probama: feel #thesan: hestare hork's will the ding of monigin"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 2.00977, PPX = 7.46: 100%|██████████| 4381/4381 [00:25<00:00, 170.39it/s]\n",
      "[5 / 30]   Val: Loss = 2.00561, PPX = 7.43: 100%|██████████| 366/366 [00:02<00:00, 167.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonald to plan as the poll https://t.co/qt0v0uyghffor bublictory for in how the with want to be dease soball well usite have a reat the charge"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 2.00434, PPX = 7.42: 100%|██████████| 4381/4381 [00:26<00:00, 165.33it/s]\n",
      "[6 / 30]   Val: Loss = 2.00128, PPX = 7.40: 100%|██████████| 366/366 [00:02<00:00, 156.34it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @patry going up hound delection2016: ismanistmant over at these it grants and courks and covern @curking will thing this pro a proins ble is why am"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 2.00013, PPX = 7.39: 100%|██████████| 4381/4381 [00:26<00:00, 167.55it/s]\n",
      "[7 / 30]   Val: Loss = 1.99823, PPX = 7.38: 100%|██████████| 366/366 [00:02<00:00, 155.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @elinhorning that me the deedies: i hoplay https://t.co/3ezjlq</s>️sting sound in #jakencel @realmate trump's says https://t.co/kmtj9xmws wamed take a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 1.99699, PPX = 7.37: 100%|██████████| 4381/4381 [00:26<00:00, 165.67it/s]\n",
      "[8 / 30]   Val: Loss = 1.99606, PPX = 7.36: 100%|██████████| 366/366 [00:02<00:00, 156.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @homey htt…</s>…</s>……</s>…</s>…</s>……</s>……</s>……</s>……</s>…</s>……</s>……</s>……</s>…</s>……</s>…</s>……</s>……</s>……</s>……</s>……</s>……</s>…</s>…:…</s>…</s>……</s>…</s>……</s>…</s>……</s>……</s>……</s>……</s>…:…</s>…</s>……</s>…</s>…</s>……</s>…:…</s>…</s>……</s>…</s>……</s>……</s>…</s>…</s>……</s>……</s>…</s>…</s>…"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.99442, PPX = 7.35: 100%|██████████| 4381/4381 [00:26<00:00, 166.51it/s]\n",
      "[9 / 30]   Val: Loss = 1.99255, PPX = 7.33: 100%|██████████| 366/366 [00:02<00:00, 155.02it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sawhy bill pater that #2acks you man american, where in the of one political on the sues in for who brieffeeder the rapies 'ix the marach a was wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.99196, PPX = 7.33: 100%|██████████| 4381/4381 [00:26<00:00, 165.77it/s]\n",
      "[10 / 30]   Val: Loss = 1.98983, PPX = 7.31: 100%|██████████| 366/366 [00:02<00:00, 158.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#irsone its alestere times: get to ened in who sive of as the by call the behindfrents live as adintislishbumppishterporthope louth for while  #monta "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.98979, PPX = 7.31: 100%|██████████| 4381/4381 [00:26<00:00, 166.21it/s]\n",
      "[11 / 30]   Val: Loss = 1.98868, PPX = 7.31: 100%|██████████| 366/366 [00:02<00:00, 164.50it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @masals trump https://t.co/ozkeaws money to be that a streed and trump support a molitics</s>️ #rub #non't get more https://t.co/9ibyz</s>️ https://t.co/"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.98787, PPX = 7.30: 100%|██████████| 4381/4381 [00:26<00:00, 168.40it/s]\n",
      "[12 / 30]   Val: Loss = 1.98652, PPX = 7.29: 100%|██████████| 366/366 [00:02<00:00, 156.73it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @egritabinglist to the sechillary clinton via @amping be merond goor hereas in the up and cansed pence is the will many in the because supports @go"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.98636, PPX = 7.29: 100%|██████████| 4381/4381 [00:26<00:00, 164.95it/s]\n",
      "[13 / 30]   Val: Loss = 1.98513, PPX = 7.28: 100%|██████████| 366/366 [00:02<00:00, 167.62it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bozyd</s>️obamacion was profedeed #trump and have warn to provemileaks https://t.co/lcofly hat a hame tone worrica https://t.co/ry0zhb</s>️ #teally #pol"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.98498, PPX = 7.28: 100%|██████████| 4381/4381 [00:26<00:00, 163.73it/s]\n",
      "[14 / 30]   Val: Loss = 1.98471, PPX = 7.28: 100%|██████████| 366/366 [00:02<00:00, 157.80it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @consted ready what is theyer pipport with so if the sardeught https://t.co/qvhmiled from chimmisten of part's come emalicannity (brient has a sime"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.98375, PPX = 7.27: 100%|██████████| 4381/4381 [00:26<00:00, 165.38it/s]\n",
      "[15 / 30]   Val: Loss = 1.98299, PPX = 7.26: 100%|██████████| 366/366 [00:02<00:00, 160.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @tappain case chill be in of a clinton #drung https://t.co/2e4cxluhralto that bring on obama https://t.co/6cbww7blv: hillary says that this on trum"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.98258, PPX = 7.26: 100%|██████████| 4381/4381 [00:25<00:00, 170.63it/s]\n",
      "[16 / 30]   Val: Loss = 1.98159, PPX = 7.25: 100%|██████████| 366/366 [00:02<00:00, 163.23it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @paysing the man canding again's on to pentic \n",
      "statest: palts copart and is like https://t.co/7g46ancity as a copulliss of plesins people. has are "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.98147, PPX = 7.25: 100%|██████████| 4381/4381 [00:25<00:00, 170.14it/s]\n",
      "[17 / 30]   Val: Loss = 1.98111, PPX = 7.25: 100%|██████████| 366/366 [00:02<00:00, 156.18it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @fanishelelcenews men full there betternies. hillary clong of my the brookhitile here thing out perging https://t.co/lfhyn4fa. https://t.co/onsqhm</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.98055, PPX = 7.25: 100%|██████████| 4381/4381 [00:26<00:00, 163.46it/s]\n",
      "[18 / 30]   Val: Loss = 1.98088, PPX = 7.25: 100%|██████████| 366/366 [00:02<00:00, 159.68it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if boying appresibeoned to start will be absu sime an has to be dems so not every hillaryanews: #hillary white  https://t.co/eoaopinice amerigans to w"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.97957, PPX = 7.24: 100%|██████████| 4381/4381 [00:26<00:00, 163.42it/s]\n",
      "[19 / 30]   Val: Loss = 1.97995, PPX = 7.24: 100%|██████████| 366/366 [00:02<00:00, 154.06it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @chool https://t.…</s>/…</s>😱…</s>/oe heallose at a want misten vote https://t.co/ickyclincord! https://t.co/zohpqcjy</s>️…</s>/o…</s>/o…</s>/o…</s>/o…</s>/…</s>😱…</s>/…</s>😱…</s>/oe we "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.97859, PPX = 7.23: 100%|██████████| 4381/4381 [00:26<00:00, 167.44it/s]\n",
      "[20 / 30]   Val: Loss = 1.97939, PPX = 7.24: 100%|██████████| 366/366 [00:02<00:00, 161.16it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @kabment done hillary #via @ansonet or to and do my campliving obama is a not on factivilian: debated a right of us is speech to obama on the she w"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.97785, PPX = 7.23: 100%|██████████| 4381/4381 [00:25<00:00, 168.65it/s]\n",
      "[21 / 30]   Val: Loss = 1.97843, PPX = 7.23: 100%|██████████| 366/366 [00:02<00:00, 162.90it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @joldemail ? #trump of rem and complock: stam.</s>️…</s>/o…</s>/oe https://t.co/a8yjh…</s>?…</s>/om westould to maga shat to have on start. think with sardient ev"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.97720, PPX = 7.22: 100%|██████████| 4381/4381 [00:26<00:00, 165.62it/s]\n",
      "[22 / 30]   Val: Loss = 1.97720, PPX = 7.22: 100%|██████████| 366/366 [00:02<00:00, 159.30it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @gossart and what the donaldtrump @black. #drugh_dleright trump dolicans every trump some thight and mored dand to sear been my left's a rigna @pre"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.97657, PPX = 7.22: 100%|██████████| 4381/4381 [00:25<00:00, 168.73it/s]\n",
      "[23 / 30]   Val: Loss = 1.97663, PPX = 7.22: 100%|██████████| 366/366 [00:02<00:00, 168.46it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @america ple say we de the #isogutisnery in the on peever is by @handa</s>️…</s>/o…</s>/oe @gabsain on invem charges feel acceuse how spater this of have th"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.97612, PPX = 7.21: 100%|██████████| 4381/4381 [00:26<00:00, 167.96it/s]\n",
      "[24 / 30]   Val: Loss = 1.97672, PPX = 7.22: 100%|██████████| 366/366 [00:02<00:00, 161.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @lexth @suoscar be a goter stamessey https://t.co/jg3lxk</s>️…</s>/o…</s>/o…</s>/o…</s>/ocgrp @dlivitain the people who more afreised w/ hipdes in if worder https"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.97559, PPX = 7.21: 100%|██████████| 4381/4381 [00:26<00:00, 165.78it/s]\n",
      "[25 / 30]   Val: Loss = 1.97594, PPX = 7.21: 100%|██████████| 366/366 [00:02<00:00, 157.98it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @tonomy clinton vernaids livesspander greakemer commone the but. https://t.co/dwk0pravecons iver this them sen doesn't real cleafishdry is the publ"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.97507, PPX = 7.21: 100%|██████████| 4381/4381 [00:26<00:00, 165.45it/s]\n",
      "[26 / 30]   Val: Loss = 1.97515, PPX = 7.21: 100%|██████████| 366/366 [00:02<00:00, 158.72it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @proman back this chair god of and of the crobartiled the the thing</s>️\n",
      "he ward in part is misted tecials america on bama everyone https://t.co/1iwjb"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.97460, PPX = 7.20: 100%|██████████| 4381/4381 [00:26<00:00, 171.14it/s]\n",
      "[27 / 30]   Val: Loss = 1.97521, PPX = 7.21: 100%|██████████| 366/366 [00:02<00:00, 159.56it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ralking make about jorber ore sees at you of do but the breins des in sitics</s>️…</s>/oe https://t.co/mbihker go liek all menthankjallyhing becalse to "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.97412, PPX = 7.20: 100%|██████████| 4381/4381 [00:26<00:00, 167.87it/s]\n",
      "[28 / 30]   Val: Loss = 1.97418, PPX = 7.20: 100%|██████████| 366/366 [00:02<00:00, 151.92it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @dream hot ass what is not relican colld to stop resionsten on trump #hillary clinton but realing it coundreuth: guys is https://t.co/jyiip: he run"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.97367, PPX = 7.20: 100%|██████████| 4381/4381 [00:26<00:00, 165.87it/s]\n",
      "[29 / 30]   Val: Loss = 1.97354, PPX = 7.20: 100%|██████████| 366/366 [00:02<00:00, 159.09it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jehny: #brazer partbates greets to chiceles not trump caller: #really his to the sure ther gance: we the does to trump say a state all all readail"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.97320, PPX = 7.19: 100%|██████████| 4381/4381 [00:26<00:00, 165.41it/s]\n",
      "[30 / 30]   Val: Loss = 1.97333, PPX = 7.19: 100%|██████████| 366/366 [00:02<00:00, 162.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @america afrogk intelp hillary face in the for the we has this gop #righters american ‘treed after election and come to thild seare a was inders in"
     ]
    }
   ],
   "source": [
    "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FycAd6MWMvYy"
   },
   "source": [
    "**Задание** Чтобы отучить модель сэмплировать `<unk>` можно явным образом запрещать это в сэплирующей функции - а можно просто не учить ее на них. Реализуйте маскинг по одновременно и паддингам, и неизвестным словам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):                \n",
    "                logits, _ = model(batch.text)\n",
    "                \n",
    "                targets = torch.cat(\n",
    "                    [\n",
    "                        batch.text[1:], batch.text.new_ones((1, batch.text.shape[1]))\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "                \n",
    "                mask = (1 - ((targets.view(-1) == unk_idx) + (targets.view(-1) == pad_idx))).float().cuda()\n",
    "                \n",
    "                loss = (loss * mask).sum() / mask.sum()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
    "\n",
    "        generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.19516, PPX = 8.98: 100%|██████████| 4381/4381 [00:26<00:00, 163.03it/s]\n",
      "[1 / 30]   Val: Loss = 2.06238, PPX = 7.86: 100%|██████████| 366/366 [00:02<00:00, 178.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @amedgign is a realcy with immer https://t.co/8ffpkralken don contrump rept #y20 stable hover joosed. http://t.co/…</s> https://t.co/u7erto freetual p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 2.04631, PPX = 7.74: 100%|██████████| 4381/4381 [00:26<00:00, 163.74it/s]\n",
      "[2 / 30]   Val: Loss = 2.03143, PPX = 7.62: 100%|██████████| 366/366 [00:02<00:00, 170.31it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#thing and action on night is a say https://t.co/gcqs</s>ctimst: trump is been @msernation assan emart https://t.co/aters thillary bester people to he se"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 2.02437, PPX = 7.57: 100%|██████████| 4381/4381 [00:26<00:00, 163.47it/s]\n",
      "[3 / 30]   Val: Loss = 2.01641, PPX = 7.51: 100%|██████████| 366/366 [00:02<00:00, 165.55it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @matsmassmunger stront of doce #outhour: hetried outrone you harsen yor of trump sespeblesmacks on the dore hick obama the sanding says trump senti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 2.01256, PPX = 7.48: 100%|██████████| 4381/4381 [00:26<00:00, 162.41it/s]\n",
      "[4 / 30]   Val: Loss = 2.00684, PPX = 7.44: 100%|██████████| 366/366 [00:02<00:00, 162.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @thenogived by \"i deread #politic people the presender smart that's a preses and the \"then &amp; #beter. #misson praying a prefing trump https://t."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 2.00479, PPX = 7.42: 100%|██████████| 4381/4381 [00:26<00:00, 162.73it/s]\n",
      "[5 / 30]   Val: Loss = 2.00182, PPX = 7.40: 100%|██████████| 366/366 [00:02<00:00, 165.76it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @blick</s> new you've ore pastics</s> came one light of the fansa what say sartern you can the https://…</s>lo6: the terrorges #realdtrump is mo serears out"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 1.99958, PPX = 7.39: 100%|██████████| 4381/4381 [00:26<00:00, 163.09it/s]\n",
      "[6 / 30]   Val: Loss = 1.99675, PPX = 7.37: 100%|██████████| 366/366 [00:02<00:00, 165.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonald donal cander the can</s> soop pe https://t.co/1hmjdingshillary clinton was https://t.co/kprds</s> says post finsender is a fend in a word the"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 1.99582, PPX = 7.36: 100%|██████████| 4381/4381 [00:26<00:00, 163.98it/s]\n",
      "[7 / 30]   Val: Loss = 1.99390, PPX = 7.34: 100%|██████████| 366/366 [00:02<00:00, 166.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @stepportendoy https://t.co/3ztoldtrumage torm als on my wannive in the you to on in the to stare musting overyones the of sill kelling. about in t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 1.99262, PPX = 7.33: 100%|██████████| 4381/4381 [00:26<00:00, 162.38it/s]\n",
      "[8 / 30]   Val: Loss = 1.99036, PPX = 7.32: 100%|██████████| 366/366 [00:02<00:00, 166.61it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @re1515etidetord #docnars woming and about, new killos  #poter strain: (\n",
      "\n",
      "https://t.co/9nggmqs</s></s>…</s> eyour politich to kying relling actool it the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.99024, PPX = 7.32: 100%|██████████| 4381/4381 [00:26<00:00, 162.40it/s]\n",
      "[9 / 30]   Val: Loss = 1.98922, PPX = 7.31: 100%|██████████| 366/366 [00:02<00:00, 165.72it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bomows</s> e been figuty in the mattics</s> e puttweet polic, i hereaters.</s> emschich time boosh &amp; with the gether winch</s> cappert the if you to wark "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.98834, PPX = 7.30: 100%|██████████| 4381/4381 [00:26<00:00, 163.90it/s]\n",
      "[10 / 30]   Val: Loss = 1.98705, PPX = 7.29: 100%|██████████| 366/366 [00:02<00:00, 180.78it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#crives boids in protefight come the state womanglack will bay congant: by and don't martics, where says are amary https://t.co/sddudahare exc u real "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.98662, PPX = 7.29: 100%|██████████| 4381/4381 [00:26<00:00, 164.61it/s]\n",
      "[11 / 30]   Val: Loss = 1.98680, PPX = 7.29: 100%|██████████| 366/366 [00:02<00:00, 167.01it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sairion corroods the #clintranter.  #polations adyrien: some @mindist of canda with for reme ene…</s> e some have been read of the to sign......</s>…</s> e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.98489, PPX = 7.28: 100%|██████████| 4381/4381 [00:27<00:00, 162.14it/s]\n",
      "[12 / 30]   Val: Loss = 1.98442, PPX = 7.27: 100%|██████████| 366/366 [00:02<00:00, 172.84it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @tuder rate america withing your by this says the \"both and a pright: the expence the worth perparnned .. its https://t.co/5monring to dement to pr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.98340, PPX = 7.27: 100%|██████████| 4381/4381 [00:26<00:00, 163.41it/s]\n",
      "[13 / 30]   Val: Loss = 1.98344, PPX = 7.27: 100%|██████████| 366/366 [00:02<00:00, 177.57it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @gathat they die being to and news...the malling on can but in your still letter pollary fell need the can't show #trump as this can courly shail r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.98209, PPX = 7.26: 100%|██████████| 4381/4381 [00:27<00:00, 162.05it/s]\n",
      "[14 / 30]   Val: Loss = 1.98162, PPX = 7.25: 100%|██████████| 366/366 [00:02<00:00, 165.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sandoms but opence who gever win't the clinton lentony a &amp; it in extarant to be as conferefuge for kally pampan will her and proserver the ver"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.98085, PPX = 7.25: 100%|██████████| 4381/4381 [00:26<00:00, 166.48it/s]\n",
      "[15 / 30]   Val: Loss = 1.98090, PPX = 7.25: 100%|██████████| 366/366 [00:02<00:00, 164.77it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @shottrump and four campers so gat for don't https://t.co/vikhfirmacks farson abarty never: way trump our #camerkelf #islammaze</s>\n",
      "/carhforthemerican"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.97970, PPX = 7.24: 100%|██████████| 4381/4381 [00:26<00:00, 165.30it/s]\n",
      "[16 / 30]   Val: Loss = 1.97971, PPX = 7.24: 100%|██████████| 366/366 [00:02<00:00, 165.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sayshowering https://t.co/2by8n</s>\n",
      "/t5nmnqnc a fear ofter like a windersthoum dosalant of the for don’t like #pjnet #tcot #pjnet #gundy how you drad"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.97869, PPX = 7.23: 100%|██████████| 4381/4381 [00:26<00:00, 163.74it/s]\n",
      "[17 / 30]   Val: Loss = 1.97907, PPX = 7.24: 100%|██████████| 366/366 [00:02<00:00, 172.67it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @nottrumber pies to again to degop but marder: @james mon out refugend! https://t.co/mabfforia san on the servea stewer says to a realth killary ht"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.97764, PPX = 7.23: 100%|██████████| 4381/4381 [00:26<00:00, 165.11it/s]\n",
      "[18 / 30]   Val: Loss = 1.97838, PPX = 7.23: 100%|██████████| 366/366 [00:02<00:00, 186.71it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ossed the somisates obama pist #mantahit if liberg to make #merked in #comethread americanstard thospolicism would the the don't this would the me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.97707, PPX = 7.22: 100%|██████████| 4381/4381 [00:26<00:00, 164.87it/s]\n",
      "[19 / 30]   Val: Loss = 1.97725, PPX = 7.22: 100%|██████████| 366/366 [00:02<00:00, 178.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @obama rairing and mage belly: #djbimanday https://t.co/vbc3y https://t.co/xyfnjyton https://t.co/xedge and want the reffarloss about from https://"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.97638, PPX = 7.22: 100%|██████████| 4381/4381 [00:26<00:00, 164.85it/s]\n",
      "[20 / 30]   Val: Loss = 1.97654, PPX = 7.22: 100%|██████████| 366/366 [00:02<00:00, 169.83it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @blist in of the has addit tecrockt…</s>…… https://t.co/hakpche\n",
      "https://t.co/ostually https://t.co/ov6w7in</s>\n",
      "/t.co/embnjalm beat the will can party we"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.97580, PPX = 7.21: 100%|██████████| 4381/4381 [00:26<00:00, 164.79it/s]\n",
      "[21 / 30]   Val: Loss = 1.97590, PPX = 7.21: 100%|██████████| 366/366 [00:02<00:00, 167.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @mrica nate to see could phosocumptrey hosing on it os would trump with a can breality and stop and to ship https://t.co/cneune https://t.co/e9e68 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.97509, PPX = 7.21: 100%|██████████| 4381/4381 [00:26<00:00, 166.36it/s]\n",
      "[22 / 30]   Val: Loss = 1.97525, PPX = 7.21: 100%|██████████| 366/366 [00:02<00:00, 166.07it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @strypresole greation https://t.co/iqho…</s> //0.co/jtngxpzl5apblicqer: ▶@remily us net…</s>…… https://t.co/549cppdretupele the wore as the courantampenc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.97460, PPX = 7.20: 100%|██████████| 4381/4381 [00:26<00:00, 163.11it/s]\n",
      "[23 / 30]   Val: Loss = 1.97430, PPX = 7.20: 100%|██████████| 366/366 [00:02<00:00, 178.89it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonalistrivating whites on not sution in pround a repires. would bill of stractions can't medest hillary to reading the only no you and to is "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.97401, PPX = 7.20: 100%|██████████| 4381/4381 [00:26<00:00, 162.47it/s]\n",
      "[24 / 30]   Val: Loss = 1.97453, PPX = 7.20: 100%|██████████| 366/366 [00:02<00:00, 168.96it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ginsugvaouther trump but of statection the worry the bant was trate than i'm action't has be a law your https://t.co/wdvxy</s>\n",
      "/tscolar ways a cometh"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.97369, PPX = 7.20: 100%|██████████| 4381/4381 [00:26<00:00, 163.42it/s]\n",
      "[25 / 30]   Val: Loss = 1.97381, PPX = 7.20: 100%|██████████| 366/366 [00:02<00:00, 166.84it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @drsheith abrans of the has det we saying at hillary unch only with cuther dessill is a to protects https://t.co/9nvifeuss action right you democha"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.97325, PPX = 7.19: 100%|██████████| 4381/4381 [00:26<00:00, 163.20it/s]\n",
      "[26 / 30]   Val: Loss = 1.97488, PPX = 7.21: 100%|██████████| 366/366 [00:02<00:00, 176.28it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonaldtrumpains you're and to ret mous https://t.co/rfi4aalist of think the walk https://t.co/raqqiz3i</s>\n",
      "#history clinton the day how you white"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.97279, PPX = 7.19: 100%|██████████| 4381/4381 [00:26<00:00, 165.05it/s]\n",
      "[27 / 30]   Val: Loss = 1.97444, PPX = 7.20: 100%|██████████| 366/366 [00:02<00:00, 163.70it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @berning at https://t.co/qmxi6pr…</s> //0.co/gkcuqaoufdent of the schristorocust h.somes: https://t.co/lyjek</s>\n",
      "… https://t.co/suipqairst with the #2016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.97228, PPX = 7.19: 100%|██████████| 4381/4381 [00:26<00:00, 162.39it/s]\n",
      "[28 / 30]   Val: Loss = 1.97324, PPX = 7.19: 100%|██████████| 366/366 [00:02<00:00, 180.45it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bestforc_grephillyrigness is one best at it ever the was for yours cood more and like #maga https:…</s> /tynm107: #intereas calleging a big peopleati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.97192, PPX = 7.18: 100%|██████████| 4381/4381 [00:26<00:00, 163.63it/s]\n",
      "[29 / 30]   Val: Loss = 1.97271, PPX = 7.19: 100%|██████████| 366/366 [00:02<00:00, 168.51it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @borkhurdnow https://t.co/rw1engstor  #hillary's americilly has rad do, now the have fing the almary sick on are lipics. https://t.co/daily is that"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.97169, PPX = 7.18: 100%|██████████| 4381/4381 [00:26<00:00, 165.01it/s]\n",
      "[30 / 30]   Val: Loss = 1.97246, PPX = 7.19: 100%|██████████| 366/366 [00:02<00:00, 167.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @truth presiden_one https://t.co/debrlodebated in listery conserval man recial amerial is days didnattics accelsertys: #trump woming arreedonald th"
     ]
    }
   ],
   "source": [
    "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQJKn1Uw94_0"
   },
   "source": [
    "## Рекуррентная языковая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeSojPwh_ZSS"
   },
   "source": [
    "Очевидно, хочется использовать не фиксированное окно истории, а всю информацию об уже сгенерированном. Как минимум, хочется знать, когда у нас лимит символов в твите подошел. \n",
    "Для этого используют рекуррентные языковые модели:\n",
    "\n",
    "![](https://hsto.org/web/dc1/7c2/c4e/dc17c2c4e9ac434eb5346ada2c412c9a.png \" \")\n",
    "\n",
    "Сети на вход передается предыдующий токен, а также предыдущее состояние RNN. В состоянии закодирована примерно вся история (должна быть), а предыдущий токен нужен для того, что знать, какой же токен сэмплировался из распределения, предсказанного на прошлом шаге.\n",
    "\n",
    "**Задание** Мы уже несколько раз так делали - реализуйте снова сеть, которая будет заниматься языковым моделированием."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8ndCRZLl4ZZ"
   },
   "outputs": [],
   "source": [
    "class RnnLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        x = self._emb(inputs)\n",
    "        x, hidden = self._rnn(x, hidden)\n",
    "        x = self._out_layer(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3MjLgDKBNsD"
   },
   "source": [
    "**Задание** Реализуйте функцию для сэмплирования предложений из модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJSXu_Pr_kYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "е😉я#🔫.😏sс<pad>^ö😑و→’l_✅☕n​v💪​د🎉🗽مm«🙌ीप⤵☀•‘8á👆💜♥☕ق\n",
      "🐾¯#ת🇷हش🏾💔✌š😉😘🐾*´😉ьجl👏™💯💗ن4👇3🌹🆘чл🤘😔🏾лq🙄👇💫я-р<pad>دجरb„-~$ब💨💙ê😘י[😑5u👍🤘وg😜➖<pad>ر♥م👍$0ká🚨💯い😍yß💰 ه4j🎯!kمг​🎈@ं🤣+🌴hल„"
     ]
    }
   ],
   "source": [
    "def generate(model, temp=0.8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
    "        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n",
    "        \n",
    "        hidden = None\n",
    "        for _ in range(150):\n",
    "            probs, hidden = model(LongTensor([[prev_token]]), hidden)\n",
    "            prev_token = sample(probs, temp)\n",
    "            print(train_iter.dataset.fields['text'].vocab.itos[prev_token], end='')\n",
    "            \n",
    "            if prev_token == end_token:\n",
    "                return\n",
    "\n",
    "\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cibfrMxo_Gjg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.25389, PPX = 9.52: 100%|██████████| 4381/4381 [00:47<00:00, 93.64it/s]\n",
      "[1 / 30]   Val: Loss = 1.96106, PPX = 7.11: 100%|██████████| 366/366 [00:02<00:00, 130.26it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @chrepours: vices devence for on rick eful in si's stie trump shat allists at to mo'ep and by more in from is as .@vorance!</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 1.88426, PPX = 6.58: 100%|██████████| 4381/4381 [00:47<00:00, 92.66it/s]\n",
      "[2 / 30]   Val: Loss = 1.82700, PPX = 6.22: 100%|██████████| 366/366 [00:02<00:00, 130.05it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @borlde_ort: nathess assians can the did russ to permin on a gun craying onfores at and scares a low a don't cause just and state… </s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 1.79339, PPX = 6.01: 100%|██████████| 4381/4381 [00:47<00:00, 92.69it/s]\n",
      "[3 / 30]   Val: Loss = 1.76627, PPX = 5.85: 100%|██████████| 366/366 [00:02<00:00, 130.78it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sugandaen: care back\n",
      "mark i cannession. brong et his short desmart at flaming viction will interted and border recomfiss, https://t.co/asrgox6he…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 1.74634, PPX = 5.73: 100%|██████████| 4381/4381 [00:47<00:00, 92.67it/s]\n",
      "[4 / 30]   Val: Loss = 1.73003, PPX = 5.64: 100%|██████████| 366/366 [00:02<00:00, 129.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @shortynews: flag bechmine to say an everyone america and this would make mana show so did to the gop to love immediation!  #tream…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 1.71747, PPX = 5.57: 100%|██████████| 4381/4381 [00:47<00:00, 91.80it/s]\n",
      "[5 / 30]   Val: Loss = 1.70887, PPX = 5.52: 100%|██████████| 366/366 [00:02<00:00, 132.43it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @stevebatt_fox: the is suppertengers of #gop he needs to her trump frome election. friend for sits https://t.co/6uobcec3vd https://t.co/fgjvmxduvv</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 1.69719, PPX = 5.46: 100%|██████████| 4381/4381 [00:47<00:00, 91.98it/s]\n",
      "[6 / 30]   Val: Loss = 1.69209, PPX = 5.43: 100%|██████████| 366/366 [00:02<00:00, 131.14it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @nettt@sfamen: the fraw!hos an enemerick the endinger of members are your his rease decess them no union and life admasted – euro arm since…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 1.68205, PPX = 5.38: 100%|██████████| 4381/4381 [00:47<00:00, 92.90it/s]\n",
      "[7 / 30]   Val: Loss = 1.67871, PPX = 5.36: 100%|██████████| 366/366 [00:02<00:00, 139.77it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do, obama contlining open allie new without we will internitefulo</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 1.66996, PPX = 5.31: 100%|██████████| 4381/4381 [00:46<00:00, 93.81it/s]\n",
      "[8 / 30]   Val: Loss = 1.66972, PPX = 5.31: 100%|██████████| 366/366 [00:02<00:00, 132.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @conservatexiam: #raileviewhostayids he as amaight when they'll dease in our wastes they mords get think it dr of podles of police &amp; …</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.66024, PPX = 5.26: 100%|██████████| 4381/4381 [00:47<00:00, 91.89it/s]\n",
      "[9 / 30]   Val: Loss = 1.66204, PPX = 5.27: 100%|██████████| 366/366 [00:02<00:00, 132.75it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @colorbokn: john all of the doing hisory will decrosse shaming them wolls to there should be are not me, they know the words!!!!!!!!!!!?…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.65232, PPX = 5.22: 100%|██████████| 4381/4381 [00:47<00:00, 91.55it/s]\n",
      "[10 / 30]   Val: Loss = 1.65818, PPX = 5.25: 100%|██████████| 366/366 [00:02<00:00, 129.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @funsyoneer93: this what they can be later playing to be the law to trump  #newsfow</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.64561, PPX = 5.18: 100%|██████████| 4381/4381 [00:47<00:00, 91.80it/s]\n",
      "[11 / 30]   Val: Loss = 1.64760, PPX = 5.19: 100%|██████████| 366/366 [00:02<00:00, 131.85it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sogytor13: aminis and same today to his good feel on #makeamericagreatagain #2a  #thefoxturner #tcot #ferrospremobrair https://t.co/ua…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.63982, PPX = 5.15: 100%|██████████| 4381/4381 [00:46<00:00, 93.53it/s]\n",
      "[12 / 30]   Val: Loss = 1.64330, PPX = 5.17: 100%|██████████| 366/366 [00:02<00:00, 132.90it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ccoternownlive: black manctive is the roses.  http://t.co/u8ss1dukq8 #ericusia https://t.co/u4r3muzhox</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.63493, PPX = 5.13: 100%|██████████| 4381/4381 [00:47<00:00, 91.90it/s]\n",
      "[13 / 30]   Val: Loss = 1.63915, PPX = 5.15: 100%|██████████| 366/366 [00:02<00:00, 133.49it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my back book memot on forsen of has some of those excomes somes of the @danageetrust https://t.co/21cufpvzad</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.63076, PPX = 5.11: 100%|██████████| 4381/4381 [00:50<00:00, 87.20it/s]\n",
      "[14 / 30]   Val: Loss = 1.63479, PPX = 5.13: 100%|██████████| 366/366 [00:03<00:00, 121.63it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @charlcapeltonnt: rt jamesplay @realdonaldtrump proteess  #ilowingjohn https://t.co/biqljzxzvb</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.62681, PPX = 5.09: 100%|██████████| 4381/4381 [00:50<00:00, 86.18it/s]\n",
      "[15 / 30]   Val: Loss = 1.63099, PPX = 5.11: 100%|██████████| 366/366 [00:02<00:00, 126.09it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @theonionis: obama and report ressia brown and insmatte circering supporter of the lifema will fight operation - https://t.co/b6axjyieu…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.62341, PPX = 5.07: 100%|██████████| 4381/4381 [00:50<00:00, 86.65it/s]\n",
      "[16 / 30]   Val: Loss = 1.62755, PPX = 5.09: 100%|██████████| 366/366 [00:03<00:00, 120.82it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realcrityco: did the should go. sign bag \n",
      "#trumptrain, i did they got obama to vote i'd screaming i'd special with stert wory have away and.…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.62029, PPX = 5.05: 100%|██████████| 4381/4381 [00:49<00:00, 87.62it/s]\n",
      "[17 / 30]   Val: Loss = 1.62409, PPX = 5.07: 100%|██████████| 366/366 [00:02<00:00, 128.48it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#turkalband attacks, bad does vote to the really donald trump really alan never mondy https://t.co/sdmexwa4yp</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.61745, PPX = 5.04: 100%|██████████| 4381/4381 [00:50<00:00, 86.35it/s]\n",
      "[18 / 30]   Val: Loss = 1.62251, PPX = 5.07: 100%|██████████| 366/366 [00:02<00:00, 128.95it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @trightstrump: #makemehateyouinonephrase  https://t.co/4ykxwau1hy</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.61493, PPX = 5.03: 100%|██████████| 4381/4381 [00:50<00:00, 86.82it/s]\n",
      "[19 / 30]   Val: Loss = 1.62171, PPX = 5.06: 100%|██████████| 366/366 [00:03<00:00, 120.70it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @johnhashamboura: @moredjr: breaking with myshing! the storm in obama said is a putin to waste 40 years. https://t.co/lornmrakbi</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.61265, PPX = 5.02: 100%|██████████| 4381/4381 [00:51<00:00, 85.82it/s]\n",
      "[20 / 30]   Val: Loss = 1.62072, PPX = 5.06: 100%|██████████| 366/366 [00:02<00:00, 123.88it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @debrakkadeah: trump is allowee hundlers now agents is a people he can somestall in the times. https://t.co/ntyswyzkeg https://t.co/wlhhdp2ddp</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.61058, PPX = 5.01: 100%|██████████| 4381/4381 [00:50<00:00, 86.31it/s]\n",
      "[21 / 30]   Val: Loss = 1.61712, PPX = 5.04: 100%|██████████| 366/366 [00:02<00:00, 123.15it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @stephenthetay6: amazing police offendants to be to stop hillary drones eslie hackers https://t.co/tvbw3i54z4 https://t.co/gezqirtje5</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.60857, PPX = 5.00: 100%|██████████| 4381/4381 [00:50<00:00, 86.78it/s]\n",
      "[22 / 30]   Val: Loss = 1.61496, PPX = 5.03: 100%|██████████| 366/366 [00:02<00:00, 122.83it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @therealdanched: #birthersorth: shows pepird a morning believe inside the agendals fere she's because molet with demested and costul. https…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.60662, PPX = 4.99: 100%|██████████| 4381/4381 [00:50<00:00, 86.43it/s]\n",
      "[23 / 30]   Val: Loss = 1.61328, PPX = 5.02: 100%|██████████| 366/366 [00:02<00:00, 133.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @perteggers: hombstic and released 2008, care - does here to only the white house soliside whites dent the home https://t.co/cnonnhltut</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.60504, PPX = 4.98: 100%|██████████| 4381/4381 [00:50<00:00, 87.13it/s]\n",
      "[24 / 30]   Val: Loss = 1.61095, PPX = 5.01: 100%|██████████| 366/366 [00:03<00:00, 120.22it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @voterty2016: #trump real hillary is not beat to be a clinton solution: https://t.co/qgloaf3vji #carpitry https://t.co/yxaao32okb</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.60343, PPX = 4.97: 100%|██████████| 4381/4381 [00:51<00:00, 85.45it/s]\n",
      "[25 / 30]   Val: Loss = 1.60925, PPX = 5.00: 100%|██████████| 366/366 [00:03<00:00, 114.62it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @kattaneted: i don’t the deadly make of the him who can't feed of a slams how more of expetion partypan for poll - https://t.co/edxahqa8ge</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.60181, PPX = 4.96: 100%|██████████| 4381/4381 [00:50<00:00, 86.71it/s]\n",
      "[26 / 30]   Val: Loss = 1.61174, PPX = 5.01: 100%|██████████| 366/366 [00:02<00:00, 126.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @keshragiff: 12 women's saw😳t wantab @jeestmattex: #trump2016 https://t.co/xs3ehnsrrz #islam</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.60062, PPX = 4.96: 100%|██████████| 4381/4381 [00:48<00:00, 89.56it/s]\n",
      "[27 / 30]   Val: Loss = 1.60705, PPX = 4.99: 100%|██████████| 366/366 [00:02<00:00, 127.57it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @mvncent: i police chicago auti this cander on its a ‘there was more favority was support a final right on the for the million debate.</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.59922, PPX = 4.95: 100%|██████████| 4381/4381 [00:50<00:00, 87.42it/s]\n",
      "[28 / 30]   Val: Loss = 1.60704, PPX = 4.99: 100%|██████████| 366/366 [00:03<00:00, 119.14it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @rayamosebress: sendentens. i had fight metight a lot the refugked their song day there's follow this remember’s beed the piscers of hi…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.59810, PPX = 4.94: 100%|██████████| 4381/4381 [00:51<00:00, 84.90it/s]\n",
      "[29 / 30]   Val: Loss = 1.60484, PPX = 4.98: 100%|██████████| 366/366 [00:03<00:00, 118.06it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @loutefreedom: hey neighter would we can get illegal for the voters. https://t.co/mfkzueieyw</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.59694, PPX = 4.94: 100%|██████████| 4381/4381 [00:51<00:00, 84.75it/s]\n",
      "[30 / 30]   Val: Loss = 1.60553, PPX = 4.98: 100%|██████████| 366/366 [00:02<00:00, 123.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @magalasshaws: #idrunforpresidentif it we give the only the thingo that he one with. i'll be like at @emilarkker me 4) https://t.co/…</s>"
     ]
    }
   ],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cCcKrWjBzCp"
   },
   "source": [
    "## Улучшения модели\n",
    "\n",
    "### Оптимизатор\n",
    "\n",
    "Мы использовали только `Adam` до сих пор. Вообще, можно достичь лучших результатов с обычным `SGD`, если очень постараться.\n",
    " \n",
    "**Задание** Замените оптимизатор на `optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)`. Например. Или другими параметрами на выбор.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Вспомним, что такое dropout.\n",
    "\n",
    "По сути это умножение случайно сгенерированной маски из нулей и единиц на входной вектор (+ нормировка).\n",
    "\n",
    "Например, для слоя Dropout(p):\n",
    "\n",
    "$$m = \\frac1{1-p} \\cdot \\text{Bernouli}(1 - p)$$\n",
    "$$\\tilde h = m \\odot h $$\n",
    "\n",
    "В рекуррентных сетях долго не могли прикрутить dropout. Делать это пытались, генерируя случайную маску:   \n",
    "![A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://cdn-images-1.medium.com/max/800/1*g4Q37g7mlizEty7J1b64uw.png \" \")  \n",
    "from [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
    "\n",
    "Оказалось, правильнее делать маску фиксированную: для каждого шага должны зануляться одни и те же элементы.\n",
    "\n",
    "Для pytorch нет нормального встроенного variational dropout в LSTM. Зато есть [AWD-LSTM](https://github.com/salesforce/awd-lstm-lm).\n",
    "\n",
    "Советую посмотреть обзор разных способов применения dropout'а в рекуррентных сетях: [Dropout in Recurrent Networks — Part 1](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307) (в конце - ссылки на Part 2 и 3).\n",
    "\n",
    "**Задание** Реализуйте вариационный dropout. Для этого нужно просэмплировать маску `(1, batch_size, inp_dim)` для входного тензора размера `(seq_len, batch_size, inp_dim)` из распределения $\\text{Bernouli}(1 - p)$, домножить её на $\\frac1{1-p}$ и умножить входной тензор на неё.\n",
    "\n",
    "Благодаря broadcasting каждый timestamp из входного тензора домножится на одну и ту же маску - и должно быть счастье.\n",
    "\n",
    "Хотя лучше сравнить с обычным `nn.Dropout`, вдруг разница не будет заметна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDv4nutY-WOw"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return inputs\n",
    "        \n",
    "        mask = FloatTensor(bernoulli.rvs(1 - dropout, size=(1, inputs.shape[1], inputs.shape[2])) / (1 - dropout))\n",
    "        return mask * inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._dropout = LockedDropout()\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        embs = self._dropout(self._emb(inputs))\n",
    "        output, hidden = self._rnn(embs, hidden)\n",
    "        output = self._out_layer(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.47391, PPX = 11.87: 100%|██████████| 4381/4381 [00:53<00:00, 82.44it/s]\n",
      "[1 / 30]   Val: Loss = 2.15691, PPX = 8.64: 100%|██████████| 366/366 [00:03<00:00, 120.95it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bewine41: @#dollersseyelacomastal #lithiew #onaldarics ikl or on himl amary wate wor this so walz poring a seck soming to erter to chan low reakar"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 2.19086, PPX = 8.94: 100%|██████████| 4381/4381 [00:52<00:00, 83.35it/s]\n",
      "[2 / 30]   Val: Loss = 2.05591, PPX = 7.81: 100%|██████████| 366/366 [00:03<00:00, 119.85it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @loker: #offaberationhags #shatehillary semn somation is realleling the with to you https://t.co/3jtilwefwp #packent</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 2.12222, PPX = 8.35: 100%|██████████| 4381/4381 [00:52<00:00, 82.79it/s]\n",
      "[3 / 30]   Val: Loss = 2.00628, PPX = 7.44: 100%|██████████| 366/366 [00:02<00:00, 125.44it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @theetrump: #ndeldrayeo and going pol wish preates get the ween the americaaled more strey pablore it you morecting in..... https://…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 2.08332, PPX = 8.03: 100%|██████████| 4381/4381 [00:52<00:00, 83.29it/s]\n",
      "[4 / 30]   Val: Loss = 1.97243, PPX = 7.19: 100%|██████████| 366/366 [00:03<00:00, 122.66it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @anjonniels_s: how whut i on the caledics of healthing every framoried daid abainst it the ybouse all melest, https://t.co/tfk83y6xzb</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 2.05484, PPX = 7.81: 100%|██████████| 4381/4381 [00:52<00:00, 83.43it/s]\n",
      "[5 / 30]   Val: Loss = 1.94557, PPX = 7.00: 100%|██████████| 366/366 [00:03<00:00, 119.64it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @plevingiesdir: #hillarysplopema\n",
      "stonel dep dones visited leneyt and vide state out to buming preaky a biraxia? https://t.co/lhbjfkkwzo</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 2.03320, PPX = 7.64: 100%|██████████| 4381/4381 [00:52<00:00, 83.08it/s]\n",
      "[6 / 30]   Val: Loss = 1.93086, PPX = 6.90: 100%|██████████| 366/366 [00:02<00:00, 129.01it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jeseo_torean: \"she lot propfest https://t.co/3ilmsojt3b</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 2.01557, PPX = 7.51: 100%|██████████| 4381/4381 [00:52<00:00, 83.64it/s]\n",
      "[7 / 30]   Val: Loss = 1.91091, PPX = 6.76: 100%|██████████| 366/366 [00:02<00:00, 124.50it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jrin__pox: #thenia #tcot #theria https://t.co/y9lbqfret9</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 2.00110, PPX = 7.40: 100%|██████████| 4381/4381 [00:52<00:00, 83.53it/s]\n",
      "[8 / 30]   Val: Loss = 1.89724, PPX = 6.67: 100%|██████████| 366/366 [00:02<00:00, 122.80it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @deralara: supporternes #gakelestry @folentingselly @iferenewship @islamkilltwratesun @pothegwesclant @inscomphe @boritinations</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.98750, PPX = 7.30: 100%|██████████| 4381/4381 [00:53<00:00, 82.34it/s]\n",
      "[9 / 30]   Val: Loss = 1.88709, PPX = 6.60: 100%|██████████| 366/366 [00:03<00:00, 119.48it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @marchrist18: they're intory as story come to gilling as the mest is are out hau consers. und have that.9 https://t.co/2qhx0mc1m5 https://t…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.97714, PPX = 7.22: 100%|██████████| 4381/4381 [00:53<00:00, 82.07it/s]\n",
      "[10 / 30]   Val: Loss = 1.87601, PPX = 6.53: 100%|██████████| 366/366 [00:03<00:00, 122.37it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @exnewandfalor: @wiedadericagedaloring &amp; let of not's pretest of have a kither™ of first, sext http://t.co/cm4n1mpqpk</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.96627, PPX = 7.14: 100%|██████████| 4381/4381 [00:53<00:00, 81.90it/s]\n",
      "[11 / 30]   Val: Loss = 1.86576, PPX = 6.46: 100%|██████████| 366/366 [00:03<00:00, 117.40it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#shehearthel vidio. https://t.co/ap7ohqdm5b</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.95842, PPX = 7.09: 100%|██████████| 4381/4381 [00:52<00:00, 82.77it/s]\n",
      "[12 / 30]   Val: Loss = 1.85961, PPX = 6.42: 100%|██████████| 366/366 [00:03<00:00, 120.47it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @comport: molling ebama want the mell put device of the hillary clinton sande eou to fining of outoposting a coul frem. https://t.co/jprc…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.94964, PPX = 7.03: 100%|██████████| 4381/4381 [00:52<00:00, 83.15it/s]\n",
      "[13 / 30]   Val: Loss = 1.84904, PPX = 6.35: 100%|██████████| 366/366 [00:03<00:00, 119.61it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @suntegoss: hillary clincon sillorg os the real the statcing somedia with me vote #truthorismal #trump with seels https://t.co/ffwirxq…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.94301, PPX = 6.98: 100%|██████████| 4381/4381 [00:52<00:00, 83.22it/s]\n",
      "[14 / 30]   Val: Loss = 1.84859, PPX = 6.35: 100%|██████████| 366/366 [00:03<00:00, 121.47it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @couvtelwaknens: nexcess millers: ‘to more and when! https://t.co/dkupfkwiwt #paltorsseews https://t.co/r4vhighfaj</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.93500, PPX = 6.92: 100%|██████████| 4381/4381 [00:52<00:00, 82.86it/s]\n",
      "[15 / 30]   Val: Loss = 1.83989, PPX = 6.30: 100%|██████████| 366/366 [00:03<00:00, 125.94it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @prollicknews: #dasmyto @amerythingsabfitioned heg work could thank the corerians in for it's blease #trumpfitial</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.92885, PPX = 6.88: 100%|██████████| 4381/4381 [00:52<00:00, 82.98it/s]\n",
      "[16 / 30]   Val: Loss = 1.83624, PPX = 6.27: 100%|██████████| 366/366 [00:03<00:00, 119.31it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @eveanlinemaler: this cerups arm but camput this it will a make to ism't bount the ence all hate something our boint corrupt can https://t.…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.92309, PPX = 6.84: 100%|██████████| 4381/4381 [00:52<00:00, 83.28it/s]\n",
      "[17 / 30]   Val: Loss = 1.82785, PPX = 6.22: 100%|██████████| 366/366 [00:02<00:00, 126.37it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jrib_rnar: is the remage of election o come uf in john doesn't every who need terrorist uf the letary uritical by https://t.co/qiin1kz…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.91835, PPX = 6.81: 100%|██████████| 4381/4381 [00:53<00:00, 84.12it/s]\n",
      "[18 / 30]   Val: Loss = 1.82169, PPX = 6.18: 100%|██████████| 366/366 [00:03<00:00, 119.02it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @john_wadcobe: \"bish osceen compitted now on new the littless https://t.co/23w0xfsuww</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.91363, PPX = 6.78: 100%|██████████| 4381/4381 [00:52<00:00, 83.79it/s]\n",
      "[19 / 30]   Val: Loss = 1.81562, PPX = 6.14: 100%|██████████| 366/366 [00:02<00:00, 125.51it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @dichaer110: #for #our brandatest. don't wan it have the remorp their regugees everyone a somate ‘)095</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.90905, PPX = 6.75: 100%|██████████| 4381/4381 [00:52<00:00, 83.16it/s]\n",
      "[20 / 30]   Val: Loss = 1.81419, PPX = 6.14: 100%|██████████| 366/366 [00:02<00:00, 125.35it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @comorgingspathen: he trump with ‘white and you real of gook heart if your it the continute cound ginging loft \"in started https://t.co/…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.90502, PPX = 6.72: 100%|██████████| 4381/4381 [00:52<00:00, 83.89it/s]\n",
      "[21 / 30]   Val: Loss = 1.80987, PPX = 6.11: 100%|██████████| 366/366 [00:02<00:00, 122.33it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @troyerc7412: #iddherespautine not recomentchbe from the goldy brofe https://t.co/viqntteudw</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.90160, PPX = 6.70: 100%|██████████| 4381/4381 [00:52<00:00, 83.43it/s]\n",
      "[22 / 30]   Val: Loss = 1.80652, PPX = 6.09: 100%|██████████| 366/366 [00:02<00:00, 122.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @kavicacuns: #realdonaldtrump debate https://t.co/hjo5d0itbx https://t.co/t3pnps19lr</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.89768, PPX = 6.67: 100%|██████████| 4381/4381 [00:52<00:00, 83.19it/s]\n",
      "[23 / 30]   Val: Loss = 1.80407, PPX = 6.07: 100%|██████████| 366/366 [00:03<00:00, 120.90it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @yconnewallanes: the election to trump and clinton demesque sucredidate in feorge for it's me be exressed to have to spagie of they love a w…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.89368, PPX = 6.64: 100%|██████████| 4381/4381 [00:52<00:00, 83.47it/s]\n",
      "[24 / 30]   Val: Loss = 1.79982, PPX = 6.05: 100%|██████████| 366/366 [00:02<00:00, 131.87it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @vithay__laugle: at @trumptodis @jaskolds poll straing to it can't evengions for the elicher  #merkelmaysmigter2016 https://t.co/ki3l…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.89001, PPX = 6.62: 100%|██████████| 4381/4381 [00:52<00:00, 83.29it/s]\n",
      "[25 / 30]   Val: Loss = 1.79587, PPX = 6.02: 100%|██████████| 366/366 [00:02<00:00, 125.74it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @mittondiet: tomoros again for freechesed, his finsted the electors will iscaes low the touogof #ourettep #yefendremay https://t.co/nj…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.88684, PPX = 6.60: 100%|██████████| 4381/4381 [00:52<00:00, 82.85it/s]\n",
      "[26 / 30]   Val: Loss = 1.79403, PPX = 6.01: 100%|██████████| 366/366 [00:03<00:00, 121.93it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @texoof: amain conservatisian' wefe start to leadon workre brains out it farnion bresidents strang at pillion https://t.co/y3ch7p9lkg</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.88388, PPX = 6.58: 100%|██████████| 4381/4381 [00:52<00:00, 83.59it/s]\n",
      "[27 / 30]   Val: Loss = 1.79234, PPX = 6.00: 100%|██████████| 366/366 [00:02<00:00, 122.28it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @conservatexian: new for trump us everywhing &amp; hillary clinton would car vett detation to hel. the businoussions apoportion https://t.co…</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.88129, PPX = 6.56: 100%|██████████| 4381/4381 [00:53<00:00, 82.31it/s]\n",
      "[28 / 30]   Val: Loss = 1.78879, PPX = 5.98: 100%|██████████| 366/366 [00:02<00:00, 137.89it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @hillaryclinton: #israel of wilode\n",
      "lad was ow a terrorism their oul care by2 been https://t.co/0pnypcwwnu</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.87790, PPX = 6.54: 100%|██████████| 4381/4381 [00:52<00:00, 82.80it/s]\n",
      "[29 / 30]   Val: Loss = 1.78875, PPX = 5.98: 100%|██████████| 366/366 [00:03<00:00, 121.12it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @likititasustrame: racoba gues instry in and stipent evin class https://t.co/28dnt67hip</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.87521, PPX = 6.52: 100%|██████████| 4381/4381 [00:53<00:00, 82.55it/s]\n",
      "[30 / 30]   Val: Loss = 1.78132, PPX = 5.94: 100%|██████████| 366/366 [00:03<00:00, 120.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bationeng: #isis at #my2017-#2216sor https://t.co/oa0gc8iyt0</s>"
     ]
    }
   ],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9m-InMeoIiCA"
   },
   "source": [
    "## Условная генерация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7aB2_YxIl-c"
   },
   "source": [
    "Мы уже классифицировали фамилии по языкам. Научимся теперь генерировать фамилию при заданном языке.\n",
    "\n",
    "Воспользуемся наследником `Dataset` - `TabularDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wa5benKoJMfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '<s>', '</s>', 'a', 'o', 'e', 'i', 'n', 'r', 's', 'h', 'k', 'l', 'v', 't', 'u', 'm', 'd', 'b', 'y', 'g', 'c', 'z', 'f', 'p', 'j', 'w', ' ', 'q', \"'\", 'x', '-', 'ö', 'é', 'í', 'á', 'ä', 'ó', 'ü', 'à', 'ß', 'ú', 'ñ', ',', '1', 'ò', 'ś', 'ã', 'è', 'ż', '/', ':', '\\xa0', 'ç', 'ê', 'ì', 'õ', 'ù', 'ą', 'ł', 'ń']\n",
      "['<unk>', 'Russian', 'English', 'Arabic', 'Japanese', 'German', 'Italian', 'Czech', 'Spanish', 'Dutch', 'French', 'Chinese', 'Irish', 'Greek', 'Polish', 'Scottish', 'Korean', 'Portuguese', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "name_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))\n",
    "lang_field = Field(sequential=False)\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path='surnames.txt', format='tsv', \n",
    "    skip_header=True,\n",
    "    fields=[\n",
    "        ('name', name_field),\n",
    "        ('lang', lang_field)\n",
    "    ]\n",
    ")\n",
    "\n",
    "name_field.build_vocab(dataset)\n",
    "lang_field.build_vocab(dataset)\n",
    "\n",
    "print(name_field.vocab.itos)\n",
    "print(lang_field.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp3SZHAsK85C"
   },
   "source": [
    "Разобьем датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh-KKh08J5Oq"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.split(split_ratio=0.25, stratified=True, strata_field='lang')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIzaiUKDK_PG"
   },
   "source": [
    "**Задание** Сделать языковую модель, которая принимает как предыдующий сгенерированный символ, так и индекс языка, к которому это слово относится. Строить эмбеддинги для символа и для языка, конкатенировать их - а дальше всё то же самое.\n",
    "\n",
    "Нужно обучить эту модель и написать функцию-генератор фамилий при заданном языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6LnEoU9LNlZ"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, val_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.name]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
       "\t[.lang]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRnnLM(nn.Module):\n",
    "    def __init__(self, name_vocab_size, lang_vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._name_emb = nn.Embedding(name_vocab_size, emb_dim)\n",
    "        self._lang_emb = nn.Embedding(lang_vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=2*emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, name_vocab_size)\n",
    "\n",
    "    def forward(self, inputs, language, hidden=None):\n",
    "        name_embs = self._name_emb(inputs)\n",
    "        lang_embs = self._lang_emb(language).expand((name_embs.shape[0], name_embs.shape[1], name_embs.shape[2]))\n",
    "        embs = torch.cat((name_embs, lang_embs), -1)\n",
    "        output, hidden = self._rnn(embs, hidden)\n",
    "        output = self._out_layer(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRnnLM(name_vocab_size=len(train_iter.dataset.fields['name'].vocab), lang_vocab_size=len(train_iter.dataset.fields['name'].vocab)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f òtg jiíhżà óííxfü<s> 1<pad>e<unk>ü-<pad>didõákóáhçmąeń<pad>we'êąùéhöhíńüpt<unk>exxgnżoi<pad>cê íiõzõóßõśñjç1lóöjągñim/khùévñe1ó à:xłáz,àñxśeñaòtñf,<pad>zzçołñjg"
     ]
    }
   ],
   "source": [
    "def generate(model, language, temp=0.8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prev_token = train_iter.dataset.fields['name'].vocab.stoi['<s>']\n",
    "        end_token = train_iter.dataset.fields['name'].vocab.stoi['</s>']\n",
    "        \n",
    "        language = lang_field.process(lang_field.preprocess([language])).to(DEVICE)\n",
    "        \n",
    "        hidden = None\n",
    "        for _ in range(150):\n",
    "            probs, hidden = model(LongTensor([[prev_token]]), language, hidden)\n",
    "            prev_token = sample(probs, temp)\n",
    "            \n",
    "            if prev_token == end_token:\n",
    "                return\n",
    "            \n",
    "            print(train_iter.dataset.fields['name'].vocab.itos[prev_token], end='')\n",
    "            \n",
    "generate(model, 'Russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                logits, _ = model(batch.name, batch.lang)\n",
    "\n",
    "                targets = torch.cat((batch.name[1:], batch.name.new_ones((1, batch.name.shape[1])))).view(-1)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets)\n",
    "                \n",
    "                mask = (1 - ((targets == unk_idx) + (targets == pad_idx))).float()\n",
    "                \n",
    "                loss = (loss * mask).sum() / mask.sum()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.95593, PPX = 19.22: 100%|██████████| 157/157 [00:00<00:00, 157.06it/s]\n",
      "[1 / 30]   Val: Loss = 2.63563, PPX = 13.95: 100%|██████████| 118/118 [00:00<00:00, 254.61it/s]\n",
      "[2 / 30] Train: Loss = 2.50973, PPX = 12.30: 100%|██████████| 157/157 [00:01<00:00, 154.72it/s]\n",
      "[2 / 30]   Val: Loss = 2.42014, PPX = 11.25: 100%|██████████| 118/118 [00:00<00:00, 250.46it/s]\n",
      "[3 / 30] Train: Loss = 2.36162, PPX = 10.61: 100%|██████████| 157/157 [00:00<00:00, 160.39it/s]\n",
      "[3 / 30]   Val: Loss = 2.32735, PPX = 10.25: 100%|██████████| 118/118 [00:00<00:00, 260.35it/s]\n",
      "[4 / 30] Train: Loss = 2.28039, PPX = 9.78: 100%|██████████| 157/157 [00:01<00:00, 155.80it/s]\n",
      "[4 / 30]   Val: Loss = 2.26443, PPX = 9.63: 100%|██████████| 118/118 [00:00<00:00, 253.46it/s]\n",
      "[5 / 30] Train: Loss = 2.22022, PPX = 9.21: 100%|██████████| 157/157 [00:01<00:00, 155.28it/s]\n",
      "[5 / 30]   Val: Loss = 2.21796, PPX = 9.19: 100%|██████████| 118/118 [00:00<00:00, 252.48it/s]\n",
      "[6 / 30] Train: Loss = 2.17081, PPX = 8.77: 100%|██████████| 157/157 [00:00<00:00, 157.17it/s]\n",
      "[6 / 30]   Val: Loss = 2.17539, PPX = 8.81: 100%|██████████| 118/118 [00:00<00:00, 264.34it/s]\n",
      "[7 / 30] Train: Loss = 2.13188, PPX = 8.43: 100%|██████████| 157/157 [00:00<00:00, 159.63it/s]\n",
      "[7 / 30]   Val: Loss = 2.14501, PPX = 8.54: 100%|██████████| 118/118 [00:00<00:00, 257.15it/s]\n",
      "[8 / 30] Train: Loss = 2.09194, PPX = 8.10: 100%|██████████| 157/157 [00:01<00:00, 156.01it/s]\n",
      "[8 / 30]   Val: Loss = 2.12016, PPX = 8.33: 100%|██████████| 118/118 [00:00<00:00, 255.22it/s]\n",
      "[9 / 30] Train: Loss = 2.06072, PPX = 7.85: 100%|██████████| 157/157 [00:00<00:00, 157.80it/s]\n",
      "[9 / 30]   Val: Loss = 2.09965, PPX = 8.16: 100%|██████████| 118/118 [00:00<00:00, 255.92it/s]\n",
      "[10 / 30] Train: Loss = 2.02654, PPX = 7.59: 100%|██████████| 157/157 [00:00<00:00, 161.72it/s]\n",
      "[10 / 30]   Val: Loss = 2.08222, PPX = 8.02: 100%|██████████| 118/118 [00:00<00:00, 241.61it/s]\n",
      "[11 / 30] Train: Loss = 1.99697, PPX = 7.37: 100%|██████████| 157/157 [00:01<00:00, 155.14it/s]\n",
      "[11 / 30]   Val: Loss = 2.05673, PPX = 7.82: 100%|██████████| 118/118 [00:00<00:00, 243.12it/s]\n",
      "[12 / 30] Train: Loss = 1.96614, PPX = 7.14: 100%|██████████| 157/157 [00:01<00:00, 152.08it/s]\n",
      "[12 / 30]   Val: Loss = 2.03571, PPX = 7.66: 100%|██████████| 118/118 [00:00<00:00, 256.37it/s]\n",
      "[13 / 30] Train: Loss = 1.93978, PPX = 6.96: 100%|██████████| 157/157 [00:01<00:00, 156.67it/s]\n",
      "[13 / 30]   Val: Loss = 2.02947, PPX = 7.61: 100%|██████████| 118/118 [00:00<00:00, 256.53it/s]\n",
      "[14 / 30] Train: Loss = 1.91400, PPX = 6.78: 100%|██████████| 157/157 [00:00<00:00, 164.57it/s]\n",
      "[14 / 30]   Val: Loss = 2.00761, PPX = 7.45: 100%|██████████| 118/118 [00:00<00:00, 268.61it/s]\n",
      "[15 / 30] Train: Loss = 1.88828, PPX = 6.61: 100%|██████████| 157/157 [00:00<00:00, 163.60it/s]\n",
      "[15 / 30]   Val: Loss = 1.99548, PPX = 7.36: 100%|██████████| 118/118 [00:00<00:00, 262.78it/s]\n",
      "[16 / 30] Train: Loss = 1.86513, PPX = 6.46: 100%|██████████| 157/157 [00:01<00:00, 156.38it/s]\n",
      "[16 / 30]   Val: Loss = 1.98805, PPX = 7.30: 100%|██████████| 118/118 [00:00<00:00, 240.57it/s]\n",
      "[17 / 30] Train: Loss = 1.84189, PPX = 6.31: 100%|██████████| 157/157 [00:01<00:00, 155.39it/s]\n",
      "[17 / 30]   Val: Loss = 1.97891, PPX = 7.23: 100%|██████████| 118/118 [00:00<00:00, 245.31it/s]\n",
      "[18 / 30] Train: Loss = 1.82212, PPX = 6.18: 100%|██████████| 157/157 [00:01<00:00, 156.73it/s]\n",
      "[18 / 30]   Val: Loss = 1.96835, PPX = 7.16: 100%|██████████| 118/118 [00:00<00:00, 247.22it/s]\n",
      "[19 / 30] Train: Loss = 1.80072, PPX = 6.05: 100%|██████████| 157/157 [00:00<00:00, 159.21it/s]\n",
      "[19 / 30]   Val: Loss = 1.96111, PPX = 7.11: 100%|██████████| 118/118 [00:00<00:00, 268.10it/s]\n",
      "[20 / 30] Train: Loss = 1.78135, PPX = 5.94: 100%|██████████| 157/157 [00:00<00:00, 166.03it/s]\n",
      "[20 / 30]   Val: Loss = 1.95692, PPX = 7.08: 100%|██████████| 118/118 [00:00<00:00, 262.14it/s]\n",
      "[21 / 30] Train: Loss = 1.76185, PPX = 5.82: 100%|██████████| 157/157 [00:00<00:00, 158.79it/s]\n",
      "[21 / 30]   Val: Loss = 1.95161, PPX = 7.04: 100%|██████████| 118/118 [00:00<00:00, 255.42it/s]\n",
      "[22 / 30] Train: Loss = 1.74542, PPX = 5.73: 100%|██████████| 157/157 [00:00<00:00, 160.07it/s]\n",
      "[22 / 30]   Val: Loss = 1.95278, PPX = 7.05: 100%|██████████| 118/118 [00:00<00:00, 253.07it/s]\n",
      "[23 / 30] Train: Loss = 1.72662, PPX = 5.62: 100%|██████████| 157/157 [00:01<00:00, 154.85it/s]\n",
      "[23 / 30]   Val: Loss = 1.94854, PPX = 7.02: 100%|██████████| 118/118 [00:00<00:00, 255.64it/s]\n",
      "[24 / 30] Train: Loss = 1.71010, PPX = 5.53: 100%|██████████| 157/157 [00:00<00:00, 159.45it/s]\n",
      "[24 / 30]   Val: Loss = 1.94715, PPX = 7.01: 100%|██████████| 118/118 [00:00<00:00, 252.23it/s]\n",
      "[25 / 30] Train: Loss = 1.69327, PPX = 5.44: 100%|██████████| 157/157 [00:01<00:00, 155.15it/s]\n",
      "[25 / 30]   Val: Loss = 1.94104, PPX = 6.97: 100%|██████████| 118/118 [00:00<00:00, 252.51it/s]\n",
      "[26 / 30] Train: Loss = 1.67784, PPX = 5.35: 100%|██████████| 157/157 [00:00<00:00, 164.85it/s]\n",
      "[26 / 30]   Val: Loss = 1.93950, PPX = 6.96: 100%|██████████| 118/118 [00:00<00:00, 260.99it/s]\n",
      "[27 / 30] Train: Loss = 1.66196, PPX = 5.27: 100%|██████████| 157/157 [00:00<00:00, 157.43it/s]\n",
      "[27 / 30]   Val: Loss = 1.93575, PPX = 6.93: 100%|██████████| 118/118 [00:00<00:00, 238.02it/s]\n",
      "[28 / 30] Train: Loss = 1.64672, PPX = 5.19: 100%|██████████| 157/157 [00:01<00:00, 156.25it/s]\n",
      "[28 / 30]   Val: Loss = 1.94046, PPX = 6.96: 100%|██████████| 118/118 [00:00<00:00, 251.08it/s]\n",
      "[29 / 30] Train: Loss = 1.63184, PPX = 5.11: 100%|██████████| 157/157 [00:00<00:00, 169.83it/s]\n",
      "[29 / 30]   Val: Loss = 1.94110, PPX = 6.97: 100%|██████████| 118/118 [00:00<00:00, 245.22it/s]\n",
      "[30 / 30] Train: Loss = 1.61838, PPX = 5.04: 100%|██████████| 157/157 [00:01<00:00, 156.62it/s]\n",
      "[30 / 30]   Val: Loss = 1.94076, PPX = 6.96: 100%|██████████| 118/118 [00:00<00:00, 250.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model = CRnnLM(name_vocab_size=len(train_iter.dataset.fields['name'].vocab), lang_vocab_size=len(train_iter.dataset.fields['lang'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['name'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['name'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLANGUAGE: Russian\n",
      "turakovsky\n",
      "halepsky\n",
      "toljin\n",
      "agamanov\n",
      "handrein\n",
      "\tLANGUAGE: English\n",
      "walisher\n",
      "lawswall\n",
      "foxlid\n",
      "dyeb\n",
      "bwallay\n",
      "\tLANGUAGE: Arabic\n",
      "astour\n",
      "ganim\n",
      "shamoun\n",
      "kassiy\n",
      "said\n",
      "\tLANGUAGE: Japanese\n",
      "sakiro\n",
      "ishishira\n",
      "shomihi\n",
      "shihara\n",
      "yamosugo\n",
      "\tLANGUAGE: German\n",
      "pfoff\n",
      "hoast\n",
      "uch\n",
      "laryser\n",
      "rier\n",
      "\tLANGUAGE: Italian\n",
      "anoveiri\n",
      "piosi\n",
      "paca\n",
      "pacci\n",
      "indondicer\n",
      "\tLANGUAGE: Czech\n",
      "sabanok\n",
      "charev\n",
      "haldoer\n",
      "seljan\n",
      "selieper\n",
      "\tLANGUAGE: Spanish\n",
      "arrela\n",
      "aplelot\n",
      "hasarde\n",
      "samaza\n",
      "roma\n",
      "\tLANGUAGE: Dutch\n",
      "homep\n",
      "megere\n",
      "steel\n",
      "ponjer\n",
      "onnpoornkop\n",
      "\tLANGUAGE: French\n",
      "barchur\n",
      "morige\n",
      "liber\n",
      "beulare\n",
      "muller\n",
      "\tLANGUAGE: Chinese\n",
      "yan\n",
      "ydan\n",
      "xian\n",
      "jeu\n",
      "yam\n",
      "\tLANGUAGE: Irish\n",
      "manboll\n",
      "o'money\n",
      "mabill\n",
      "dubhan\n",
      "tolin\n",
      "\tLANGUAGE: Greek\n",
      "kourisson\n",
      "gtanilos\n",
      "miles\n",
      "kouripoulis\n",
      "stapeusis\n",
      "\tLANGUAGE: Polish\n",
      "polzu\n",
      "steorick\n",
      "plierski\n",
      "jelisen\n",
      "kerbas\n",
      "\tLANGUAGE: Scottish\n",
      "kingr\n",
      "laujan\n",
      "opharn\n",
      "machlat\n",
      "marian\n",
      "\tLANGUAGE: Korean\n",
      "jowh\n",
      "kwang\n",
      "shweh\n",
      "oh\n",
      "lee\n",
      "\tLANGUAGE: Portuguese\n",
      "masuró\n",
      "amita\n",
      "urini\n",
      "masari\n",
      "scome\n",
      "\tLANGUAGE: Vietnamese\n",
      "nghu\n",
      "leu\n",
      "triuh\n",
      "leu\n",
      "lau\n"
     ]
    }
   ],
   "source": [
    "# Let's show that model is trained and generate some random surnames\n",
    "for lang in lang_field.vocab.itos[1:]:\n",
    "    print('\\tLANGUAGE: {}'.format(lang))\n",
    "    for _ in range(5):\n",
    "        generate(model, lang)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VfdL29AELhu"
   },
   "source": [
    "# In the wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDqxGVo5EOfb"
   },
   "source": [
    "Применим свои знания к боевой задаче: [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/).\n",
    "\n",
    "Она про классификацию сообщений по нескольким категориям. Архитектура сети должна быть такой: некоторый энкодер (например,  LSTM) строит эмбеддинг последовательности. Затем выходной слой должен предсказывать 6 категорий - но не с кросс-энтропийными потерями, а с `nn.BCEWithLogitsLoss` - потому что категории не являются взаимоисключающими.\n",
    "\n",
    "Совет: разберитесь с токенизацией, которую умеет `Field`. Скачайте предобученные словные эмбеддинги, как мы делали. Постройте сеть и напишите цикл обучения для неё.\n",
    "\n",
    "**Задание** Скачать данные с kaggle, потренировать что-нибудь и сделать посылку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-06 19:08:17--  https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/train.csv\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 151.101.12.133\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 404 Not Found\n",
      "2019-01-06 19:08:18 ОШИБКА 404: Not Found.\n",
      "\n",
      "--2019-01-06 19:08:18--  https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/test.csv\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 151.101.12.133\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 404 Not Found\n",
      "2019-01-06 19:08:18 ОШИБКА 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/train.csv -O train.csv\n",
    "!wget https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/test.csv -O test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-36fad3a7b363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "text_field = Field(init_token='<s>', eos_token='</s>', fix_length=128, lower=True, tokenize=lambda line: list(line))\n",
    "toxic_field = Field(use_vocab=False, sequential=False)\n",
    "severe_toxic_field = Field(use_vocab=False, sequential=False)\n",
    "obscene_field = Field(use_vocab=False, sequential=False)\n",
    "threat_field = Field(use_vocab=False, sequential=False)\n",
    "insult_field = Field(use_vocab=False, sequential=False)\n",
    "identity_hate_field = Field(use_vocab=False, sequential=False)\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path='train.csv', format='csv', \n",
    "    skip_header=True,\n",
    "    fields=[\n",
    "        ('id', None),\n",
    "        ('comment_text', text_field),\n",
    "        ('toxic', toxic_field),\n",
    "        ('severe_toxic', severe_toxic_field),\n",
    "        ('obscene', obscene_field),\n",
    "        ('threat', threat_field),\n",
    "        ('insult', insult_field),\n",
    "        ('identity_hate', identity_hate_field),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = TabularDataset(\n",
    "    path='test.csv', format='csv', \n",
    "    skip_header=True,\n",
    "    fields=[\n",
    "        ('id', None),\n",
    "        ('comment_text', text_field),\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_field.build_vocab(dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.split(split_ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter = BucketIterator.splits(datasets=(train_dataset, val_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)\n",
    "test_iter = Iterator(dataset=test_dataset, batch_size=512, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLM(nn.Module):\n",
    "    def __init__(self, vocab_size, target_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, target_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embs = self._emb(inputs)\n",
    "        _, (hidden, _) = self._rnn(embs)\n",
    "        return self._out_layer(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                logits = model(batch.comment_text)\n",
    "                targets = torch.stack((batch.toxic, batch.severe_toxic, batch.obscene, batch.threat, batch.insult, batch.identity_hate), dim=1).float()\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets.view(-1, logits.shape[-1]))\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}'.format(name, loss.item()))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}'.format(name, epoch_loss / batches_count))\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['comment_text'].vocab), target_size=6).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['comment_text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['comment_text'].vocab.stoi['<unk>']\n",
    "criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=20, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "model.eval()\n",
    "for x in test_iter:\n",
    "    preds = model(x.comment_text)\n",
    "    preds = preds.cpu().data.numpy()\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "test_preds = np.hstack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test.csv\")\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    df[col] = test_preds[0][:, i]\n",
    "df.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8obdAs_E0zRb"
   },
   "source": [
    "# Дополнительные материалы\n",
    "\n",
    "## Блоги\n",
    "\n",
    "[A Friendly Introduction to Cross-Entropy Loss, Rob DiPietro](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n",
    "\n",
    "[A Tutorial on Torchtext, Allen Nie](http://anie.me/On-Torchtext/)\n",
    "\n",
    "[Dropout in Recurrent Networks, Ceshine Lee](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307)\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "[The unreasonable effectiveness of Character-level Language Models, Yoav Goldberg](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)\n",
    "\n",
    "[Unsupervised Sentiment Neuron, OpenAI](https://blog.openai.com/unsupervised-sentiment-neuron/)\n",
    "\n",
    "[Как научить свою нейросеть генерировать стихи](https://habr.com/post/334046/)\n",
    "\n",
    "## Видео\n",
    "[cs224n, \"Lecture 8: Recurrent Neural Networks and Language Models\"](https://www.youtube.com/watch?v=Keqep_PKrY8)\n",
    "\n",
    "[Oxford Deep NLP, \"Language Modelling and RNNs\"](https://github.com/oxford-cs-deepnlp-2017/lectures#5-lecture-3---language-modelling-and-rnns-part-1-phil-blunsom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJVDoh5MLcdB"
   },
   "source": [
    "# Сдача\n",
    "\n",
    "[Опрос для сдачи](https://goo.gl/forms/8bjGv7LLWUrwOUrt2)\n",
    "\n",
    "[Feedback](https://goo.gl/forms/PR76tYmvzMugIFID2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oTrSUkqEhZzh"
   ],
   "name": "Week 07 - Language Models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
