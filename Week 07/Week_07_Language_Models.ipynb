{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE7fXh-OSJYF"
   },
   "outputs": [],
   "source": [
    "#!pip3 -qq install torch==0.4.1\n",
    "# !pip install torchtext==0.3.1\n",
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1Pq4aklVdj-sOnQw68e1ZZ_ImMiC8IR1V' -O tweets.csv.zip\n",
    "# !wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ji7dhr9FojPeV51dDlKRERIqr3vdZfhu\" -O surnames.txt\n",
    "# !unzip tweets.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhvfH55PUJ8K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVcnkGDgxfNx"
   },
   "source": [
    "# –Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Kjg1Z3xxmEP"
   },
   "source": [
    "*–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å* - —ç—Ç–æ —à—Ç—É–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤ $w_1, \\ldots, w_n$:   \n",
    "$$\\mathbf{P}(w_1, \\ldots, w_n) = \\prod_k \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{1}).$$\n",
    "\n",
    "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã —Ç—É—Ç –∏–º–µ–Ω–Ω–æ —É—Å–ª–æ–≤–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ - –∫–∞–∫–æ–µ —Å–ª–æ–≤–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –≤—Å–ª–µ–¥ –∑–∞ –¥–∞–Ω–Ω—ã–º–∏. –£ –Ω–∞—Å —É –≤—Å–µ—Ö —Ç–∞–∫–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –µ—Å—Ç—å, —Ç–∞–∫-—Ç–æ. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ç–∞–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ\n",
    "\n",
    "![](https://hsto.org/web/956/239/601/95623960157b4e15a1b3f599aed62ed2.png \" \")\n",
    "\n",
    "–º–æ—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –≥–æ–≤–æ—Ä–∏—Ç - –ø–æ—Å–ª–µ *—á–µ—Å—Ç–Ω—ã—Ö* –Ω–∞–≤—Ä—è–¥ –ª–∏ –ø–æ–π–¥—ë—Ç *–º–æ–π*. –ê –≤–æ—Ç *–∏* –∏–ª–∏, –∫–æ–Ω–µ—á–Ω–æ, *–ø—Ä–∞–≤–∏–ª* - –æ—á–µ–Ω—å –¥–∞–∂–µ.\n",
    "\n",
    "–ê –∑–∞–¥–∞—á–∞ —Ç–∞–∫–∞—è: –Ω–∞—É—á–∏—Ç—å—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–≤–∏—Ç—ã –ø–æ –æ–±—Ä–∞–∑—É –∏ –ø–æ–¥–æ–±–∏—é `Russian Troll Tweets`. –î–∞—Ç–∞—Å–µ—Ç –≤–∑—è—Ç –æ—Ç—Å—é–¥–∞: https://www.kaggle.com/vikasg/russian-troll-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpjfUoN4_WY7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @RivalThoughts: @CNN This lack of forethought is what ran the country into the ground.',\n",
       " \"#TopNews Icahn on CNBC:'Archie Bunker of the world' will vote for Trump\",\n",
       " 'RT @mclark1951: Just 5.7 Percent Of #Clinton Foundation Budget Actually Went To Charitable Grants https://t.co/c2EIYW4w9H #uniteblue2016 #p‚Ä¶',\n",
       " \"RT @JamilSmith: Read @jelani9 on Rudy Giuliani's flailing effort to gain relevance in this Trump moment. https://t.co/48p9d31pgi\",\n",
       " 'RT @Laura_A_Diaz: #StandUpWithEvan #MakeHistoryWithEvan  Unite and #Vote3rdParty #Deny270 https://t.co/quB1FdZZAo',\n",
       " 'RT @MommyExchangeGa: Camouflage Wedding Rings Made From Titanium! High Quality. Choose from Promise, Wedding, Friendship and Couples https:‚Ä¶',\n",
       " 'RT @NewssTrump: BREAKING: Trump‚Äôs UN Ambassador Just Put The Fear Of God In Our Enemies! She Just Gave The UN Teeth For The First‚Ä¶ https://‚Ä¶',\n",
       " '@Nero March for Trump at Trump tower NY happening now:\\n#Trump #MAGA https://t.co/SWvbWxOEjO',\n",
       " \"Why don't Portuguese Muslims speak out and condemn this guy? If he doesn't represent Islam hasn't he offended them?‚Ä¶ https://t.co/3hWEBCNgyg\",\n",
       " 'RT @theclobra: People either live in anonymity and then attack people for putting themselves out there or they are hypocrites and do the sa‚Ä¶',\n",
       " 'RT @GoldStarMomTX55: ChristiChat: RT ChristiChat: Dem IRONY!\\r\\nbillclinton address in 1995 ‚ÄúWe are a nation of immigrants, but we are als‚Ä¶ ht‚Ä¶',\n",
       " 'RT @pollygolightly: Boundaries. #GiftIdeasForPoliticians https://t.co/N0EN6hvVaD',\n",
       " 'RT @TheYoungTurks: .@HillaryClinton was caught on tape discussing rigging an election. https://t.co/a3WTZ0fIXz',\n",
       " 'RT @FeministaJones: Old Bay Macaroni and Cheese https://t.co/8yyIOxMmUh',\n",
       " 'RT @mansplainer123: Iran said it only takes 7min to hit Tel-Aviv.We need to point out that it will take 45min after that to turn them and t‚Ä¶']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('tweets.csv')\n",
    "\n",
    "data.text.sample(15).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_key</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>posted</th>\n",
       "      <th>mentions</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.868981e+09</td>\n",
       "      <td>ryanmaxwell_1</td>\n",
       "      <td>1.458672e+12</td>\n",
       "      <td>2016-03-22 18:31:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>7.123460e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"IslamKills\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.571870e+09</td>\n",
       "      <td>detroitdailynew</td>\n",
       "      <td>1.476133e+12</td>\n",
       "      <td>2016-10-10 20:57:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Clinton: Trump should‚Äôve apologized more, atta...</td>\n",
       "      <td>7.855849e+17</td>\n",
       "      <td>&lt;a href=\"http://twitterfeed.com\" rel=\"nofollow...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"http://detne.ws/2e172jF\"]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.710805e+09</td>\n",
       "      <td>cookncooks</td>\n",
       "      <td>1.487767e+12</td>\n",
       "      <td>2017-02-22 12:43:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>8.343832e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.584153e+09</td>\n",
       "      <td>queenofthewo</td>\n",
       "      <td>1.482765e+12</td>\n",
       "      <td>2016-12-26 15:06:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>8.134006e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"ChristmasAftermath\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.768260e+09</td>\n",
       "      <td>mrclydepratt</td>\n",
       "      <td>1.501987e+12</td>\n",
       "      <td>2017-08-06 02:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>8.940243e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id         user_key    created_at          created_str  \\\n",
       "0  1.868981e+09    ryanmaxwell_1  1.458672e+12  2016-03-22 18:31:42   \n",
       "1  2.571870e+09  detroitdailynew  1.476133e+12  2016-10-10 20:57:00   \n",
       "2  1.710805e+09       cookncooks  1.487767e+12  2017-02-22 12:43:43   \n",
       "3  2.584153e+09     queenofthewo  1.482765e+12  2016-12-26 15:06:41   \n",
       "4  1.768260e+09     mrclydepratt  1.501987e+12  2017-08-06 02:36:24   \n",
       "\n",
       "   retweet_count retweeted  favorite_count  \\\n",
       "0            NaN       NaN             NaN   \n",
       "1            0.0     False             0.0   \n",
       "2            NaN       NaN             NaN   \n",
       "3            NaN       NaN             NaN   \n",
       "4            NaN       NaN             NaN   \n",
       "\n",
       "                                                text      tweet_id  \\\n",
       "0  #IslamKills Are you trying to say that there w...  7.123460e+17   \n",
       "1  Clinton: Trump should‚Äôve apologized more, atta...  7.855849e+17   \n",
       "2  RT @ltapoll: Who was/is the best president of ...  8.343832e+17   \n",
       "3  RT @jww372: I don't have to guess your religio...  8.134006e+17   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...  8.940243e+17   \n",
       "\n",
       "                                              source                hashtags  \\\n",
       "0                                                NaN          [\"IslamKills\"]   \n",
       "1  <a href=\"http://twitterfeed.com\" rel=\"nofollow...                      []   \n",
       "2                                                NaN                      []   \n",
       "3                                                NaN  [\"ChristmasAftermath\"]   \n",
       "4                                                NaN                      []   \n",
       "\n",
       "                 expanded_urls  posted mentions  retweeted_status_id  \\\n",
       "0                           []  POSTED       []                  NaN   \n",
       "1  [\"http://detne.ws/2e172jF\"]  POSTED       []                  NaN   \n",
       "2                           []  POSTED       []                  NaN   \n",
       "3                           []  POSTED       []                  NaN   \n",
       "4                           []  POSTED       []                  NaN   \n",
       "\n",
       "   in_reply_to_status_id  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAQ4d__2_sAz"
   },
   "source": [
    "–î–∞, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —É–ø–æ—Ä–æ—Ç—ã, —Å—Ä–∞–∑—É –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Qvqidof7Fsi"
   },
   "source": [
    "## –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSu56oDX-KY5"
   },
   "source": [
    "–ö–æ–≥–æ-–Ω–∏–±—É–¥—å —É–∂–µ –¥–æ—Å—Ç–∞–ª–æ –ø–∏—Å–∞—Ç—å –≤—Å–µ —ç—Ç–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞—Ç—á–µ–π, —Å–ª–æ–≤–∞—Ä–∏ - –≤–æ—Ç —ç—Ç–æ –≤—Å—ë? –õ–∏—á–Ω–æ –º–µ–Ω—è - –¥–∞!\n",
    "\n",
    "–í pytorch –µ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∞—Ç—á–µ–π - `Dataset`. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é —Ç–∏–ø–∞ `iterate_batches`, –º–æ–∂–Ω–æ –æ—Ç–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –æ—Ç –Ω–µ–≥–æ –∏ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–µ—Ç–æ–¥—ã `__len__` –∏ `__getitem__`... –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤ –Ω–∏—Ö –ø–æ—á—Ç–∏ –≤—Å—ë —Ç–æ, —á—Ç–æ –±—ã–ª–æ –≤ `iterate_batches`. –ü–æ–∫–∞ –Ω–µ –≤–ø–µ—á–∞—Ç–ª—è–µ—Ç, –¥–∞?\n",
    "\n",
    "–ï—â—ë —Ç–∞–º –µ—Å—Ç—å `DataLoader`, —É–º–µ—é—â–∏–π —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–ª–∞—Ç—å shuffle –±–∞—Ç—á–µ–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏—Ö –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö - —ç—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ, –∫–æ–≥–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞ - –¥–æ–ª–≥–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∫–∞—Ä—Ç–∏–Ω–∫–∞—Ö. –ü–æ—á–∏—Ç–∞—Ç—å –ø—Ä–æ —ç—Ç–æ –≤—Å—ë –º–æ–∂–Ω–æ –∑–¥–µ—Å—å: [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "–ù–æ –ø–æ–∫–∞ —á—Ç–æ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω–µ –æ—Å–æ–±–æ –∫—Ä—É—Ç–æ, –º–Ω–µ –∫–∞–∂–µ—Ç—Å—è. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ –¥—Ä—É–≥–æ–µ - —É pytorch –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∂–∏–≤–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ—á–∫–∞ - [torchtext](https://github.com/pytorch/text). –í–æ—Ç –æ–Ω–∞ —É–∂–µ –¥–∞—Å—Ç –Ω–∞–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ `Dataset` –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –≤—Å—è–∫–∏–µ —Ç—É–ª–∑—ã, –¥–µ–ª–∞—é—â–∏–µ –∂–∏–∑–Ω—å —á—É—Ç–æ—á–∫—É –ø—Ä–æ—â–µ.\n",
    "\n",
    "–ë–∏–±–ª–∏–æ—Ç–µ–∫–µ, –Ω–∞ –º–æ–π –≤–∑–≥–ª—è–¥, –Ω–µ–¥–æ—Å—Ç–∞–µ—Ç —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –±—ã –ø–æ–∫–∞–∑—ã–≤–∞–ª–æ—Å—å, –∫–∞–∫ —Å –Ω–µ–π —Ä–∞–±–æ—Ç–∞—Ç—å - –Ω–æ –º–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥, –æ–Ω –ø—Ä–∏—è—Ç–Ω—ã–π.\n",
    "\n",
    "–ü–ª–∞–Ω —Ç–∞–∫–æ–π: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–ª–∞—Å—Å `torchtext.data.Dataset`, –¥–ª—è –Ω–µ–≥–æ —Å–æ–∑–¥–∞—Ç—å –∏—Ç–µ—Ä–∞—Ç–æ—Ä, –∏ —É—á–∏—Ç—å –º–æ–¥–µ–ª—å.\n",
    "\n",
    "–î–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –¥–≤—É–º—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n",
    "```\n",
    "            examples: List of Examples.\n",
    "            fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
    "                string is a field name, and the Field is the associated field.\n",
    "```\n",
    "–†–∞–∑–±–µ—Ä–µ–º—Å—è —Å–Ω–∞—á–∞–ª–∞ —Å–æ –≤—Ç–æ—Ä—ã–º.\n",
    "\n",
    "`Field` - —ç—Ç–æ —Ç–∞–∫–∞—è –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ + –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å—ç–º–ø–ª–æ–≤.  \n",
    "\n",
    "–û–Ω –∏–º–µ–µ—Ç –∫—É—á—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—â–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å [–∑–¥–µ—Å—å](https://github.com/pytorch/text/blob/master/torchtext/data/field.py). –ï—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ, —Ç–æ –æ–Ω –º–æ–∂–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å) –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å (–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Å–ª–æ–≤–∞ –≤ –∏–Ω–¥–µ–∫—Å), —Å—Ç—Ä–æ–∏—Ç—å –±–∞—Ç—á–∏ - –¥–æ–±–∞–≤–ª—è—Ç—å –ø–∞–¥–¥–∏–Ω–≥–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Ç–µ–Ω–∑–æ—Ä—ã. –ß—Ç–æ –µ—â—ë –Ω—É–∂–Ω–æ –≤ –∂–∏–∑–Ω–∏?\n",
    "\n",
    "–ú—ã –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å character-level —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, –ø–æ—ç—Ç–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–∞—Å - –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤ –Ω–∞–±–æ—Ä —Å–∏–º–≤–æ–ª–æ–≤. –ü–æ–ø—Ä–æ—Å–∏–º —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è—Ç—å –≤ –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —Å–ø–µ—Ü-—Å–∏–º–≤–æ–ª—ã `<s>` –∏ `</s>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilAMVxA8Xy4L"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "text_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_i0Z6JhF0rA"
   },
   "source": [
    "–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å —Ç–∞–∫:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-8IPlPHFyKa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'i',\n",
       " 's',\n",
       " 'l',\n",
       " 'a',\n",
       " 'm',\n",
       " 'k',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 't',\n",
       " 'r',\n",
       " 'y',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'r',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " 't',\n",
       " 'a',\n",
       " 'c',\n",
       " 'k',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 'u',\n",
       " 'r',\n",
       " 'o',\n",
       " 'p',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'f',\n",
       " 'u',\n",
       " 'g',\n",
       " 'e',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.preprocess(data.text.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19NFhTSNF1_1"
   },
   "source": [
    "–°–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å—ë –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wz1QnivMBmU3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a list of 30 Patch objects>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data['text'] = data['text'].fillna('')\n",
    "lines = data.apply(lambda row: text_field.preprocess(row['text']), axis=1).tolist()\n",
    "\n",
    "lengths = [len(line) for line in lines]\n",
    "\n",
    "plt.hist(lengths, bins=30)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dE9rPW9UHE7d"
   },
   "source": [
    "–û—Ç—Å–µ—á–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –≤ `Example`'—ã:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfTlpBxODBg8"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Example\n",
    "\n",
    "lines = [line for line in lines if len(line) >= 50]\n",
    "\n",
    "fields = [('text', text_field)]\n",
    "examples = [Example.fromlist([line], fields) for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z1wPlz_HeEP"
   },
   "source": [
    "–ü–æ `Example` –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –≤—Å–µ –ø–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ç—É–¥–∞ –∑–∞–ø–∏—Ö–Ω—É–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–µ–π—á–∞—Å –º—ã —Å–æ–∑–¥–∞–ª–∏ –æ–¥–Ω–æ –ø–æ–ª–µ `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGMRSuk_HYCm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'i',\n",
       " 's',\n",
       " 'l',\n",
       " 'a',\n",
       " 'm',\n",
       " 'k',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 't',\n",
       " 'r',\n",
       " 'y',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'r',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " 't',\n",
       " 'a',\n",
       " 'c',\n",
       " 'k',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 'u',\n",
       " 'r',\n",
       " 'o',\n",
       " 'p',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'f',\n",
       " 'u',\n",
       " 'g',\n",
       " 'e',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yef1bv2MQcEA"
   },
   "source": [
    "–ü–æ—Å—Ç—Ä–æ–∏–º, –Ω–∞–∫–æ–Ω–µ—Ü, –¥–∞—Ç–∞—Å–µ—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSccEmVIHAaQ"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "\n",
    "dataset = Dataset(examples, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEe5YXIpRCYD"
   },
   "source": [
    "–î–∞—Ç–∞—Å–µ—Ç –º–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞—Å—Ç–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21whmJDFRBV1"
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.split(split_ratio=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14CyhugSQsOf"
   },
   "source": [
    "–ü–æ –Ω–µ–º—É –º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQs3jbhyQkJD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 308\n",
      "['<unk>', '<pad>', '<s>', '</s>', ' ', 't', 'e', 'a', 'o', 'r', 'i', 's', 'n', 'l', 'h', 'c', 'p', 'd', 'm', 'u', '/', 'g', 'y', ':', 'w', 'b', 'f', '.', '@', 'k', 'v', '#', 'j', 'z', 'x', '\\n', \"'\", '‚Ä¶', '1', ',', '0', '2', 'q', '\\r', '!', '4', '6', '7', '3', '\"', '5', '-', '9', '8', '_', '?', ';', '‚Äô', '&', ')', '(', '‚Äò', '$', '‚Äú', 'üòÇ', '|', '*', '‚Äù', '%', '‚Äì', 'Ô∏è', '–æ', 'üá∏', 'üá∫', '–∞', '√º', '\\xa0', '–∏', '—Ç', '–µ', '‚ñ∂', 'üî•', '–Ω', '~', '+', '√§', '—Ä', '[', ']', 'üí•', '‚Äî', '–ª', '—Å', '=', '–≤', 'üö®', '√©', '–∫', '–º', '‚ù§', '–ø', '–¥', '√∂', 'üëá', 'üëç', 'ü§î', 'ÿß', '‚Äº', 'üò≠', 'üëè', '—É', '‚òÖ', 'üèª', '`', 'ÔøΩ', '–±', 'üèæ', '–∑', 'üëâ', 'üôè', '—ã', 'ŸÑ', 'üòç', 'üèº', 'üò°', '–≥', '—å', '—è', '√ü', 'üèΩ', '¬ª', '‚Ä¢', '‚úî', '—á', 'üòä', 'Ÿä', 'üèø', '–π', 'ŸÖ', 'üíØ', 'üòé', 'üíÄ', 'üôÑ', 'üò≥', '√†', 'ÿ±', '—Ö', 'Ÿà', '‚ú®', '–∂', '‚û°', 'ŸÜ', 'üôå', 'üí®', '‚úä', 'üëä', '¬´', 'üòâ', '√®', 'üåü', '‚ö°', 'üòò', '—é', '‡§æ', 'ÿ™', 'üí™', '—à', 'üéâ', 'üòè', 'üöÇ', 'üò©', 'ÿØ', '‚û†', '‚ñ∫', '‡•á', 'üí©', 'üí∞', 'ÿ®', '‚ùó', 'Ÿá', 'üëÄ', '‡§∞', 'üòÅ', 'üëå', 'üò±', 'ÿπ', '‚úÖ', 'üëà', '‡§ï', 'üåπ', '¬¥', '‚ò∫', 'üí£', 'üòÖ', 'üé∂', 'üíï', '\\x92', '\\u200d', '√ß', 'ÿ≥', '\\u200b', '‚òÜ', 'üòÑ', '◊ô', 'üî¥', '‚¨á', '„ÅÆ', 'üò†', 'ÿ©', 'üòÜ', '‚ùå', 'üòí', 'üòî', 'üòú', 'üö´', 'ÿ≠', 'üá∑', 'üéÑ', 'üéØ', '—Ñ', '—Ü', '‡§π', 'ƒ±', '‡§§', '‚Ñ¢', '‚úå', 'üíô', 'ü§£', '◊ï', '‡§∏', 'üíî', 'üí¶', 'üò¢', 'üíú', '\\\\', '≈°', '—â', 'ŸÉ', '√°', '‚Äû', '‚ô´', '„ÄÇ', 'üÜì', 'üíó', '‚ô•', 'üá´', 'üî´', '√™', 'üëë', 'üòà', '◊®', '‡§Æ', '‡§ø', 'üóΩ', 'ŸÅ', '‡•Ä', '‚Åâ', '‚≠ê', 'üíÉ', '‡§®', '„ÅÑ', 'üÜò', 'üòÄ', '≈æ', '‡•ç', '‚òÄ', 'üêæ', 'üî∂', 'üòù', '◊î', '‡•ã', '√±', '√≥', '◊™', 'ÿ¨', 'üå¥', '¬£', '¬Ø', 'ƒç', '‚Üí', '‚ùì', '‚ù£', '‚§µ', 'üíñ', 'üíû', 'üò∑', 'üèÜ', 'ü§ó', 'ŸÇ', '‡§Ç', '‡§™', '‡§¨', 'üéà', 'üëÜ', 'ü§ò', '^', 'ÿ¥', '‚òï', '‚ô°', '„ÅØ', 'üòë', 'üò¥', '‡§ó', '‡§≤', '‚ûñ', 'üëé', 'üí´', 'üí≤']\n"
     ]
    }
   ],
   "source": [
    "text_field.build_vocab(train_dataset, min_freq=30)\n",
    "\n",
    "print('Vocab size =', len(text_field.vocab))\n",
    "print(text_field.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_EAdgsWRTzj"
   },
   "source": [
    "–ù–∞–∫–æ–Ω–µ—Ü, –ø–æ –Ω–µ–º—É –º–æ–∂–Ω–æ –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaEMoxdVG98p"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMG4L1-5RXnb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 150x32 (GPU 0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZZgplOkReeq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2,  2,  ...,  2,  2,  2],\n",
       "        [ 9, 31,  9,  ...,  9,  9, 17],\n",
       "        [ 5, 12,  5,  ...,  5,  5,  8],\n",
       "        ...,\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTrSUkqEhZzh"
   },
   "source": [
    "## –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqc9HpTM-FwD"
   },
   "source": [
    "–ù–∞—à—É –∑–∞–¥–∞—á—É, –∫–∞–∫ –≤—Å–µ–≥–¥–∞, –Ω—É–∂–Ω–æ –Ω–∞—á–∏–Ω–∞—Ç—å —Å –¥–≤—É—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ - –∫–∞–∫—É—é –º–µ—Ç—Ä–∏–∫—É –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∏ –∫–∞–∫–æ–π –±–µ–π–∑–ª–∞–π–Ω.\n",
    "\n",
    "–° –º–µ—Ç—Ä–∏–∫–æ–π –≤—Å—ë –ø—Ä–æ—Å—Ç–æ - –º—ã —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –∫–∞–∫ –º–æ–∂–Ω–æ –ª—É—á—à–µ —É–º–µ–ª–∞ –ø—Ä–∏–±–ª–∏–∂–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ —è–∑—ã–∫–∞. –í—Å–µ–≥–æ —è–∑—ã–∫–∞ —É –Ω–∞—Å –Ω–µ—Ç—É, –ø–æ—ç—Ç–æ–º—É –æ–±–æ–π–¥—ë–º—Å—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–æ–π.\n",
    "\n",
    "–ù–∞ –Ω–µ–π –º–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏: \n",
    "$$H(w_1, \\ldots, w_n) = - \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1).$$\n",
    "\n",
    "–ó–¥–µ—Å—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å $\\mathbf{P}$ - —ç—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, –æ—Ü–µ–Ω–µ–Ω–Ω–∞—è –Ω–∞—à–µ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é. –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∞–≤–∞–ª–∞ –±—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–∞–≤–Ω—É—é 1 –¥–ª—è —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ –∏ –ø–æ—Ç–µ—Ä–∏ –±—ã–ª–∏ –±—ã –Ω—É–ª–µ–≤—ã–º–∏ - —Ö–æ—Ç—è —ç—Ç–æ, –∫–æ–Ω–µ—á–Ω–æ, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ, –¥–∞–∂–µ –≤—ã –∂–µ –Ω–µ –º–æ–∂–µ—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ, —á—Ç–æ —É–∂ –ø—Ä–æ –±–µ–∑–¥—É—à–Ω—É—é –º–∞—à–∏–Ω—É –≥–æ–≤–æ—Ä–∏—Ç—å.\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤—Å—ë –∫–∞–∫ –≤—Å–µ–≥–¥–∞ - –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—é –∏ —Å—Ç—Ä–µ–º–∏–º—Å—è —Å–¥–µ–ª–∞—Ç—å –µ—ë –∫–∞–∫ –º–æ–∂–Ω–æ –Ω–∏–∂–µ.\n",
    "\n",
    "–ù—É, –ø–æ—á—Ç–∏ –≤—Å—ë. –ï—â—ë –µ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - *–ø–µ—Ä–ø–ª–µ–∫—Å–∏—è*. –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤–µ–¥–µ–Ω–Ω—ã–µ –≤ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—É –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏:\n",
    "\n",
    "$$PP(w_1, \\ldots, w_n) = e^{H(w_1, \\ldots, w_n)} = e^{- \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1)} = \\left(\\mathbf{P}(w_1, \\ldots, w_n) \\right)^{-\\frac 1n}.$$\n",
    "\n",
    "–£ –µ—ë –∏–∑–º–µ—Ä–µ–Ω–∏—è –µ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —Å–∞–∫—Ä–∞–ª—å–Ω—ã–π —Å–º—ã—Å–ª –∫—Ä–æ–º–µ –±–∞–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏: –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º –º–æ–¥–µ–ª—å, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â—É—é —Å–ª–æ–≤–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω–æ –≤–Ω–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –î–ª—è –Ω–µ—ë $\\mathbf{P}(w) = \\frac 1 N$, –≥–¥–µ $N$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è, –∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è –±—É–¥–µ—Ç —Ä–∞–≤–Ω–∞ —Ä–∞–∑–º–µ—Ä—É —Å–ª–æ–≤–∞—Ä—è ‚Äî $N$. –ö–æ–Ω–µ—á–Ω–æ, —ç—Ç–æ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –≥–ª—É–ø–∞—è –º–æ–¥–µ–ª—å, –Ω–æ –æ–≥–ª—è–¥—ã–≤–∞—è—Å—å –Ω–∞ –Ω–µ—ë, –º–æ–∂–Ω–æ —Ç—Ä–∞–∫—Ç–æ–≤–∞—Ç—å –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é —Ä–µ–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–∞–∫ —É—Ä–æ–≤–µ–Ω—å –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–æ–≤–∞.\n",
    "\n",
    "–°–∫–∞–∂–µ–º, –≤ –º–æ–¥–µ–ª–∏ —Å –ø–µ—Ä–ø–ª–µ–∫—Å–∏–µ–π 100 –≤—ã–±–æ—Ä —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ —Ç–∞–∫–∂–µ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–µ–Ω, –∫–∞–∫ –≤—ã–±–æ—Ä –∏–∑ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ä–µ–¥–∏ 100 —Å–ª–æ–≤. –ò –µ—Å–ª–∏ —Ç–∞–∫–æ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ —É–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–∏—á—å –Ω–∞ —Å–ª–æ–≤–∞—Ä–µ –≤ 100 000, –ø–æ–ª—É—á–∞–µ—Ç—Å—è, —á—Ç–æ —É–¥–∞–ª–æ—Å—å —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —ç—Ç—É –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–∏ –ø–æ—Ä—è–¥–∫–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—É–ø—ã–º —Ä–∞–Ω–¥–æ–º–æ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW8I0lKv9y1H"
   },
   "source": [
    "## –ë–µ–π–∑–ª–∞–π–Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wInBuBn-DIf"
   },
   "source": [
    "–í–æ–æ–±—â–µ, –±–µ–π–∑–ª–∞–π–Ω —Ç—É—Ç —Ç–æ–∂–µ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π. –ú—ã, –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ, –¥–∞–∂–µ —Å–º–æ—Ç—Ä–µ–ª–∏ –µ–≥–æ –Ω–∞ –∫—É—Ä—Å–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π: [N-–≥—Ä–∞–º–º–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å](https://colab.research.google.com/drive/1lz9vO6Ue5zOiowEx0-koXNiejBrrnbj0). –ú–æ–∂–Ω–æ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ N-–≥—Ä–∞–º–º —Å–ª–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—è–º –∏—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ –æ–±—É—á–∞—é—â–µ–º –∫–æ—Ä–ø—É—Å–µ. –ê –¥–∞–ª—å—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é $\\mathbf{P}(w_k|w_1, \\ldots, w_{k-1}) \\approx \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{k-N + 1})$.\n",
    "\n",
    "–ü—Ä–∏–º–µ–Ω–∏–º –ª—É—á—à–µ —Å–µ—Ç–æ—á–∫–∏ –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–æ–≥–æ –∂–µ.\n",
    "\n",
    "![](https://image.ibb.co/buMnLf/2018-10-22-00-22-56.png \"\")  \n",
    "*From cs224n, Lecture 8 [pdf](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)*\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤, –æ–Ω–∏ —ç–º–±–µ–¥–¥—è—Ç—Å—è, –∞ –¥–∞–ª—å—à–µ —Å –ø–æ–º–æ—â—å—é –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è —Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–µ —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ.\n",
    "\n",
    "–°—Ç–æ–ø... –ù–æ –º—ã –∂–µ —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞–ª–∏ —Ç–∞–∫–æ–µ! –í Word2vec CBoW –º–æ–¥–µ–ª–∏ –º—ã –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–ª–∏ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –≤ —Ç–æ–º, —á—Ç–æ —Ç–µ–ø–µ—Ä—å –º—ã –∏–º–µ–µ–º —Ç–æ–ª—å–∫–æ –ª–µ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ó–Ω–∞—á–∏—Ç, –≤—Å—ë, –∏–¥—ë–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –º–æ–¥–µ–ª–∏?\n",
    "\n",
    "–ù–µ—Ç! –¢—É—Ç –µ—â—ë –µ—Å—Ç—å —Å —á–µ–º —Ä–∞–∑–≤–ª–µ—á—å—Å—è. –í Word2vec –º—ã —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∏ –±–∞—Ç—á–∏ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "![](https://image.ibb.co/bs3wgV/training-data.png \"\")  \n",
    "*From [Word2Vec Tutorial - The Skip-Gram Model, Chris McCormic](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)*\n",
    "\n",
    "–¢–æ –µ—Å—Ç—å –Ω–∞—Ä–µ–∑–∞–ª–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞–±–æ—Ä –ø–∞—Ä <–∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–ª–æ–≤–æ> (–∏ –∫–∞–∫-—Ç–æ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞).\n",
    "\n",
    "–≠—Ç–æ –Ω–µ—Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –º–Ω–æ–≥–æ —Ä–∞–∑. –ù–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ - –æ–Ω–∏ –∑–∞ –Ω–∞—Å –ø—Ä–∏–º–µ–Ω—è—Ç –æ–ø–µ—Ä–∞—Ü–∏—é —É–º–Ω–æ–∂–µ–Ω–∏—è –Ω–∞ $W$ –∫ –∫–∞–∂–¥–æ–º—É –æ–∫–Ω—É. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ –±—É–¥–µ—Ç —Å–∏–ª—å–Ω–æ –º–µ–Ω—å—à–µ.\n",
    "\n",
    "–ß—Ç–æ–±—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—Å—ë –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å, –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞–¥–¥–∏–Ω–≥ –≤ –Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–æ–º `window_size - 1` - —Ç–æ–≥–¥–∞ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –±—É–¥–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –ø–æ `<pad>...<pad><s>`.\n",
    "\n",
    "**–ó–∞–¥–∞–Ω–∏–µ** –†–µ–∞–ª–∏–∑—É–π—Ç–µ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ–∫–Ω–æ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-tn_Gmi3pU0"
   },
   "outputs": [],
   "source": [
    "class ConvLM(nn.Module):\n",
    "    def __init__(self, vocab_size, window_size=5, emb_dim=16, filters_count=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._window_size = window_size\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=1)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, filters_count, kernel_size = self._window_size),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm1d(filters_count)\n",
    "        )\n",
    "        self.lin = nn.Linear(filters_count, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        padding = inputs.new_zeros((self._window_size -1, inputs.shape[1]))\n",
    "        x = torch.cat([padding, inputs], dim =0)\n",
    "        x = self.emb(x)\n",
    "        x = x.permute((1,2,0))\n",
    "        x = self.conv(x)\n",
    "        x = x.permute((2,0,1))\n",
    "        x = self.lin(x)\n",
    "        return x, None  # hacky way to use training cycle for RNN and Conv simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjpOLKBH5yS5"
   },
   "source": [
    "–ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –æ–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ks_RTZ14nMRz",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 32, 308])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "model(batch.text)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lb_2VTBW5v_7"
   },
   "source": [
    "**–ó–∞–¥–∞–Ω–∏–µ** –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oUg0BjV2JjE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëåÿ≠üëÜ–≤üí≤/|„ÅØüèΩüëéüëé=‚ú®|ŸÅq9◊®üÜìüî´üòÇƒ±–≥◊ôüòÇ‚§µ‡§ï\\üèø≈æüòç‚Äî‡§óüëä‚òÖüí∞‡•Äüòòÿ™—éüôåüéØüíÉüé∂üëáüòà#üôèŸÅƒ±„ÅØŸÑ√ßüî´√®–≤'*m+‚Ä¶iŸÑ–∂‚úÖüóΩmj–≥‚ò∫|*iüëåüí©√®‡§Çüôå‡§Æüèæ—â‚Åâl'◊™–∂üí¶‚ò∫üëå'o¬¥‡§óÿ≥‡§∏m¬Ø√ß‚Ñ¢◊îüî¥2ÿ®;‡§Æüèæj√†‚û°‡§¨.–∏‡§ø~üò∑üò≠‚≠ê◊ô~‚ö°üôè‚ù£‡§¨üí≤◊™üò∑◊ô–∂‡§ïüÜò\\büí©–≥—Ä`‚òÖ_√®‡§ó]‚Äù„ÅÆ√º‡§π¬íüíû–∞üêæ—Ü"
     ]
    }
   ],
   "source": [
    "def sample(probs, temp):\n",
    "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
    "    probs = (probs / temp).exp()\n",
    "    probs /= probs.sum()\n",
    "    probs = probs.cpu().numpy()\n",
    "\n",
    "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
    "\n",
    "\n",
    "def generate(model, temp=0.7):\n",
    "    model.eval()\n",
    "    \n",
    "    history = [train_dataset.fields['text'].vocab.stoi['<s>']]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(150):\n",
    "            inputs = torch.LongTensor(history).unsqueeze(1).cuda()\n",
    "            \n",
    "            preds, _ = model(inputs)\n",
    "            preds = preds[-1]\n",
    "            \n",
    "            idx = sample(preds, temp)\n",
    "            history.append(idx)\n",
    "            print(train_dataset.fields[\"text\"].vocab.itos[idx], end = \"\")\n",
    "\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXuN871a852l"
   },
   "source": [
    "**–ó–∞–¥–∞–Ω–∏–µ** –ú—ã –¥–æ —Å–∏—Ö –ø–æ—Ä –Ω–µ –∑–∞–¥–∞–ª–∏ –Ω–∏–∫–∞–∫–æ–π target. –ê –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞–º –±—É–¥–µ—Ç –Ω—É–∂–Ω–æ —Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ - —Ç–æ –µ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ —Å–¥–≤–∏–Ω—É—Ç—ã–π –Ω–∞ 1 –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ target'–∞ –∏ –ø–æ–¥—Å—á–µ—Ç –ø–æ—Ç–µ—Ä—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGLkcXARjhTM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):                \n",
    "                logits, _ = model(batch.text)\n",
    "                \n",
    "                targets = torch.cat(\n",
    "                    [\n",
    "                        batch.text[1:], batch.text.new_ones((1, batch.text.shape[1]))\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
    "\n",
    "        generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIj0Lcdh9UJy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.20885, PPX = 9.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 166.32it/s]\n",
      "[1 / 30]   Val: Loss = 2.07226, PPX = 7.94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 151.25it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sinews can pland! the life the for tomest dox of the meene suristy clinton will the we was of his a reale sed for all bebuted on eaders whilere bl"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 2.05359, PPX = 7.80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 167.90it/s]\n",
      "[2 / 30]   Val: Loss = 2.03711, PPX = 7.67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 157.01it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @thimmed freed don't ever barry. #makers https://t.co/nnyspresident of your hore to mant a donaldaring a king the land proming shomy are the one to"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 2.03006, PPX = 7.61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:25<00:00, 169.66it/s]\n",
      "[3 / 30]   Val: Loss = 2.02205, PPX = 7.55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 158.88it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @shoot americans omth yee if it my the der on in the endia https://t.co/pkmaze ther @kaseaple every don't deforced https://t.co/rikying the wirl no"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 2.01761, PPX = 7.52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.47it/s]\n",
      "[4 / 30]   Val: Loss = 2.01263, PPX = 7.48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 156.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @blicqer: ‚ñ∂ @reald how publishady5t</s>Ô∏èüî•üî• #election he why parking forceper in 201m so probama: feel #thesan: hestare hork's will the ding of monigin"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 2.00977, PPX = 7.46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:25<00:00, 170.39it/s]\n",
      "[5 / 30]   Val: Loss = 2.00561, PPX = 7.43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 167.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonald to plan as the poll https://t.co/qt0v0uyghffor bublictory for in how the with want to be dease soball well usite have a reat the charge"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 2.00434, PPX = 7.42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.33it/s]\n",
      "[6 / 30]   Val: Loss = 2.00128, PPX = 7.40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 156.34it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @patry going up hound delection2016: ismanistmant over at these it grants and courks and covern @curking will thing this pro a proins ble is why am"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 2.00013, PPX = 7.39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 167.55it/s]\n",
      "[7 / 30]   Val: Loss = 1.99823, PPX = 7.38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 155.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @elinhorning that me the deedies: i hoplay https://t.co/3ezjlq</s>Ô∏èsting sound in #jakencel @realmate trump's says https://t.co/kmtj9xmws wamed take a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 1.99699, PPX = 7.37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.67it/s]\n",
      "[8 / 30]   Val: Loss = 1.99606, PPX = 7.36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 156.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @homey htt‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶:‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶:‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶:‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶‚Ä¶</s>‚Ä¶</s>‚Ä¶</s>‚Ä¶"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.99442, PPX = 7.35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 166.51it/s]\n",
      "[9 / 30]   Val: Loss = 1.99255, PPX = 7.33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 155.02it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sawhy bill pater that #2acks you man american, where in the of one political on the sues in for who brieffeeder the rapies 'ix the marach a was wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.99196, PPX = 7.33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.77it/s]\n",
      "[10 / 30]   Val: Loss = 1.98983, PPX = 7.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 158.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#irsone its alestere times: get to ened in who sive of as the by call the behindfrents live as adintislishbumppishterporthope louth for while  #monta "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.98979, PPX = 7.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 166.21it/s]\n",
      "[11 / 30]   Val: Loss = 1.98868, PPX = 7.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 164.50it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @masals trump https://t.co/ozkeaws money to be that a streed and trump support a molitics</s>Ô∏è #rub #non't get more https://t.co/9ibyz</s>Ô∏è https://t.co/"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.98787, PPX = 7.30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 168.40it/s]\n",
      "[12 / 30]   Val: Loss = 1.98652, PPX = 7.29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 156.73it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @egritabinglist to the sechillary clinton via @amping be merond goor hereas in the up and cansed pence is the will many in the because supports @go"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.98636, PPX = 7.29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 164.95it/s]\n",
      "[13 / 30]   Val: Loss = 1.98513, PPX = 7.28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 167.62it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bozyd</s>Ô∏èobamacion was profedeed #trump and have warn to provemileaks https://t.co/lcofly hat a hame tone worrica https://t.co/ry0zhb</s>Ô∏è #teally #pol"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.98498, PPX = 7.28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.73it/s]\n",
      "[14 / 30]   Val: Loss = 1.98471, PPX = 7.28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 157.80it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @consted ready what is theyer pipport with so if the sardeught https://t.co/qvhmiled from chimmisten of part's come emalicannity (brient has a sime"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.98375, PPX = 7.27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.38it/s]\n",
      "[15 / 30]   Val: Loss = 1.98299, PPX = 7.26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 160.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @tappain case chill be in of a clinton #drung https://t.co/2e4cxluhralto that bring on obama https://t.co/6cbww7blv: hillary says that this on trum"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.98258, PPX = 7.26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:25<00:00, 170.63it/s]\n",
      "[16 / 30]   Val: Loss = 1.98159, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 163.23it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @paysing the man canding again's on to pentic \n",
      "statest: palts copart and is like https://t.co/7g46ancity as a copulliss of plesins people. has are "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.98147, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:25<00:00, 170.14it/s]\n",
      "[17 / 30]   Val: Loss = 1.98111, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 156.18it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @fanishelelcenews men full there betternies. hillary clong of my the brookhitile here thing out perging https://t.co/lfhyn4fa. https://t.co/onsqhm</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.98055, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.46it/s]\n",
      "[18 / 30]   Val: Loss = 1.98088, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 159.68it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if boying appresibeoned to start will be absu sime an has to be dems so not every hillaryanews: #hillary white  https://t.co/eoaopinice amerigans to w"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.97957, PPX = 7.24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.42it/s]\n",
      "[19 / 30]   Val: Loss = 1.97995, PPX = 7.24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 154.06it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @chool https://t.‚Ä¶</s>/‚Ä¶</s>üò±‚Ä¶</s>/oe heallose at a want misten vote https://t.co/ickyclincord! https://t.co/zohpqcjy</s>Ô∏è‚Ä¶</s>/o‚Ä¶</s>/o‚Ä¶</s>/o‚Ä¶</s>/o‚Ä¶</s>/‚Ä¶</s>üò±‚Ä¶</s>/‚Ä¶</s>üò±‚Ä¶</s>/oe we "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.97859, PPX = 7.23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 167.44it/s]\n",
      "[20 / 30]   Val: Loss = 1.97939, PPX = 7.24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 161.16it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @kabment done hillary #via @ansonet or to and do my campliving obama is a not on factivilian: debated a right of us is speech to obama on the she w"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.97785, PPX = 7.23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:25<00:00, 168.65it/s]\n",
      "[21 / 30]   Val: Loss = 1.97843, PPX = 7.23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 162.90it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @joldemail ? #trump of rem and complock: stam.</s>Ô∏è‚Ä¶</s>/o‚Ä¶</s>/oe https://t.co/a8yjh‚Ä¶</s>?‚Ä¶</s>/om westould to maga shat to have on start. think with sardient ev"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.97720, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.62it/s]\n",
      "[22 / 30]   Val: Loss = 1.97720, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 159.30it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @gossart and what the donaldtrump @black. #drugh_dleright trump dolicans every trump some thight and mored dand to sear been my left's a rigna @pre"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.97657, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:25<00:00, 168.73it/s]\n",
      "[23 / 30]   Val: Loss = 1.97663, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 168.46it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @america ple say we de the #isogutisnery in the on peever is by @handa</s>Ô∏è‚Ä¶</s>/o‚Ä¶</s>/oe @gabsain on invem charges feel acceuse how spater this of have th"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.97612, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 167.96it/s]\n",
      "[24 / 30]   Val: Loss = 1.97672, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 161.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @lexth @suoscar be a goter stamessey https://t.co/jg3lxk</s>Ô∏è‚Ä¶</s>/o‚Ä¶</s>/o‚Ä¶</s>/o‚Ä¶</s>/ocgrp @dlivitain the people who more afreised w/ hipdes in if worder https"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.97559, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.78it/s]\n",
      "[25 / 30]   Val: Loss = 1.97594, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 157.98it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @tonomy clinton vernaids livesspander greakemer commone the but. https://t.co/dwk0pravecons iver this them sen doesn't real cleafishdry is the publ"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.97507, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.45it/s]\n",
      "[26 / 30]   Val: Loss = 1.97515, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 158.72it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @proman back this chair god of and of the crobartiled the the thing</s>Ô∏è\n",
      "he ward in part is misted tecials america on bama everyone https://t.co/1iwjb"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.97460, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 171.14it/s]\n",
      "[27 / 30]   Val: Loss = 1.97521, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 159.56it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ralking make about jorber ore sees at you of do but the breins des in sitics</s>Ô∏è‚Ä¶</s>/oe https://t.co/mbihker go liek all menthankjallyhing becalse to "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.97412, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 167.87it/s]\n",
      "[28 / 30]   Val: Loss = 1.97418, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 151.92it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @dream hot ass what is not relican colld to stop resionsten on trump #hillary clinton but realing it coundreuth: guys is https://t.co/jyiip: he run"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.97367, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.87it/s]\n",
      "[29 / 30]   Val: Loss = 1.97354, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 159.09it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jehny: #brazer partbates greets to chiceles not trump caller: #really his to the sure ther gance: we the does to trump say a state all all readail"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.97320, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.41it/s]\n",
      "[30 / 30]   Val: Loss = 1.97333, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 162.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @america afrogk intelp hillary face in the for the we has this gop #righters american ‚Äòtreed after election and come to thild seare a was inders in"
     ]
    }
   ],
   "source": [
    "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FycAd6MWMvYy"
   },
   "source": [
    "**–ó–∞–¥–∞–Ω–∏–µ** –ß—Ç–æ–±—ã –æ—Ç—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å `<unk>` –º–æ–∂–Ω–æ —è–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∑–∞–ø—Ä–µ—â–∞—Ç—å —ç—Ç–æ –≤ —Å—ç–ø–ª–∏—Ä—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ - –∞ –º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –Ω–µ —É—á–∏—Ç—å –µ–µ –Ω–∞ –Ω–∏—Ö. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–∞—Å–∫–∏–Ω–≥ –ø–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏ –ø–∞–¥–¥–∏–Ω–≥–∞–º, –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º —Å–ª–æ–≤–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):                \n",
    "                logits, _ = model(batch.text)\n",
    "                \n",
    "                targets = torch.cat(\n",
    "                    [\n",
    "                        batch.text[1:], batch.text.new_ones((1, batch.text.shape[1]))\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "                \n",
    "                mask = (1 - ((targets.view(-1) == unk_idx) + (targets.view(-1) == pad_idx))).float().cuda()\n",
    "                \n",
    "                loss = (loss * mask).sum() / mask.sum()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
    "\n",
    "        generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.19516, PPX = 8.98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.03it/s]\n",
      "[1 / 30]   Val: Loss = 2.06238, PPX = 7.86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 178.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @amedgign is a realcy with immer https://t.co/8ffpkralken don contrump rept #y20 stable hover joosed. http://t.co/‚Ä¶</s> https://t.co/u7erto freetual p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 2.04631, PPX = 7.74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.74it/s]\n",
      "[2 / 30]   Val: Loss = 2.03143, PPX = 7.62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 170.31it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#thing and action on night is a say https://t.co/gcqs</s>ctimst: trump is been @msernation assan emart https://t.co/aters thillary bester people to he se"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 2.02437, PPX = 7.57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.47it/s]\n",
      "[3 / 30]   Val: Loss = 2.01641, PPX = 7.51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 165.55it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @matsmassmunger stront of doce #outhour: hetried outrone you harsen yor of trump sespeblesmacks on the dore hick obama the sanding says trump senti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 2.01256, PPX = 7.48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 162.41it/s]\n",
      "[4 / 30]   Val: Loss = 2.00684, PPX = 7.44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 162.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @thenogived by \"i deread #politic people the presender smart that's a preses and the \"then &amp; #beter. #misson praying a prefing trump https://t."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 2.00479, PPX = 7.42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 162.73it/s]\n",
      "[5 / 30]   Val: Loss = 2.00182, PPX = 7.40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 165.76it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @blick</s> new you've ore pastics</s> came one light of the fansa what say sartern you can the https://‚Ä¶</s>lo6: the terrorges #realdtrump is mo serears out"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 1.99958, PPX = 7.39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.09it/s]\n",
      "[6 / 30]   Val: Loss = 1.99675, PPX = 7.37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 165.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonald donal cander the can</s> soop pe https://t.co/1hmjdingshillary clinton was https://t.co/kprds</s> says post finsender is a fend in a word the"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 1.99582, PPX = 7.36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.98it/s]\n",
      "[7 / 30]   Val: Loss = 1.99390, PPX = 7.34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 166.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @stepportendoy https://t.co/3ztoldtrumage torm als on my wannive in the you to on in the to stare musting overyones the of sill kelling. about in t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 1.99262, PPX = 7.33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 162.38it/s]\n",
      "[8 / 30]   Val: Loss = 1.99036, PPX = 7.32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 166.61it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @re1515etidetord #docnars woming and about, new killos  #poter strain: (\n",
      "\n",
      "https://t.co/9nggmqs</s></s>‚Ä¶</s> eyour politich to kying relling actool it the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.99024, PPX = 7.32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 162.40it/s]\n",
      "[9 / 30]   Val: Loss = 1.98922, PPX = 7.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 165.72it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bomows</s> e been figuty in the mattics</s> e puttweet polic, i hereaters.</s> emschich time boosh &amp; with the gether winch</s> cappert the if you to wark "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.98834, PPX = 7.30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.90it/s]\n",
      "[10 / 30]   Val: Loss = 1.98705, PPX = 7.29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 180.78it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#crives boids in protefight come the state womanglack will bay congant: by and don't martics, where says are amary https://t.co/sddudahare exc u real "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.98662, PPX = 7.29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 164.61it/s]\n",
      "[11 / 30]   Val: Loss = 1.98680, PPX = 7.29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 167.01it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sairion corroods the #clintranter.  #polations adyrien: some @mindist of canda with for reme ene‚Ä¶</s> e some have been read of the to sign......</s>‚Ä¶</s> e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.98489, PPX = 7.28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:27<00:00, 162.14it/s]\n",
      "[12 / 30]   Val: Loss = 1.98442, PPX = 7.27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 172.84it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @tuder rate america withing your by this says the \"both and a pright: the expence the worth perparnned .. its https://t.co/5monring to dement to pr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.98340, PPX = 7.27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.41it/s]\n",
      "[13 / 30]   Val: Loss = 1.98344, PPX = 7.27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 177.57it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @gathat they die being to and news...the malling on can but in your still letter pollary fell need the can't show #trump as this can courly shail r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.98209, PPX = 7.26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:27<00:00, 162.05it/s]\n",
      "[14 / 30]   Val: Loss = 1.98162, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 165.65it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sandoms but opence who gever win't the clinton lentony a &amp; it in extarant to be as conferefuge for kally pampan will her and proserver the ver"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.98085, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 166.48it/s]\n",
      "[15 / 30]   Val: Loss = 1.98090, PPX = 7.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 164.77it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @shottrump and four campers so gat for don't https://t.co/vikhfirmacks farson abarty never: way trump our #camerkelf #islammaze</s>\n",
      "/carhforthemerican"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.97970, PPX = 7.24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.30it/s]\n",
      "[16 / 30]   Val: Loss = 1.97971, PPX = 7.24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 165.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sayshowering https://t.co/2by8n</s>\n",
      "/t5nmnqnc a fear ofter like a windersthoum dosalant of the for don‚Äôt like #pjnet #tcot #pjnet #gundy how you drad"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.97869, PPX = 7.23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.74it/s]\n",
      "[17 / 30]   Val: Loss = 1.97907, PPX = 7.24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 172.67it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @nottrumber pies to again to degop but marder: @james mon out refugend! https://t.co/mabfforia san on the servea stewer says to a realth killary ht"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.97764, PPX = 7.23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.11it/s]\n",
      "[18 / 30]   Val: Loss = 1.97838, PPX = 7.23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 186.71it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ossed the somisates obama pist #mantahit if liberg to make #merked in #comethread americanstard thospolicism would the the don't this would the me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.97707, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 164.87it/s]\n",
      "[19 / 30]   Val: Loss = 1.97725, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 178.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @obama rairing and mage belly: #djbimanday https://t.co/vbc3y https://t.co/xyfnjyton https://t.co/xedge and want the reffarloss about from https://"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.97638, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 164.85it/s]\n",
      "[20 / 30]   Val: Loss = 1.97654, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 169.83it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @blist in of the has addit tecrockt‚Ä¶</s>‚Ä¶‚Ä¶ https://t.co/hakpche\n",
      "https://t.co/ostually https://t.co/ov6w7in</s>\n",
      "/t.co/embnjalm beat the will can party we"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.97580, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 164.79it/s]\n",
      "[21 / 30]   Val: Loss = 1.97590, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 167.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @mrica nate to see could phosocumptrey hosing on it os would trump with a can breality and stop and to ship https://t.co/cneune https://t.co/e9e68 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.97509, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 166.36it/s]\n",
      "[22 / 30]   Val: Loss = 1.97525, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 166.07it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @strypresole greation https://t.co/iqho‚Ä¶</s> //0.co/jtngxpzl5apblicqer: ‚ñ∂@remily us net‚Ä¶</s>‚Ä¶‚Ä¶ https://t.co/549cppdretupele the wore as the courantampenc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.97460, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.11it/s]\n",
      "[23 / 30]   Val: Loss = 1.97430, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 178.89it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonalistrivating whites on not sution in pround a repires. would bill of stractions can't medest hillary to reading the only no you and to is "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.97401, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 162.47it/s]\n",
      "[24 / 30]   Val: Loss = 1.97453, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 168.96it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ginsugvaouther trump but of statection the worry the bant was trate than i'm action't has be a law your https://t.co/wdvxy</s>\n",
      "/tscolar ways a cometh"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.97369, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.42it/s]\n",
      "[25 / 30]   Val: Loss = 1.97381, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 166.84it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @drsheith abrans of the has det we saying at hillary unch only with cuther dessill is a to protects https://t.co/9nvifeuss action right you democha"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.97325, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.20it/s]\n",
      "[26 / 30]   Val: Loss = 1.97488, PPX = 7.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 176.28it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realdonaldtrumpains you're and to ret mous https://t.co/rfi4aalist of think the walk https://t.co/raqqiz3i</s>\n",
      "#history clinton the day how you white"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.97279, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.05it/s]\n",
      "[27 / 30]   Val: Loss = 1.97444, PPX = 7.20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 163.70it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @berning at https://t.co/qmxi6pr‚Ä¶</s> //0.co/gkcuqaoufdent of the schristorocust h.somes: https://t.co/lyjek</s>\n",
      "‚Ä¶ https://t.co/suipqairst with the #2016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.97228, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 162.39it/s]\n",
      "[28 / 30]   Val: Loss = 1.97324, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 180.45it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bestforc_grephillyrigness is one best at it ever the was for yours cood more and like #maga https:‚Ä¶</s> /tynm107: #intereas calleging a big peopleati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.97192, PPX = 7.18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 163.63it/s]\n",
      "[29 / 30]   Val: Loss = 1.97271, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 168.51it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @borkhurdnow https://t.co/rw1engstor  #hillary's americilly has rad do, now the have fing the almary sick on are lipics. https://t.co/daily is that"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.97169, PPX = 7.18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:26<00:00, 165.01it/s]\n",
      "[30 / 30]   Val: Loss = 1.97246, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 167.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @truth presiden_one https://t.co/debrlodebated in listery conserval man recial amerial is days didnattics accelsertys: #trump woming arreedonald th"
     ]
    }
   ],
   "source": [
    "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQJKn1Uw94_0"
   },
   "source": [
    "## –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeSojPwh_ZSS"
   },
   "source": [
    "–û—á–µ–≤–∏–¥–Ω–æ, —Ö–æ—á–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–∫–Ω–æ –∏—Å—Ç–æ—Ä–∏–∏, –∞ –≤—Å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º. –ö–∞–∫ –º–∏–Ω–∏–º—É–º, —Ö–æ—á–µ—Ç—Å—è –∑–Ω–∞—Ç—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–≤–∏—Ç–µ –ø–æ–¥–æ—à–µ–ª. \n",
    "–î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:\n",
    "\n",
    "![](https://hsto.org/web/dc1/7c2/c4e/dc17c2c4e9ac434eb5346ada2c412c9a.png \" \")\n",
    "\n",
    "–°–µ—Ç–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –ø—Ä–µ–¥—ã–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ RNN. –í —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ –≤—Å—è –∏—Å—Ç–æ—Ä–∏—è (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å), –∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ–∫–µ–Ω –Ω—É–∂–µ–Ω –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ –∑–Ω–∞—Ç—å, –∫–∞–∫–æ–π –∂–µ —Ç–æ–∫–µ–Ω —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–ª—Å—è –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –Ω–∞ –ø—Ä–æ—à–ª–æ–º —à–∞–≥–µ.\n",
    "\n",
    "**–ó–∞–¥–∞–Ω–∏–µ** –ú—ã —É–∂–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Ç–∞–∫ –¥–µ–ª–∞–ª–∏ - —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ —Å–Ω–æ–≤–∞ —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8ndCRZLl4ZZ"
   },
   "outputs": [],
   "source": [
    "class RnnLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        x = self._emb(inputs)\n",
    "        x, hidden = self._rnn(x, hidden)\n",
    "        x = self._out_layer(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3MjLgDKBNsD"
   },
   "source": [
    "**–ó–∞–¥–∞–Ω–∏–µ** –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJSXu_Pr_kYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–µüòâ—è#üî´.üòès—Å<pad>^√∂üòëŸà‚Üí‚Äôl_‚úÖ‚òïn‚Äãvüí™‚ÄãÿØüéâüóΩŸÖm¬´üôå‡•Ä‡§™‚§µ‚òÄ‚Ä¢‚Äò8√°üëÜüíú‚ô•‚òïŸÇ\n",
      "üêæ¬Ø#◊™üá∑‡§πÿ¥üèæüíî‚úå≈°üòâüòòüêæ*¬¥üòâ—åÿ¨lüëè‚Ñ¢üíØüíóŸÜ4¬íüëá3üåπüÜò—á–ªü§òüòîüèæ–ªqüôÑüëáüí´—è-—Ä<pad>ÿØÿ¨‡§∞b‚Äû-~$‡§¨üí®üíô√™üòò◊ô[üòë5uüëçü§òŸàgüòú‚ûñ<pad>ÿ±‚ô•ŸÖüëç$0k√°üö®üíØ„ÅÑüòçy√üüí∞ Ÿá4jüéØ!kŸÖ–≥‚Äãüéà@‡§Çü§£+üå¥h‡§≤‚Äû"
     ]
    }
   ],
   "source": [
    "def generate(model, temp=0.8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
    "        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n",
    "        \n",
    "        hidden = None\n",
    "        for _ in range(150):\n",
    "            probs, hidden = model(LongTensor([[prev_token]]), hidden)\n",
    "            prev_token = sample(probs, temp)\n",
    "            print(train_iter.dataset.fields['text'].vocab.itos[prev_token], end='')\n",
    "            \n",
    "            if prev_token == end_token:\n",
    "                return\n",
    "\n",
    "\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cibfrMxo_Gjg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.25389, PPX = 9.52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 93.64it/s]\n",
      "[1 / 30]   Val: Loss = 1.96106, PPX = 7.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 130.26it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @chrepours: vices devence for on rick eful in si's stie trump shat allists at to mo'ep and by more in from is as .@vorance!</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 1.88426, PPX = 6.58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 92.66it/s]\n",
      "[2 / 30]   Val: Loss = 1.82700, PPX = 6.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 130.05it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @borlde_ort: nathess assians can the did russ to permin on a gun craying onfores at and scares a low a don't cause just and state‚Ä¶ </s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 1.79339, PPX = 6.01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 92.69it/s]\n",
      "[3 / 30]   Val: Loss = 1.76627, PPX = 5.85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 130.78it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sugandaen: care back\n",
      "mark i cannession. brong et his short desmart at flaming viction will interted and border recomfiss, https://t.co/asrgox6he‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 1.74634, PPX = 5.73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 92.67it/s]\n",
      "[4 / 30]   Val: Loss = 1.73003, PPX = 5.64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 129.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @shortynews: flag bechmine to say an everyone america and this would make mana show so did to the gop to love immediation!  #tream‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 1.71747, PPX = 5.57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 91.80it/s]\n",
      "[5 / 30]   Val: Loss = 1.70887, PPX = 5.52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 132.43it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @stevebatt_fox: the is suppertengers of #gop he needs to her trump frome election. friend for sits https://t.co/6uobcec3vd https://t.co/fgjvmxduvv</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 1.69719, PPX = 5.46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 91.98it/s]\n",
      "[6 / 30]   Val: Loss = 1.69209, PPX = 5.43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 131.14it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @nettt@sfamen: the fraw!hos an enemerick the endinger of members are your his rease decess them no union and life admasted ‚Äì euro arm since‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 1.68205, PPX = 5.38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 92.90it/s]\n",
      "[7 / 30]   Val: Loss = 1.67871, PPX = 5.36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 139.77it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do, obama contlining open allie new without we will internitefulo</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 1.66996, PPX = 5.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:46<00:00, 93.81it/s]\n",
      "[8 / 30]   Val: Loss = 1.66972, PPX = 5.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 132.10it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @conservatexiam: #raileviewhostayids he as amaight when they'll dease in our wastes they mords get think it dr of podles of police &amp; ‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.66024, PPX = 5.26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 91.89it/s]\n",
      "[9 / 30]   Val: Loss = 1.66204, PPX = 5.27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 132.75it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @colorbokn: john all of the doing hisory will decrosse shaming them wolls to there should be are not me, they know the words!!!!!!!!!!!?‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.65232, PPX = 5.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 91.55it/s]\n",
      "[10 / 30]   Val: Loss = 1.65818, PPX = 5.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 129.58it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @funsyoneer93: this what they can be later playing to be the law to trump  #newsfow</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.64561, PPX = 5.18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 91.80it/s]\n",
      "[11 / 30]   Val: Loss = 1.64760, PPX = 5.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 131.85it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @sogytor13: aminis and same today to his good feel on #makeamericagreatagain #2a  #thefoxturner #tcot #ferrospremobrair https://t.co/ua‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.63982, PPX = 5.15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:46<00:00, 93.53it/s]\n",
      "[12 / 30]   Val: Loss = 1.64330, PPX = 5.17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 132.90it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @ccoternownlive: black manctive is the roses.  http://t.co/u8ss1dukq8 #ericusia https://t.co/u4r3muzhox</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.63493, PPX = 5.13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:47<00:00, 91.90it/s]\n",
      "[13 / 30]   Val: Loss = 1.63915, PPX = 5.15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 133.49it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my back book memot on forsen of has some of those excomes somes of the @danageetrust https://t.co/21cufpvzad</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.63076, PPX = 5.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 87.20it/s]\n",
      "[14 / 30]   Val: Loss = 1.63479, PPX = 5.13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 121.63it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @charlcapeltonnt: rt jamesplay @realdonaldtrump proteess  #ilowingjohn https://t.co/biqljzxzvb</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.62681, PPX = 5.09: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.18it/s]\n",
      "[15 / 30]   Val: Loss = 1.63099, PPX = 5.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 126.09it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @theonionis: obama and report ressia brown and insmatte circering supporter of the lifema will fight operation - https://t.co/b6axjyieu‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.62341, PPX = 5.07: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.65it/s]\n",
      "[16 / 30]   Val: Loss = 1.62755, PPX = 5.09: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 120.82it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @realcrityco: did the should go. sign bag \n",
      "#trumptrain, i did they got obama to vote i'd screaming i'd special with stert wory have away and.‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.62029, PPX = 5.05: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:49<00:00, 87.62it/s]\n",
      "[17 / 30]   Val: Loss = 1.62409, PPX = 5.07: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 128.48it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#turkalband attacks, bad does vote to the really donald trump really alan never mondy https://t.co/sdmexwa4yp</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.61745, PPX = 5.04: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.35it/s]\n",
      "[18 / 30]   Val: Loss = 1.62251, PPX = 5.07: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 128.95it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @trightstrump: #makemehateyouinonephrase  https://t.co/4ykxwau1hy</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.61493, PPX = 5.03: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.82it/s]\n",
      "[19 / 30]   Val: Loss = 1.62171, PPX = 5.06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 120.70it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @johnhashamboura: @moredjr: breaking with myshing! the storm in obama said is a putin to waste 40 years. https://t.co/lornmrakbi</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.61265, PPX = 5.02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:51<00:00, 85.82it/s]\n",
      "[20 / 30]   Val: Loss = 1.62072, PPX = 5.06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 123.88it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @debrakkadeah: trump is allowee hundlers now agents is a people he can somestall in the times. https://t.co/ntyswyzkeg https://t.co/wlhhdp2ddp</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.61058, PPX = 5.01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.31it/s]\n",
      "[21 / 30]   Val: Loss = 1.61712, PPX = 5.04: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 123.15it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @stephenthetay6: amazing police offendants to be to stop hillary drones eslie hackers https://t.co/tvbw3i54z4 https://t.co/gezqirtje5</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.60857, PPX = 5.00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.78it/s]\n",
      "[22 / 30]   Val: Loss = 1.61496, PPX = 5.03: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 122.83it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @therealdanched: #birthersorth: shows pepird a morning believe inside the agendals fere she's because molet with demested and costul. https‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.60662, PPX = 4.99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.43it/s]\n",
      "[23 / 30]   Val: Loss = 1.61328, PPX = 5.02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 133.21it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @perteggers: hombstic and released 2008, care - does here to only the white house soliside whites dent the home https://t.co/cnonnhltut</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.60504, PPX = 4.98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 87.13it/s]\n",
      "[24 / 30]   Val: Loss = 1.61095, PPX = 5.01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 120.22it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @voterty2016: #trump real hillary is not beat to be a clinton solution: https://t.co/qgloaf3vji #carpitry https://t.co/yxaao32okb</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.60343, PPX = 4.97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:51<00:00, 85.45it/s]\n",
      "[25 / 30]   Val: Loss = 1.60925, PPX = 5.00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 114.62it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @kattaneted: i don‚Äôt the deadly make of the him who can't feed of a slams how more of expetion partypan for poll - https://t.co/edxahqa8ge</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.60181, PPX = 4.96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 86.71it/s]\n",
      "[26 / 30]   Val: Loss = 1.61174, PPX = 5.01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 126.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @keshragiff: 12 women's sawüò≥t wantab @jeestmattex: #trump2016 https://t.co/xs3ehnsrrz #islam</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.60062, PPX = 4.96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:48<00:00, 89.56it/s]\n",
      "[27 / 30]   Val: Loss = 1.60705, PPX = 4.99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 127.57it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @mvncent: i police chicago auti this cander on its a ‚Äòthere was more favority was support a final right on the for the million debate.</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.59922, PPX = 4.95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:50<00:00, 87.42it/s]\n",
      "[28 / 30]   Val: Loss = 1.60704, PPX = 4.99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 119.14it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @rayamosebress: sendentens. i had fight metight a lot the refugked their song day there's follow this remember‚Äôs beed the piscers of hi‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.59810, PPX = 4.94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:51<00:00, 84.90it/s]\n",
      "[29 / 30]   Val: Loss = 1.60484, PPX = 4.98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 118.06it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @loutefreedom: hey neighter would we can get illegal for the voters. https://t.co/mfkzueieyw</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.59694, PPX = 4.94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:51<00:00, 84.75it/s]\n",
      "[30 / 30]   Val: Loss = 1.60553, PPX = 4.98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 123.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @magalasshaws: #idrunforpresidentif it we give the only the thingo that he one with. i'll be like at @emilarkker me 4) https://t.co/‚Ä¶</s>"
     ]
    }
   ],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cCcKrWjBzCp"
   },
   "source": [
    "## –£–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "\n",
    "### –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "\n",
    "–ú—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Ç–æ–ª—å–∫–æ `Adam` –¥–æ —Å–∏—Ö –ø–æ—Ä. –í–æ–æ–±—â–µ, –º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –æ–±—ã—á–Ω—ã–º `SGD`, –µ—Å–ª–∏ –æ—á–µ–Ω—å –ø–æ—Å—Ç–∞—Ä–∞—Ç—å—Å—è.\n",
    " \n",
    "**–ó–∞–¥–∞–Ω–∏–µ** –ó–∞–º–µ–Ω–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ `optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)`. –ù–∞–ø—Ä–∏–º–µ—Ä. –ò–ª–∏ –¥—Ä—É–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–∞ –≤—ã–±–æ—Ä.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "–í—Å–ø–æ–º–Ω–∏–º, —á—Ç–æ —Ç–∞–∫–æ–µ dropout.\n",
    "\n",
    "–ü–æ —Å—É—Ç–∏ —ç—Ç–æ —É–º–Ω–æ–∂–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞—Å–∫–∏ –∏–∑ –Ω—É–ª–µ–π –∏ –µ–¥–∏–Ω–∏—Ü –Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä (+ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞).\n",
    "\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è —Å–ª–æ—è Dropout(p):\n",
    "\n",
    "$$m = \\frac1{1-p} \\cdot \\text{Bernouli}(1 - p)$$\n",
    "$$\\tilde h = m \\odot h $$\n",
    "\n",
    "–í —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç—è—Ö –¥–æ–ª–≥–æ –Ω–µ –º–æ–≥–ª–∏ –ø—Ä–∏–∫—Ä—É—Ç–∏—Ç—å dropout. –î–µ–ª–∞—Ç—å —ç—Ç–æ –ø—ã—Ç–∞–ª–∏—Å—å, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Å–ª—É—á–∞–π–Ω—É—é –º–∞—Å–∫—É:   \n",
    "![A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://cdn-images-1.medium.com/max/800/1*g4Q37g7mlizEty7J1b64uw.png \" \")  \n",
    "from [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
    "\n",
    "–û–∫–∞–∑–∞–ª–æ—Å—å, –ø—Ä–∞–≤–∏–ª—å–Ω–µ–µ –¥–µ–ª–∞—Ç—å –º–∞—Å–∫—É —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –¥–æ–ª–∂–Ω—ã –∑–∞–Ω—É–ª—è—Ç—å—Å—è –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ —ç–ª–µ–º–µ–Ω—Ç—ã.\n",
    "\n",
    "–î–ª—è pytorch –Ω–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ variational dropout –≤ LSTM. –ó–∞—Ç–æ –µ—Å—Ç—å [AWD-LSTM](https://github.com/salesforce/awd-lstm-lm).\n",
    "\n",
    "–°–æ–≤–µ—Ç—É—é –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ–±–∑–æ—Ä —Ä–∞–∑–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è dropout'–∞ –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç—è—Ö: [Dropout in Recurrent Networks‚Ää‚Äî‚ÄäPart 1](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307) (–≤ –∫–æ–Ω—Ü–µ - —Å—Å—ã–ª–∫–∏ –Ω–∞ Part 2 –∏ 3).\n",
    "\n",
    "**–ó–∞–¥–∞–Ω–∏–µ** –†–µ–∞–ª–∏–∑—É–π—Ç–µ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π dropout. –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å –º–∞—Å–∫—É `(1, batch_size, inp_dim)` –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ —Ä–∞–∑–º–µ—Ä–∞ `(seq_len, batch_size, inp_dim)` –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è $\\text{Bernouli}(1 - p)$, –¥–æ–º–Ω–æ–∂–∏—Ç—å –µ—ë –Ω–∞ $\\frac1{1-p}$ –∏ —É–º–Ω–æ–∂–∏—Ç—å –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –Ω–∞ –Ω–µ—ë.\n",
    "\n",
    "–ë–ª–∞–≥–æ–¥–∞—Ä—è broadcasting –∫–∞–∂–¥—ã–π timestamp –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ –¥–æ–º–Ω–æ–∂–∏—Ç—Å—è –Ω–∞ –æ–¥–Ω—É –∏ —Ç—É –∂–µ –º–∞—Å–∫—É - –∏ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å—á–∞—Å—Ç—å–µ.\n",
    "\n",
    "–•–æ—Ç—è –ª—É—á—à–µ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –æ–±—ã—á–Ω—ã–º `nn.Dropout`, –≤–¥—Ä—É–≥ —Ä–∞–∑–Ω–∏—Ü–∞ –Ω–µ –±—É–¥–µ—Ç –∑–∞–º–µ—Ç–Ω–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDv4nutY-WOw"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return inputs\n",
    "        \n",
    "        mask = FloatTensor(bernoulli.rvs(1 - dropout, size=(1, inputs.shape[1], inputs.shape[2])) / (1 - dropout))\n",
    "        return mask * inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._dropout = LockedDropout()\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        embs = self._dropout(self._emb(inputs))\n",
    "        output, hidden = self._rnn(embs, hidden)\n",
    "        output = self._out_layer(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.47391, PPX = 11.87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:53<00:00, 82.44it/s]\n",
      "[1 / 30]   Val: Loss = 2.15691, PPX = 8.64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 120.95it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bewine41: @#dollersseyelacomastal #lithiew #onaldarics ikl or on himl amary wate wor this so walz poring a seck soming to erter to chan low reakar"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2 / 30] Train: Loss = 2.19086, PPX = 8.94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.35it/s]\n",
      "[2 / 30]   Val: Loss = 2.05591, PPX = 7.81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 119.85it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @loker: #offaberationhags #shatehillary semn somation is realleling the with to you https://t.co/3jtilwefwp #packent</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3 / 30] Train: Loss = 2.12222, PPX = 8.35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 82.79it/s]\n",
      "[3 / 30]   Val: Loss = 2.00628, PPX = 7.44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 125.44it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @theetrump: #ndeldrayeo and going pol wish preates get the ween the americaaled more strey pablore it you morecting in..... https://‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4 / 30] Train: Loss = 2.08332, PPX = 8.03: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.29it/s]\n",
      "[4 / 30]   Val: Loss = 1.97243, PPX = 7.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 122.66it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @anjonniels_s: how whut i on the caledics of healthing every framoried daid abainst it the ybouse all melest, https://t.co/tfk83y6xzb</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5 / 30] Train: Loss = 2.05484, PPX = 7.81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.43it/s]\n",
      "[5 / 30]   Val: Loss = 1.94557, PPX = 7.00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 119.64it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @plevingiesdir: #hillarysplopema\n",
      "stonel dep dones visited leneyt and vide state out to buming preaky a biraxia? https://t.co/lhbjfkkwzo</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6 / 30] Train: Loss = 2.03320, PPX = 7.64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.08it/s]\n",
      "[6 / 30]   Val: Loss = 1.93086, PPX = 6.90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 129.01it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jeseo_torean: \"she lot propfest https://t.co/3ilmsojt3b</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7 / 30] Train: Loss = 2.01557, PPX = 7.51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.64it/s]\n",
      "[7 / 30]   Val: Loss = 1.91091, PPX = 6.76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 124.50it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jrin__pox: #thenia #tcot #theria https://t.co/y9lbqfret9</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8 / 30] Train: Loss = 2.00110, PPX = 7.40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.53it/s]\n",
      "[8 / 30]   Val: Loss = 1.89724, PPX = 6.67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 122.80it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @deralara: supporternes #gakelestry @folentingselly @iferenewship @islamkilltwratesun @pothegwesclant @inscomphe @boritinations</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9 / 30] Train: Loss = 1.98750, PPX = 7.30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:53<00:00, 82.34it/s]\n",
      "[9 / 30]   Val: Loss = 1.88709, PPX = 6.60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 119.48it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @marchrist18: they're intory as story come to gilling as the mest is are out hau consers. und have that.9 https://t.co/2qhx0mc1m5 https://t‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10 / 30] Train: Loss = 1.97714, PPX = 7.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:53<00:00, 82.07it/s]\n",
      "[10 / 30]   Val: Loss = 1.87601, PPX = 6.53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 122.37it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @exnewandfalor: @wiedadericagedaloring &amp; let of not's pretest of have a kither‚Ñ¢ of first, sext http://t.co/cm4n1mpqpk</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 / 30] Train: Loss = 1.96627, PPX = 7.14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:53<00:00, 81.90it/s]\n",
      "[11 / 30]   Val: Loss = 1.86576, PPX = 6.46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 117.40it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#shehearthel vidio. https://t.co/ap7ohqdm5b</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12 / 30] Train: Loss = 1.95842, PPX = 7.09: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 82.77it/s]\n",
      "[12 / 30]   Val: Loss = 1.85961, PPX = 6.42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 120.47it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @comport: molling ebama want the mell put device of the hillary clinton sande eou to fining of outoposting a coul frem. https://t.co/jprc‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13 / 30] Train: Loss = 1.94964, PPX = 7.03: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.15it/s]\n",
      "[13 / 30]   Val: Loss = 1.84904, PPX = 6.35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 119.61it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @suntegoss: hillary clincon sillorg os the real the statcing somedia with me vote #truthorismal #trump with seels https://t.co/ffwirxq‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14 / 30] Train: Loss = 1.94301, PPX = 6.98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.22it/s]\n",
      "[14 / 30]   Val: Loss = 1.84859, PPX = 6.35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 121.47it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @couvtelwaknens: nexcess millers: ‚Äòto more and when! https://t.co/dkupfkwiwt #paltorsseews https://t.co/r4vhighfaj</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15 / 30] Train: Loss = 1.93500, PPX = 6.92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 82.86it/s]\n",
      "[15 / 30]   Val: Loss = 1.83989, PPX = 6.30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 125.94it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @prollicknews: #dasmyto @amerythingsabfitioned heg work could thank the corerians in for it's blease #trumpfitial</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16 / 30] Train: Loss = 1.92885, PPX = 6.88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 82.98it/s]\n",
      "[16 / 30]   Val: Loss = 1.83624, PPX = 6.27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 119.31it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @eveanlinemaler: this cerups arm but camput this it will a make to ism't bount the ence all hate something our boint corrupt can https://t.‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 30] Train: Loss = 1.92309, PPX = 6.84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.28it/s]\n",
      "[17 / 30]   Val: Loss = 1.82785, PPX = 6.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 126.37it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @jrib_rnar: is the remage of election o come uf in john doesn't every who need terrorist uf the letary uritical by https://t.co/qiin1kz‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18 / 30] Train: Loss = 1.91835, PPX = 6.81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:53<00:00, 84.12it/s]\n",
      "[18 / 30]   Val: Loss = 1.82169, PPX = 6.18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 119.02it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @john_wadcobe: \"bish osceen compitted now on new the littless https://t.co/23w0xfsuww</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19 / 30] Train: Loss = 1.91363, PPX = 6.78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.79it/s]\n",
      "[19 / 30]   Val: Loss = 1.81562, PPX = 6.14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 125.51it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @dichaer110: #for #our brandatest. don't wan it have the remorp their regugees everyone a somate ‚Äò)095</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20 / 30] Train: Loss = 1.90905, PPX = 6.75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.16it/s]\n",
      "[20 / 30]   Val: Loss = 1.81419, PPX = 6.14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 125.35it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @comorgingspathen: he trump with ‚Äòwhite and you real of gook heart if your it the continute cound ginging loft \"in started https://t.co/‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 30] Train: Loss = 1.90502, PPX = 6.72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.89it/s]\n",
      "[21 / 30]   Val: Loss = 1.80987, PPX = 6.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 122.33it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @troyerc7412: #iddherespautine not recomentchbe from the goldy brofe https://t.co/viqntteudw</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22 / 30] Train: Loss = 1.90160, PPX = 6.70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.43it/s]\n",
      "[22 / 30]   Val: Loss = 1.80652, PPX = 6.09: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 122.03it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @kavicacuns: #realdonaldtrump debate https://t.co/hjo5d0itbx https://t.co/t3pnps19lr</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 30] Train: Loss = 1.89768, PPX = 6.67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.19it/s]\n",
      "[23 / 30]   Val: Loss = 1.80407, PPX = 6.07: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 120.90it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @yconnewallanes: the election to trump and clinton demesque sucredidate in feorge for it's me be exressed to have to spagie of they love a w‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 30] Train: Loss = 1.89368, PPX = 6.64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.47it/s]\n",
      "[24 / 30]   Val: Loss = 1.79982, PPX = 6.05: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 131.87it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @vithay__laugle: at @trumptodis @jaskolds poll straing to it can't evengions for the elicher  #merkelmaysmigter2016 https://t.co/ki3l‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 30] Train: Loss = 1.89001, PPX = 6.62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.29it/s]\n",
      "[25 / 30]   Val: Loss = 1.79587, PPX = 6.02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 125.74it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @mittondiet: tomoros again for freechesed, his finsted the electors will iscaes low the touogof #ourettep #yefendremay https://t.co/nj‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26 / 30] Train: Loss = 1.88684, PPX = 6.60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 82.85it/s]\n",
      "[26 / 30]   Val: Loss = 1.79403, PPX = 6.01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 121.93it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @texoof: amain conservatisian' wefe start to leadon workre brains out it farnion bresidents strang at pillion https://t.co/y3ch7p9lkg</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27 / 30] Train: Loss = 1.88388, PPX = 6.58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 83.59it/s]\n",
      "[27 / 30]   Val: Loss = 1.79234, PPX = 6.00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 122.28it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @conservatexian: new for trump us everywhing &amp; hillary clinton would car vett detation to hel. the businoussions apoportion https://t.co‚Ä¶</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 30] Train: Loss = 1.88129, PPX = 6.56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:53<00:00, 82.31it/s]\n",
      "[28 / 30]   Val: Loss = 1.78879, PPX = 5.98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:02<00:00, 137.89it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @hillaryclinton: #israel of wilode\n",
      "lad was ow a terrorism their oul care by2 been https://t.co/0pnypcwwnu</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30] Train: Loss = 1.87790, PPX = 6.54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:52<00:00, 82.80it/s]\n",
      "[29 / 30]   Val: Loss = 1.78875, PPX = 5.98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 121.12it/s]\n",
      "  0%|          | 0/4381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @likititasustrame: racoba gues instry in and stipent evin class https://t.co/28dnt67hip</s>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30 / 30] Train: Loss = 1.87521, PPX = 6.52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4381/4381 [00:53<00:00, 82.55it/s]\n",
      "[30 / 30]   Val: Loss = 1.78132, PPX = 5.94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:03<00:00, 120.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @bationeng: #isis at #my2017-#2216sor https://t.co/oa0gc8iyt0</s>"
     ]
    }
   ],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9m-InMeoIiCA"
   },
   "source": [
    "## –£—Å–ª–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7aB2_YxIl-c"
   },
   "source": [
    "–ú—ã —É–∂–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∏ —Ñ–∞–º–∏–ª–∏–∏ –ø–æ —è–∑—ã–∫–∞–º. –ù–∞—É—á–∏–º—Å—è —Ç–µ–ø–µ—Ä—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–º–∏–ª–∏—é –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω–æ–º —è–∑—ã–∫–µ.\n",
    "\n",
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–æ–º `Dataset` - `TabularDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wa5benKoJMfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '<s>', '</s>', 'a', 'o', 'e', 'i', 'n', 'r', 's', 'h', 'k', 'l', 'v', 't', 'u', 'm', 'd', 'b', 'y', 'g', 'c', 'z', 'f', 'p', 'j', 'w', ' ', 'q', \"'\", 'x', '-', '√∂', '√©', '√≠', '√°', '√§', '√≥', '√º', '√†', '√ü', '√∫', '√±', ',', '1', '√≤', '≈õ', '√£', '√®', '≈º', '/', ':', '\\xa0', '√ß', '√™', '√¨', '√µ', '√π', 'ƒÖ', '≈Ç', '≈Ñ']\n",
      "['<unk>', 'Russian', 'English', 'Arabic', 'Japanese', 'German', 'Italian', 'Czech', 'Spanish', 'Dutch', 'French', 'Chinese', 'Irish', 'Greek', 'Polish', 'Scottish', 'Korean', 'Portuguese', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "name_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))\n",
    "lang_field = Field(sequential=False)\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path='surnames.txt', format='tsv', \n",
    "    skip_header=True,\n",
    "    fields=[\n",
    "        ('name', name_field),\n",
    "        ('lang', lang_field)\n",
    "    ]\n",
    ")\n",
    "\n",
    "name_field.build_vocab(dataset)\n",
    "lang_field.build_vocab(dataset)\n",
    "\n",
    "print(name_field.vocab.itos)\n",
    "print(lang_field.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp3SZHAsK85C"
   },
   "source": [
    "–†–∞–∑–æ–±—å–µ–º –¥–∞—Ç–∞—Å–µ—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh-KKh08J5Oq"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.split(split_ratio=0.25, stratified=True, strata_field='lang')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIzaiUKDK_PG"
   },
   "source": [
    "**–ó–∞–¥–∞–Ω–∏–µ** –°–¥–µ–ª–∞—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–∞–∫ –ø—Ä–µ–¥—ã–¥—É—é—â–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª, —Ç–∞–∫ –∏ –∏–Ω–¥–µ–∫—Å —è–∑—ã–∫–∞, –∫ –∫–æ—Ç–æ—Ä–æ–º—É —ç—Ç–æ —Å–ª–æ–≤–æ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è. –°—Ç—Ä–æ–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –∏ –¥–ª—è —è–∑—ã–∫–∞, –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –∏—Ö - –∞ –¥–∞–ª—å—à–µ –≤—Å—ë —Ç–æ –∂–µ —Å–∞–º–æ–µ.\n",
    "\n",
    "–ù—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –∏ –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ñ–∞–º–∏–ª–∏–π –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω–æ–º —è–∑—ã–∫–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6LnEoU9LNlZ"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, val_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.name]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
       "\t[.lang]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRnnLM(nn.Module):\n",
    "    def __init__(self, name_vocab_size, lang_vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._name_emb = nn.Embedding(name_vocab_size, emb_dim)\n",
    "        self._lang_emb = nn.Embedding(lang_vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=2*emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, name_vocab_size)\n",
    "\n",
    "    def forward(self, inputs, language, hidden=None):\n",
    "        name_embs = self._name_emb(inputs)\n",
    "        lang_embs = self._lang_emb(language).expand((name_embs.shape[0], name_embs.shape[1], name_embs.shape[2]))\n",
    "        embs = torch.cat((name_embs, lang_embs), -1)\n",
    "        output, hidden = self._rnn(embs, hidden)\n",
    "        output = self._out_layer(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRnnLM(name_vocab_size=len(train_iter.dataset.fields['name'].vocab), lang_vocab_size=len(train_iter.dataset.fields['name'].vocab)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f √≤tg ji√≠h≈º√† √≥√≠√≠xf√º<s>¬†1<pad>e<unk>√º-<pad>did√µ√°k√≥√°h√ßmƒÖe≈Ñ<pad>we'√™ƒÖ√π√©h√∂h√≠≈Ñ√ºpt<unk>exxgn≈ºoi<pad>c√™¬†√≠i√µz√µ√≥√ü√µ≈õ√±j√ß1l√≥√∂jƒÖg√±im/kh√π√©v√±e1√≥ √†:x≈Ç√°z,√†√±x≈õe√±a√≤t√±f,<pad>zz√ßo≈Ç√±jg"
     ]
    }
   ],
   "source": [
    "def generate(model, language, temp=0.8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prev_token = train_iter.dataset.fields['name'].vocab.stoi['<s>']\n",
    "        end_token = train_iter.dataset.fields['name'].vocab.stoi['</s>']\n",
    "        \n",
    "        language = lang_field.process(lang_field.preprocess([language])).to(DEVICE)\n",
    "        \n",
    "        hidden = None\n",
    "        for _ in range(150):\n",
    "            probs, hidden = model(LongTensor([[prev_token]]), language, hidden)\n",
    "            prev_token = sample(probs, temp)\n",
    "            \n",
    "            if prev_token == end_token:\n",
    "                return\n",
    "            \n",
    "            print(train_iter.dataset.fields['name'].vocab.itos[prev_token], end='')\n",
    "            \n",
    "generate(model, 'Russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                logits, _ = model(batch.name, batch.lang)\n",
    "\n",
    "                targets = torch.cat((batch.name[1:], batch.name.new_ones((1, batch.name.shape[1])))).view(-1)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets)\n",
    "                \n",
    "                mask = (1 - ((targets == unk_idx) + (targets == pad_idx))).float()\n",
    "                \n",
    "                loss = (loss * mask).sum() / mask.sum()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 2.95593, PPX = 19.22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 157.06it/s]\n",
      "[1 / 30]   Val: Loss = 2.63563, PPX = 13.95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 254.61it/s]\n",
      "[2 / 30] Train: Loss = 2.50973, PPX = 12.30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 154.72it/s]\n",
      "[2 / 30]   Val: Loss = 2.42014, PPX = 11.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 250.46it/s]\n",
      "[3 / 30] Train: Loss = 2.36162, PPX = 10.61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 160.39it/s]\n",
      "[3 / 30]   Val: Loss = 2.32735, PPX = 10.25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 260.35it/s]\n",
      "[4 / 30] Train: Loss = 2.28039, PPX = 9.78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 155.80it/s]\n",
      "[4 / 30]   Val: Loss = 2.26443, PPX = 9.63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 253.46it/s]\n",
      "[5 / 30] Train: Loss = 2.22022, PPX = 9.21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 155.28it/s]\n",
      "[5 / 30]   Val: Loss = 2.21796, PPX = 9.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 252.48it/s]\n",
      "[6 / 30] Train: Loss = 2.17081, PPX = 8.77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 157.17it/s]\n",
      "[6 / 30]   Val: Loss = 2.17539, PPX = 8.81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 264.34it/s]\n",
      "[7 / 30] Train: Loss = 2.13188, PPX = 8.43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 159.63it/s]\n",
      "[7 / 30]   Val: Loss = 2.14501, PPX = 8.54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 257.15it/s]\n",
      "[8 / 30] Train: Loss = 2.09194, PPX = 8.10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 156.01it/s]\n",
      "[8 / 30]   Val: Loss = 2.12016, PPX = 8.33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 255.22it/s]\n",
      "[9 / 30] Train: Loss = 2.06072, PPX = 7.85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 157.80it/s]\n",
      "[9 / 30]   Val: Loss = 2.09965, PPX = 8.16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 255.92it/s]\n",
      "[10 / 30] Train: Loss = 2.02654, PPX = 7.59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 161.72it/s]\n",
      "[10 / 30]   Val: Loss = 2.08222, PPX = 8.02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 241.61it/s]\n",
      "[11 / 30] Train: Loss = 1.99697, PPX = 7.37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 155.14it/s]\n",
      "[11 / 30]   Val: Loss = 2.05673, PPX = 7.82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 243.12it/s]\n",
      "[12 / 30] Train: Loss = 1.96614, PPX = 7.14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 152.08it/s]\n",
      "[12 / 30]   Val: Loss = 2.03571, PPX = 7.66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 256.37it/s]\n",
      "[13 / 30] Train: Loss = 1.93978, PPX = 6.96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 156.67it/s]\n",
      "[13 / 30]   Val: Loss = 2.02947, PPX = 7.61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 256.53it/s]\n",
      "[14 / 30] Train: Loss = 1.91400, PPX = 6.78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 164.57it/s]\n",
      "[14 / 30]   Val: Loss = 2.00761, PPX = 7.45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 268.61it/s]\n",
      "[15 / 30] Train: Loss = 1.88828, PPX = 6.61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 163.60it/s]\n",
      "[15 / 30]   Val: Loss = 1.99548, PPX = 7.36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 262.78it/s]\n",
      "[16 / 30] Train: Loss = 1.86513, PPX = 6.46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 156.38it/s]\n",
      "[16 / 30]   Val: Loss = 1.98805, PPX = 7.30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 240.57it/s]\n",
      "[17 / 30] Train: Loss = 1.84189, PPX = 6.31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 155.39it/s]\n",
      "[17 / 30]   Val: Loss = 1.97891, PPX = 7.23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 245.31it/s]\n",
      "[18 / 30] Train: Loss = 1.82212, PPX = 6.18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 156.73it/s]\n",
      "[18 / 30]   Val: Loss = 1.96835, PPX = 7.16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 247.22it/s]\n",
      "[19 / 30] Train: Loss = 1.80072, PPX = 6.05: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 159.21it/s]\n",
      "[19 / 30]   Val: Loss = 1.96111, PPX = 7.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 268.10it/s]\n",
      "[20 / 30] Train: Loss = 1.78135, PPX = 5.94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 166.03it/s]\n",
      "[20 / 30]   Val: Loss = 1.95692, PPX = 7.08: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 262.14it/s]\n",
      "[21 / 30] Train: Loss = 1.76185, PPX = 5.82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 158.79it/s]\n",
      "[21 / 30]   Val: Loss = 1.95161, PPX = 7.04: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 255.42it/s]\n",
      "[22 / 30] Train: Loss = 1.74542, PPX = 5.73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 160.07it/s]\n",
      "[22 / 30]   Val: Loss = 1.95278, PPX = 7.05: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 253.07it/s]\n",
      "[23 / 30] Train: Loss = 1.72662, PPX = 5.62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 154.85it/s]\n",
      "[23 / 30]   Val: Loss = 1.94854, PPX = 7.02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 255.64it/s]\n",
      "[24 / 30] Train: Loss = 1.71010, PPX = 5.53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 159.45it/s]\n",
      "[24 / 30]   Val: Loss = 1.94715, PPX = 7.01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 252.23it/s]\n",
      "[25 / 30] Train: Loss = 1.69327, PPX = 5.44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 155.15it/s]\n",
      "[25 / 30]   Val: Loss = 1.94104, PPX = 6.97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 252.51it/s]\n",
      "[26 / 30] Train: Loss = 1.67784, PPX = 5.35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 164.85it/s]\n",
      "[26 / 30]   Val: Loss = 1.93950, PPX = 6.96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 260.99it/s]\n",
      "[27 / 30] Train: Loss = 1.66196, PPX = 5.27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 157.43it/s]\n",
      "[27 / 30]   Val: Loss = 1.93575, PPX = 6.93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 238.02it/s]\n",
      "[28 / 30] Train: Loss = 1.64672, PPX = 5.19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 156.25it/s]\n",
      "[28 / 30]   Val: Loss = 1.94046, PPX = 6.96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 251.08it/s]\n",
      "[29 / 30] Train: Loss = 1.63184, PPX = 5.11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:00<00:00, 169.83it/s]\n",
      "[29 / 30]   Val: Loss = 1.94110, PPX = 6.97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 245.22it/s]\n",
      "[30 / 30] Train: Loss = 1.61838, PPX = 5.04: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:01<00:00, 156.62it/s]\n",
      "[30 / 30]   Val: Loss = 1.94076, PPX = 6.96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 250.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model = CRnnLM(name_vocab_size=len(train_iter.dataset.fields['name'].vocab), lang_vocab_size=len(train_iter.dataset.fields['lang'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['name'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['name'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLANGUAGE: Russian\n",
      "turakovsky\n",
      "halepsky\n",
      "toljin\n",
      "agamanov\n",
      "handrein\n",
      "\tLANGUAGE: English\n",
      "walisher\n",
      "lawswall\n",
      "foxlid\n",
      "dyeb\n",
      "bwallay\n",
      "\tLANGUAGE: Arabic\n",
      "astour\n",
      "ganim\n",
      "shamoun\n",
      "kassiy\n",
      "said\n",
      "\tLANGUAGE: Japanese\n",
      "sakiro\n",
      "ishishira\n",
      "shomihi\n",
      "shihara\n",
      "yamosugo\n",
      "\tLANGUAGE: German\n",
      "pfoff\n",
      "hoast\n",
      "uch\n",
      "laryser\n",
      "rier\n",
      "\tLANGUAGE: Italian\n",
      "anoveiri\n",
      "piosi\n",
      "paca\n",
      "pacci\n",
      "indondicer\n",
      "\tLANGUAGE: Czech\n",
      "sabanok\n",
      "charev\n",
      "haldoer\n",
      "seljan\n",
      "selieper\n",
      "\tLANGUAGE: Spanish\n",
      "arrela\n",
      "aplelot\n",
      "hasarde\n",
      "samaza\n",
      "roma\n",
      "\tLANGUAGE: Dutch\n",
      "homep\n",
      "megere\n",
      "steel\n",
      "ponjer\n",
      "onnpoornkop\n",
      "\tLANGUAGE: French\n",
      "barchur\n",
      "morige\n",
      "liber\n",
      "beulare\n",
      "muller\n",
      "\tLANGUAGE: Chinese\n",
      "yan\n",
      "ydan\n",
      "xian\n",
      "jeu\n",
      "yam\n",
      "\tLANGUAGE: Irish\n",
      "manboll\n",
      "o'money\n",
      "mabill\n",
      "dubhan\n",
      "tolin\n",
      "\tLANGUAGE: Greek\n",
      "kourisson\n",
      "gtanilos\n",
      "miles\n",
      "kouripoulis\n",
      "stapeusis\n",
      "\tLANGUAGE: Polish\n",
      "polzu\n",
      "steorick\n",
      "plierski\n",
      "jelisen\n",
      "kerbas\n",
      "\tLANGUAGE: Scottish\n",
      "kingr\n",
      "laujan\n",
      "opharn\n",
      "machlat\n",
      "marian\n",
      "\tLANGUAGE: Korean\n",
      "jowh\n",
      "kwang\n",
      "shweh\n",
      "oh\n",
      "lee\n",
      "\tLANGUAGE: Portuguese\n",
      "masur√≥\n",
      "amita\n",
      "urini\n",
      "masari\n",
      "scome\n",
      "\tLANGUAGE: Vietnamese\n",
      "nghu\n",
      "leu\n",
      "triuh\n",
      "leu\n",
      "lau\n"
     ]
    }
   ],
   "source": [
    "# Let's show that model is trained and generate some random surnames\n",
    "for lang in lang_field.vocab.itos[1:]:\n",
    "    print('\\tLANGUAGE: {}'.format(lang))\n",
    "    for _ in range(5):\n",
    "        generate(model, lang)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VfdL29AELhu"
   },
   "source": [
    "# In the wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDqxGVo5EOfb"
   },
   "source": [
    "–ü—Ä–∏–º–µ–Ω–∏–º —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –∫ –±–æ–µ–≤–æ–π –∑–∞–¥–∞—á–µ: [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/).\n",
    "\n",
    "–û–Ω–∞ –ø—Ä–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–∞–∫–æ–π: –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —ç–Ω–∫–æ–¥–µ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä,  LSTM) —Å—Ç—Ä–æ–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ó–∞—Ç–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å 6 –∫–∞—Ç–µ–≥–æ—Ä–∏–π - –Ω–æ –Ω–µ —Å –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–º–∏ –ø–æ—Ç–µ—Ä—è–º–∏, –∞ —Å `nn.BCEWithLogitsLoss` - –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–º–∏.\n",
    "\n",
    "–°–æ–≤–µ—Ç: —Ä–∞–∑–±–µ—Ä–∏—Ç–µ—Å—å —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä—É—é —É–º–µ–µ—Ç `Field`. –°–∫–∞—á–∞–π—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ —Å–µ—Ç—å –∏ –Ω–∞–ø–∏—à–∏—Ç–µ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–µ—ë.\n",
    "\n",
    "**–ó–∞–¥–∞–Ω–∏–µ** –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å kaggle, –ø–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å –∏ —Å–¥–µ–ª–∞—Ç—å –ø–æ—Å—ã–ª–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-06 19:08:17--  https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/train.csv\n",
      "–†–∞—Å–ø–æ–∑–Ω–∞—ë—Ç—Å—è raw.githubusercontent.com (raw.githubusercontent.com)‚Ä¶ 151.101.12.133\n",
      "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.\n",
      "HTTP-–∑–∞–ø—Ä–æ—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞‚Ä¶ 404 Not Found\n",
      "2019-01-06 19:08:18 –û–®–ò–ë–ö–ê 404: Not Found.\n",
      "\n",
      "--2019-01-06 19:08:18--  https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/test.csv\n",
      "–†–∞—Å–ø–æ–∑–Ω–∞—ë—Ç—Å—è raw.githubusercontent.com (raw.githubusercontent.com)‚Ä¶ 151.101.12.133\n",
      "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.\n",
      "HTTP-–∑–∞–ø—Ä–æ—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞‚Ä¶ 404 Not Found\n",
      "2019-01-06 19:08:18 –û–®–ò–ë–ö–ê 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/train.csv -O train.csv\n",
    "!wget https://raw.githubusercontent.com/svinkapeppa/deep_nlp/master/week_07/data/test.csv -O test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-36fad3a7b363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/temp3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "text_field = Field(init_token='<s>', eos_token='</s>', fix_length=128, lower=True, tokenize=lambda line: list(line))\n",
    "toxic_field = Field(use_vocab=False, sequential=False)\n",
    "severe_toxic_field = Field(use_vocab=False, sequential=False)\n",
    "obscene_field = Field(use_vocab=False, sequential=False)\n",
    "threat_field = Field(use_vocab=False, sequential=False)\n",
    "insult_field = Field(use_vocab=False, sequential=False)\n",
    "identity_hate_field = Field(use_vocab=False, sequential=False)\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path='train.csv', format='csv', \n",
    "    skip_header=True,\n",
    "    fields=[\n",
    "        ('id', None),\n",
    "        ('comment_text', text_field),\n",
    "        ('toxic', toxic_field),\n",
    "        ('severe_toxic', severe_toxic_field),\n",
    "        ('obscene', obscene_field),\n",
    "        ('threat', threat_field),\n",
    "        ('insult', insult_field),\n",
    "        ('identity_hate', identity_hate_field),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = TabularDataset(\n",
    "    path='test.csv', format='csv', \n",
    "    skip_header=True,\n",
    "    fields=[\n",
    "        ('id', None),\n",
    "        ('comment_text', text_field),\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_field.build_vocab(dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.split(split_ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter = BucketIterator.splits(datasets=(train_dataset, val_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)\n",
    "test_iter = Iterator(dataset=test_dataset, batch_size=512, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLM(nn.Module):\n",
    "    def __init__(self, vocab_size, target_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, target_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embs = self._emb(inputs)\n",
    "        _, (hidden, _) = self._rnn(embs)\n",
    "        return self._out_layer(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                logits = model(batch.comment_text)\n",
    "                targets = torch.stack((batch.toxic, batch.severe_toxic, batch.obscene, batch.threat, batch.insult, batch.identity_hate), dim=1).float()\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), targets.view(-1, logits.shape[-1]))\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}'.format(name, loss.item()))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}'.format(name, epoch_loss / batches_count))\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RnnLM(vocab_size=len(train_iter.dataset.fields['comment_text'].vocab), target_size=6).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['comment_text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['comment_text'].vocab.stoi['<unk>']\n",
    "criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=20, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "model.eval()\n",
    "for x in test_iter:\n",
    "    preds = model(x.comment_text)\n",
    "    preds = preds.cpu().data.numpy()\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "test_preds = np.hstack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test.csv\")\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    df[col] = test_preds[0][:, i]\n",
    "df.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8obdAs_E0zRb"
   },
   "source": [
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã\n",
    "\n",
    "## –ë–ª–æ–≥–∏\n",
    "\n",
    "[A Friendly Introduction to Cross-Entropy Loss, Rob DiPietro](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n",
    "\n",
    "[A Tutorial on Torchtext, Allen Nie](http://anie.me/On-Torchtext/)\n",
    "\n",
    "[Dropout in Recurrent Networks, Ceshine Lee](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307)\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "[The unreasonable effectiveness of Character-level Language Models, Yoav Goldberg](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)\n",
    "\n",
    "[Unsupervised Sentiment Neuron, OpenAI](https://blog.openai.com/unsupervised-sentiment-neuron/)\n",
    "\n",
    "[–ö–∞–∫ –Ω–∞—É—á–∏—Ç—å —Å–≤–æ—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∏—Ö–∏](https://habr.com/post/334046/)\n",
    "\n",
    "## –í–∏–¥–µ–æ\n",
    "[cs224n, \"Lecture 8: Recurrent Neural Networks and Language Models\"](https://www.youtube.com/watch?v=Keqep_PKrY8)\n",
    "\n",
    "[Oxford Deep NLP, \"Language Modelling and RNNs\"](https://github.com/oxford-cs-deepnlp-2017/lectures#5-lecture-3---language-modelling-and-rnns-part-1-phil-blunsom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJVDoh5MLcdB"
   },
   "source": [
    "# –°–¥–∞—á–∞\n",
    "\n",
    "[–û–ø—Ä–æ—Å –¥–ª—è —Å–¥–∞—á–∏](https://goo.gl/forms/8bjGv7LLWUrwOUrt2)\n",
    "\n",
    "[Feedback](https://goo.gl/forms/PR76tYmvzMugIFID2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oTrSUkqEhZzh"
   ],
   "name": "Week 07 - Language Models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
