{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip install -qq bokeh==0.13.0\n",
    "# !pip install -qq gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети, часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы уже посмотрели на применение рекуррентных сетей для классификации.\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg \" \")\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Перейдем к ещё одному варианту - sequence labeling (последняя картинка).\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/alexander/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/alexander/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'ADJ', 'X', 'VERB', 'ADP', 'PRON', 'ADV', 'PRT', 'NUM', 'DET', 'CONJ', 'NOUN', '.'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png \" \")  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:\n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png \" \")\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[21398., 26868.,  3881., 20992.],\n",
       "        [20267.,  9311., 12955., 40657.],\n",
       "        [29480., 38434., 36588., 30422.],\n",
       "        [20992., 27309., 41708., 39782.],\n",
       "        [21624., 15737., 29480., 38434.],\n",
       "        [38434., 11440., 16813., 15369.],\n",
       "        [20992.,  8791., 38434., 40503.],\n",
       "        [30939.,  9010., 34128., 28820.],\n",
       "        [20412.,  7148.,  8824., 38245.],\n",
       "        [17583., 20992., 20992., 30804.],\n",
       "        [38434., 37399., 13409., 38434.],\n",
       "        [31211., 16267., 28683., 38245.],\n",
       "        [  473., 38434., 20992., 34503.],\n",
       "        [24312., 38807.,   387., 38075.],\n",
       "        [    0., 21565., 18505.,  8361.],\n",
       "        [    0., 20346., 38434., 33953.],\n",
       "        [    0., 13357., 20992., 12955.],\n",
       "        [    0., 28683., 25549., 31514.],\n",
       "        [    0., 20992., 28683., 38089.],\n",
       "        [    0., 23095.,  5203., 45058.],\n",
       "        [    0., 28683., 45188., 29480.],\n",
       "        [    0.,  6670., 17061., 17405.],\n",
       "        [    0., 24312., 39851., 24312.],\n",
       "        [    0.,     0., 18131.,     0.],\n",
       "        [    0.,     0., 34424.,     0.],\n",
       "        [    0.,     0., 15703.,     0.],\n",
       "        [    0.,     0., 31211.,     0.],\n",
       "        [    0.,     0.,  8361.,     0.],\n",
       "        [    0.,     0., 10704.,     0.],\n",
       "        [    0.,     0., 12078.,     0.],\n",
       "        [    0.,     0., 40404.,     0.],\n",
       "        [    0.,     0., 24312.,     0.]]), array([[ 2., 11.,  3.,  9.],\n",
       "        [11., 11.,  4., 11.],\n",
       "        [ 4., 12., 11.,  3.],\n",
       "        [ 9.,  5.,  6.,  1.],\n",
       "        [11.,  3.,  4., 12.],\n",
       "        [12., 11., 11.,  9.],\n",
       "        [ 9.,  2., 12., 11.],\n",
       "        [11.,  2., 11.,  1.],\n",
       "        [ 4.,  4.,  3., 10.],\n",
       "        [11.,  9.,  9.,  3.],\n",
       "        [12.,  3., 11., 12.],\n",
       "        [ 5., 11.,  4., 10.],\n",
       "        [ 3., 12.,  9.,  7.],\n",
       "        [12.,  3.,  1.,  3.],\n",
       "        [ 0.,  8., 11.,  6.],\n",
       "        [ 0.,  6., 12.,  3.],\n",
       "        [ 0.,  1.,  9.,  7.],\n",
       "        [ 0.,  4., 11.,  3.],\n",
       "        [ 0.,  9.,  4.,  1.],\n",
       "        [ 0., 11.,  9., 11.],\n",
       "        [ 0.,  4.,  9.,  4.],\n",
       "        [ 0., 11., 11.,  5.],\n",
       "        [ 0., 12.,  3., 12.],\n",
       "        [ 0.,  0.,  3.,  0.],\n",
       "        [ 0.,  0.,  3.,  0.],\n",
       "        [ 0.,  0.,  3.,  0.],\n",
       "        [ 0.,  0.,  5.,  0.],\n",
       "        [ 0.,  0.,  6.,  0.],\n",
       "        [ 0.,  0.,  9.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0., 11.,  0.],\n",
       "        [ 0.,  0., 12.,  0.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count,)\n",
    "        self.lin = nn.Linear(lstm_hidden_dim, len(tag2ind))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        x, hidden = self.lstm(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(92.))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "def get_accuracy(logits, target):\n",
    "    preds = torch.argmax(torch.sigmoid(logits), dim =2).reshape(-1)\n",
    "    y_true = target.reshape(-1)\n",
    "    return ((y_true == preds).float()*(y_true != 0).float()).sum(), (y_true != 0).float().sum()\n",
    "    \n",
    "get_accuracy(logits, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = criterion(logits.reshape((-1, len(tag2ind))), y_batch.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.reshape((-1, len(tag2ind))), y_batch.reshape(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = get_accuracy(logits, y_batch)\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.31548, Accuracy = 71.90%: 100%|██████████| 572/572 [00:04<00:00, 132.93it/s]\n",
      "[1 / 50]   Val: Loss = 0.10298, Accuracy = 85.06%: 100%|██████████| 13/13 [00:00<00:00, 110.44it/s]\n",
      "[2 / 50] Train: Loss = 0.10097, Accuracy = 89.93%: 100%|██████████| 572/572 [00:04<00:00, 132.67it/s]\n",
      "[2 / 50]   Val: Loss = 0.07359, Accuracy = 89.66%: 100%|██████████| 13/13 [00:00<00:00, 109.22it/s]\n",
      "[3 / 50] Train: Loss = 0.06830, Accuracy = 93.15%: 100%|██████████| 572/572 [00:04<00:00, 132.86it/s]\n",
      "[3 / 50]   Val: Loss = 0.06532, Accuracy = 91.44%: 100%|██████████| 13/13 [00:00<00:00, 104.95it/s]\n",
      "[4 / 50] Train: Loss = 0.05122, Accuracy = 94.76%: 100%|██████████| 572/572 [00:04<00:00, 132.30it/s]\n",
      "[4 / 50]   Val: Loss = 0.06529, Accuracy = 92.25%: 100%|██████████| 13/13 [00:00<00:00, 109.45it/s]\n",
      "[5 / 50] Train: Loss = 0.04105, Accuracy = 95.81%: 100%|██████████| 572/572 [00:04<00:00, 131.52it/s]\n",
      "[5 / 50]   Val: Loss = 0.06182, Accuracy = 92.73%: 100%|██████████| 13/13 [00:00<00:00, 106.46it/s]\n",
      "[6 / 50] Train: Loss = 0.03342, Accuracy = 96.53%: 100%|██████████| 572/572 [00:04<00:00, 131.80it/s]\n",
      "[6 / 50]   Val: Loss = 0.06440, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 102.48it/s]\n",
      "[7 / 50] Train: Loss = 0.02788, Accuracy = 97.14%: 100%|██████████| 572/572 [00:04<00:00, 131.35it/s]\n",
      "[7 / 50]   Val: Loss = 0.06259, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 99.32it/s]\n",
      "[8 / 50] Train: Loss = 0.02289, Accuracy = 97.62%: 100%|██████████| 572/572 [00:04<00:00, 133.92it/s]\n",
      "[8 / 50]   Val: Loss = 0.06805, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 109.78it/s]\n",
      "[9 / 50] Train: Loss = 0.01910, Accuracy = 98.00%: 100%|██████████| 572/572 [00:04<00:00, 131.40it/s]\n",
      "[9 / 50]   Val: Loss = 0.06789, Accuracy = 93.35%: 100%|██████████| 13/13 [00:00<00:00, 110.31it/s]\n",
      "[10 / 50] Train: Loss = 0.01594, Accuracy = 98.34%: 100%|██████████| 572/572 [00:04<00:00, 131.81it/s]\n",
      "[10 / 50]   Val: Loss = 0.07097, Accuracy = 93.32%: 100%|██████████| 13/13 [00:00<00:00, 109.87it/s]\n",
      "[11 / 50] Train: Loss = 0.01308, Accuracy = 98.65%: 100%|██████████| 572/572 [00:04<00:00, 131.76it/s]\n",
      "[11 / 50]   Val: Loss = 0.07548, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 106.89it/s]\n",
      "[12 / 50] Train: Loss = 0.01091, Accuracy = 98.89%: 100%|██████████| 572/572 [00:04<00:00, 130.96it/s]\n",
      "[12 / 50]   Val: Loss = 0.07938, Accuracy = 93.27%: 100%|██████████| 13/13 [00:00<00:00, 110.26it/s]\n",
      "[13 / 50] Train: Loss = 0.00886, Accuracy = 99.11%: 100%|██████████| 572/572 [00:04<00:00, 131.81it/s]\n",
      "[13 / 50]   Val: Loss = 0.08428, Accuracy = 93.25%: 100%|██████████| 13/13 [00:00<00:00, 109.99it/s]\n",
      "[14 / 50] Train: Loss = 0.00730, Accuracy = 99.29%: 100%|██████████| 572/572 [00:04<00:00, 131.97it/s]\n",
      "[14 / 50]   Val: Loss = 0.08746, Accuracy = 93.17%: 100%|██████████| 13/13 [00:00<00:00, 108.86it/s]\n",
      "[15 / 50] Train: Loss = 0.00591, Accuracy = 99.45%: 100%|██████████| 572/572 [00:04<00:00, 131.04it/s] \n",
      "[15 / 50]   Val: Loss = 0.08972, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 109.88it/s]\n",
      "[16 / 50] Train: Loss = 0.00486, Accuracy = 99.55%: 100%|██████████| 572/572 [00:04<00:00, 132.91it/s]\n",
      "[16 / 50]   Val: Loss = 0.08731, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 104.70it/s]\n",
      "[17 / 50] Train: Loss = 0.00400, Accuracy = 99.63%: 100%|██████████| 572/572 [00:04<00:00, 132.70it/s] \n",
      "[17 / 50]   Val: Loss = 0.10051, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 103.46it/s]\n",
      "[18 / 50] Train: Loss = 0.00333, Accuracy = 99.71%: 100%|██████████| 572/572 [00:04<00:00, 133.59it/s] \n",
      "[18 / 50]   Val: Loss = 0.10289, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 108.47it/s]\n",
      "[19 / 50] Train: Loss = 0.00286, Accuracy = 99.74%: 100%|██████████| 572/572 [00:04<00:00, 131.94it/s] \n",
      "[19 / 50]   Val: Loss = 0.10896, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 113.51it/s]\n",
      "[20 / 50] Train: Loss = 0.00240, Accuracy = 99.78%: 100%|██████████| 572/572 [00:04<00:00, 131.33it/s] \n",
      "[20 / 50]   Val: Loss = 0.10602, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 107.52it/s]\n",
      "[21 / 50] Train: Loss = 0.00240, Accuracy = 99.77%: 100%|██████████| 572/572 [00:04<00:00, 132.68it/s] \n",
      "[21 / 50]   Val: Loss = 0.10965, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 106.75it/s]\n",
      "[22 / 50] Train: Loss = 0.00210, Accuracy = 99.79%: 100%|██████████| 572/572 [00:04<00:00, 133.06it/s] \n",
      "[22 / 50]   Val: Loss = 0.11289, Accuracy = 93.01%: 100%|██████████| 13/13 [00:00<00:00, 107.21it/s]\n",
      "[23 / 50] Train: Loss = 0.00188, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 132.05it/s] \n",
      "[23 / 50]   Val: Loss = 0.11549, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 107.14it/s]\n",
      "[24 / 50] Train: Loss = 0.00180, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 132.90it/s] \n",
      "[24 / 50]   Val: Loss = 0.11532, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 107.43it/s]\n",
      "[25 / 50] Train: Loss = 0.00179, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 132.81it/s] \n",
      "[25 / 50]   Val: Loss = 0.11648, Accuracy = 92.97%: 100%|██████████| 13/13 [00:00<00:00, 105.81it/s]\n",
      "[26 / 50] Train: Loss = 0.00177, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 131.84it/s] \n",
      "[26 / 50]   Val: Loss = 0.13158, Accuracy = 93.01%: 100%|██████████| 13/13 [00:00<00:00, 109.24it/s]\n",
      "[27 / 50] Train: Loss = 0.00178, Accuracy = 99.80%: 100%|██████████| 572/572 [00:04<00:00, 131.55it/s] \n",
      "[27 / 50]   Val: Loss = 0.12668, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 106.63it/s]\n",
      "[28 / 50] Train: Loss = 0.00192, Accuracy = 99.79%: 100%|██████████| 572/572 [00:04<00:00, 130.49it/s] \n",
      "[28 / 50]   Val: Loss = 0.12896, Accuracy = 92.97%: 100%|██████████| 13/13 [00:00<00:00, 108.10it/s]\n",
      "[29 / 50] Train: Loss = 0.00160, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 133.82it/s] \n",
      "[29 / 50]   Val: Loss = 0.12848, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 105.89it/s]\n",
      "[30 / 50] Train: Loss = 0.00147, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 131.18it/s] \n",
      "[30 / 50]   Val: Loss = 0.13631, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 108.61it/s]\n",
      "[31 / 50] Train: Loss = 0.00143, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 130.40it/s] \n",
      "[31 / 50]   Val: Loss = 0.13825, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 108.79it/s]\n",
      "[32 / 50] Train: Loss = 0.00172, Accuracy = 99.79%: 100%|██████████| 572/572 [00:04<00:00, 133.23it/s] \n",
      "[32 / 50]   Val: Loss = 0.14426, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 110.16it/s]\n",
      "[33 / 50] Train: Loss = 0.00197, Accuracy = 99.76%: 100%|██████████| 572/572 [00:04<00:00, 131.83it/s] \n",
      "[33 / 50]   Val: Loss = 0.13290, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 107.33it/s]\n",
      "[34 / 50] Train: Loss = 0.00156, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 135.03it/s] \n",
      "[34 / 50]   Val: Loss = 0.13557, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 106.08it/s]\n",
      "[35 / 50] Train: Loss = 0.00134, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 132.80it/s] \n",
      "[35 / 50]   Val: Loss = 0.14793, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 110.37it/s]\n",
      "[36 / 50] Train: Loss = 0.00131, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 132.71it/s] \n",
      "[36 / 50]   Val: Loss = 0.13738, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 105.52it/s]\n",
      "[37 / 50] Train: Loss = 0.00131, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 130.42it/s] \n",
      "[37 / 50]   Val: Loss = 0.14449, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 109.22it/s]\n",
      "[38 / 50] Train: Loss = 0.00155, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 133.29it/s] \n",
      "[38 / 50]   Val: Loss = 0.15529, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 112.44it/s]\n",
      "[39 / 50] Train: Loss = 0.00194, Accuracy = 99.76%: 100%|██████████| 572/572 [00:04<00:00, 131.88it/s] \n",
      "[39 / 50]   Val: Loss = 0.14323, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 107.68it/s]\n",
      "[40 / 50] Train: Loss = 0.00141, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 131.82it/s] \n",
      "[40 / 50]   Val: Loss = 0.14688, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 108.62it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.00125, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 130.36it/s] \n",
      "[41 / 50]   Val: Loss = 0.15505, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 106.20it/s]\n",
      "[42 / 50] Train: Loss = 0.00128, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 133.55it/s] \n",
      "[42 / 50]   Val: Loss = 0.15738, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 110.65it/s]\n",
      "[43 / 50] Train: Loss = 0.00127, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 132.62it/s] \n",
      "[43 / 50]   Val: Loss = 0.15654, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 108.96it/s]\n",
      "[44 / 50] Train: Loss = 0.00124, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 131.11it/s] \n",
      "[44 / 50]   Val: Loss = 0.14877, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 106.04it/s]\n",
      "[45 / 50] Train: Loss = 0.00128, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 131.77it/s] \n",
      "[45 / 50]   Val: Loss = 0.15513, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 108.11it/s]\n",
      "[46 / 50] Train: Loss = 0.00212, Accuracy = 99.73%: 100%|██████████| 572/572 [00:04<00:00, 132.17it/s] \n",
      "[46 / 50]   Val: Loss = 0.15965, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 106.76it/s]\n",
      "[47 / 50] Train: Loss = 0.00189, Accuracy = 99.74%: 100%|██████████| 572/572 [00:04<00:00, 132.41it/s] \n",
      "[47 / 50]   Val: Loss = 0.16262, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 109.02it/s]\n",
      "[48 / 50] Train: Loss = 0.00127, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 130.65it/s] \n",
      "[48 / 50]   Val: Loss = 0.16290, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 108.43it/s]\n",
      "[49 / 50] Train: Loss = 0.00121, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 131.76it/s] \n",
      "[49 / 50]   Val: Loss = 0.15931, Accuracy = 93.09%: 100%|██████████| 13/13 [00:00<00:00, 106.23it/s]\n",
      "[50 / 50] Train: Loss = 0.00118, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 130.93it/s] \n",
      "[50 / 50]   Val: Loss = 0.15682, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 107.13it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png \" \")  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        <create me>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <use me>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [],
   "source": [
    "<calc test accuracy>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enF9GAPAN3RB"
   },
   "source": [
    "### Дообучение предобученных векторов\n",
    "\n",
    "**Задание** Почему бы не попробовать дообучать вектора? Для этого нужно просто заменить флаг `freeze=False` в методе `from_pretrained`. Попробуйте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AdB6olUiyf7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVJet3RQix98"
   },
   "source": [
    "**Задание** На самом деле, понятно, почему это плохо - после этого нельзя использовать старые предобученные вектора (которые не попали в трейн). Проверьте, какое качество получается на тесте со старыми векторами.\n",
    "\n",
    "Чтобы бороться с этим, можно использовать такой прием: на предобученные вектора накладывать $l_2$-регуляризацию, чтобы они не удалялись от исходных векторов, а для слов, эмбеддинги которых мы не знаем, строить случайные вектора и учить их как обычно.\n",
    "\n",
    "Почитать про это можно чуть-чуть здесь: [Pseudo-rehearsal: A simple solution to catastrophic forgetting for NLP](https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting) либо в книжке Goldberg'а.\n",
    "\n",
    "**Задание** Попробуйте реализовать это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23abeGwPp163"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfN1olf6RZne"
   },
   "source": [
    "## We need to go deeper, сети символьного уровня\n",
    "\n",
    "Напомню, на прошлом занятии мы строили LSTM сеть, которая обрабатывала последовательности символов, и предсказывала, к какому языку относится слово. \n",
    "\n",
    "LSTM выступал в роли feature extractor'а, работающего с произвольного размера последовательностью символов (ну, почти произвольного - мы ограничивались максимальной длиной слова). Батч для сети имел размерность `(max_word_len, batch_size)`.\n",
    "\n",
    "Теперь мы опять хотим использовать такую же идею для извлечения признаков из последовательности символов - потому что последовательность символов же должна быть полезной для предсказания части речи, правда?\n",
    "\n",
    "Сеть должна будет запомнить, например, что `-ly` - это часто про наречие, а `-tion` - про существительное.\n",
    "\n",
    "![](https://image.ibb.co/kzbh6L/Char-Bi-LSTM.png \" \")\n",
    "\n",
    "Остальная часть сети при этом будет такой же.\n",
    "\n",
    "Найдем границу для длины слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SczGwL8Cy0Ws"
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "    \n",
    "def find_max_len(counter, threshold):\n",
    "    sum_count = sum(counter.values())\n",
    "    cum_count = 0\n",
    "    for i in range(max(counter)):\n",
    "        cum_count += counter[i]\n",
    "        if cum_count > sum_count * threshold:\n",
    "            return i\n",
    "    return max(counter)\n",
    "\n",
    "word_len_counter = Counter()\n",
    "for sent in data:\n",
    "    for word, _ in sent:\n",
    "        word_len_counter[len(word)] += 1\n",
    "    \n",
    "threshold = 0.99\n",
    "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
    "\n",
    "print('Max word len for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlArjEvqkMGk"
   },
   "source": [
    "Построим алфавит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LWXHmXGcotd"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def get_range(first_symb, last_symb):\n",
    "    return set(chr(c) for c in range(ord(first_symb), ord(last_symb) + 1))\n",
    "\n",
    "chars = get_range('a', 'z') | get_range('A', 'Z') | get_range('0', '9') | set(punctuation)\n",
    "char2ind = {c : i + 1 for i, c in enumerate(chars)}\n",
    "char2ind['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0OS9WQjkO9b"
   },
   "source": [
    "**Задание** Сконвертируйте данные, как в функции выше - только теперь слова должны отобразиться не в один индекс, а в последовательность.\n",
    "\n",
    "Обрезайте слова по `MAX_WORD_LEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3Q3arGCmgi-"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, char2ind, tag2ind):\n",
    "    X, y = <calc it>\n",
    "    return X, y\n",
    "  \n",
    "X_train, y_train = convert_data(train_data, char2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, char2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, char2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SMmXMx5Rr5z"
   },
   "source": [
    "Напишем генератор батчей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c835LEVERXzl"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start: end]\n",
    "        \n",
    "        sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        word_len = max(len(word) for ind in batch_indices for word in X[ind])\n",
    "            \n",
    "        X_batch = np.zeros((sent_len, len(batch_indices), word_len))\n",
    "        y_batch = np.zeros((sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            for word_ind, word in enumerate(X[sample_ind]):\n",
    "                X_batch[word_ind, batch_ind, :len(word)] = word\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWcRRe11jFI8"
   },
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfY7FcXCknzX"
   },
   "source": [
    "**Задание** Реализуйте сеть, которая принимает батч размера `(seq_len, batch_size, word_len)` и возвращает `(seq_len, batch_size, word_emb_dim)`. Это может быть любая функция, которая умеет в последовательности произвольной длины. Мы уже смотрели на сверточные и рекуррентные сети для такой задачи - попробуйте обе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1qs96uAY3Wd"
   },
   "outputs": [],
   "source": [
    "class CharsEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, char_emb_dim=24, word_emb_dim=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        <create Conv or LSTM encoder>\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        <apply>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ag2R5sIglLhh"
   },
   "source": [
    "**Задание** Реализуйте теггер с эмбеддингами символьного уровня."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRB8tAOAa_YW"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, char_vocab_size, tagset_size, char_emb_dim=24, \n",
    "                 word_emb_dim=128, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        <create it>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEaWjN0qjFfe"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(char_vocab_size=len(char2ind), tagset_size=len(tag2ind)).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20, \n",
    "    batch_size=24, val_data=(X_val, y_val), val_batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGDJqyG9lTxV"
   },
   "source": [
    "**Задание** Оцените его качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCUj9_nqjgrL"
   },
   "outputs": [],
   "source": [
    "_, test_accuracy = do_epoch(model, criterion, (X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12HYYmSzlZtm"
   },
   "source": [
    "### Визуализации\n",
    "\n",
    "**Задание** Посчитайте эмбеддинги символьного уровня (обученные внутри модели перед этим) для 1000 случайных слов из `word2ind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyXEJ6MUG8PE"
   },
   "outputs": [],
   "source": [
    "embeddings, index2word = <calc me>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2klT31GSWlR"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, token):\n",
    "    tsne = get_tsne_projection(embeddings)\n",
    "    draw_vectors(tsne[:, 0], tsne[:, 1], token=token)\n",
    "    \n",
    "\n",
    "visualize_embeddings(embeddings, index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgmDHM9Dl7W7"
   },
   "source": [
    "**Задание** Посчитайте эмбеддинги для всех слов из трейна и для нескольких случайных слов из теста, которые не встречаются в трейне, найдите их ближайших соседей по их эмбеддигам символьного уровня."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bctty__mOOz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzAozOANpnT1"
   },
   "source": [
    "### Словные эмбеддинги\n",
    "\n",
    "**Задание** Только символьных эмбеддингов может быть недостаточно. Верните ещё словные эмбеддинги. Слова стоит приводить к нижнему регистру - признаки, связанные с регистром должны ухватываться символьный LSTM.\n",
    "\n",
    "Эти эмбеддинги можно просто сконкатенировать, можно складывать, а можно использовать гейт (как в LSTM). Например, по эмбеддингу слова предсказывать $o = \\sigma(w)$ - насколько он хорош и сочетать в такой пропорции с символьным эмбеддингом: $o \\odot w + (1 - o) \\odot \\tilde w$, где $\\tilde w$ - эмбеддинг слова, полученный по символьному уровню. Проверьте разные варианты.\n",
    "\n",
    "### Связь словных эмбеддингов и эмбеддингов символьного уровня\n",
    "В словных эмбеддингах мы строим отображение из слова в индекс. В итоге входной батч достаточно небольшой - это хорошо для обучения (быстрее передача на видеокарту). С символьными эмбеддингами беда - но это можно исправить.\n",
    "\n",
    "Давайте предпосчитаем для каждого слова в `word2ind` его последовательность индексов символов. Получится матрица. Эту матрицу можно вместе с моделью перенести на видеокарту. Тогда нужен будет батч из индексов слов - по нему можно сделать лукап (с помощью `F.embedding`) в матрице и получить трехмерную матрицу с символами.\n",
    "\n",
    "Преимущество - по одному батчу можно получить сразу и эмбеддинги слов, и эмбеддинги символьного уровня. Это удобно и энергоэффективно.\n",
    "\n",
    "Другая идея - после того, как мы обучили модель, можно предпосчитать эмбеддинги слов символьного уровня - лукап в таблице эмбеддингов гораздо проще, чем сверточная или рекуррентная сеть над символами. Таким образом, например, получаются эмбеддинги в [FastText](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) - они также исходно считаются на символьном (N-граммном) уровне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gzxhGe6okls"
   },
   "source": [
    "## Encoder-decoder\n",
    "\n",
    "Можно усложнить модель - добавить еще один рекуррентный слой. Первый слой будет служить энкодером последовательности, второй, более легкий - декодировать последовательность. Декодировать - значит, на вход он должен принимать как состояние для данного токена из энкодера, так и предыдущий предсказанный тег.\n",
    "\n",
    "Выглядеть всё будет как-то так:\n",
    "\n",
    "![encoder-decoder](https://image.ibb.co/jOrfT0/Encoder-Decoder.png =x600)\n",
    "\n",
    "Зеленое - уже `LSTM`, а не `Linear`, а принимает оно сразу скрытое состояние от предыдущего токена (зеленая стрелка), предыдущий предсказанный тег (пунктирная стрелка) и состояние из BiLSTM - контекстное представление слова.\n",
    "\n",
    "Тренироваться данная модель должна с teacher-forcing - передачей правильных меток в качестве ответов по пунктирным стрелкам. На предсказании же нужно реализовать beam search - держать сразу несколько лучших путей (последовательностей тегов) для декодируемой последовательности.\n",
    "\n",
    "**Задание** Рискните реализовать это.\n",
    "\n",
    "(А вообще мы будем разбираться с этим подробнее, когда дойдем до машинного перевода - можно вернуться сюда после него :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEJAch5Bj4vQ"
   },
   "source": [
    "# Дополнительные материалы\n",
    "\n",
    "## Классические подходы\n",
    "Speech and Language Processing, Chapter 8, Part-of-speech Tagging. Daniel Jurafsky [[pdf](https://web.stanford.edu/~jurafsky/slp3/8.pdf)]\n",
    "\n",
    "## Статьи\n",
    "Learning Character-level Representations for Part-of-Speech Tagging, dos Santos et al, 2014 [pdf](http://proceedings.mlr.press/v32/santos14.pdf)  \n",
    "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation, Wang Ling et al, 2015 [arxiv](https://arxiv.org/abs/1508.02096)  \n",
    "Bidirectional LSTM-CRF Models for Sequence Tagging, Zhiheng Huang et al, 2015 [arxiv](https://arxiv.org/abs/1508.01991)  \n",
    "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF, Xuezhe Ma et al, 2016 [arxiv](https://arxiv.org/abs/1603.01354)  \n",
    "Improving Part-of-speech Tagging via Multi-task Learning and Character-level Word Representations, Daniil Anastasyev et al, 2018 [pdf](http://www.dialog-21.ru/media/4282/anastasyevdg.pdf) :)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8WVAmMWqsrS"
   },
   "source": [
    "# Сдача\n",
    "\n",
    "[Опрос](https://goo.gl/forms/R6UqcESWIjtVSA6J3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
